[{"path":"index.html","id":"course-overview","chapter":"Course overview","heading":"Course overview","text":"","code":""},{"path":"index.html","id":"motivation","chapter":"Course overview","heading":"Motivation","text":"Ecological modeling within USACE primarily spreadsheet-based activity featuring deterministic habitat models. However, ecological modeling capacity USACE biologists can greatly improved incorporating modern data science practices, especially uptake R, programming language used majority ecological research conducted today. R promotes research well-documented, reproducible, interdisciplinary, extensible. R also works kinds data, produces high-quality graphics, broad supportive network users, easier learn ever . training series focus teaching USACE biologists practitioners use R ecological modeling.","code":""},{"path":"index.html","id":"intended-audience","chapter":"Course overview","heading":"Intended audience","text":"Modeling experience: considering “modelers” projects, familiarity ecological modeling, wanting expand modeling toolbox. open learning use code.Career trajectory: USACE biologists geospatial analysts; course may also benefit ecological/environmental engineers.Institutional context: USACE team members; partners working closely USACE.Pre-requisites: None! Although familiarity ecological models used USACE may useful. Similarly, experience numerical modeling, coding, data analysis may also help required.","code":""},{"path":"index.html","id":"learning-objectives","chapter":"Course overview","heading":"Learning objectives","text":"Become familiar R coding basics, RStudio environment, benefits using R conducting analyses.Learn important building blocks data “wrangling” visualization R.Learn key methods seeking help R coding problems.Increase familiarity existing habitat modeling tools.Develop ability conduct basic non-habitat ecological modeling compare traditional habitat models.","code":""},{"path":"index.html","id":"module-format","chapter":"Course overview","heading":"Module format","text":"Asynchronous, self-directed modules accompanied full R scripts carry analyses. format selected :Durable (.e., tutorials persist online)Flexible learnersHas potential impact beyond USACE","code":""},{"path":"index.html","id":"module-structure","chapter":"Course overview","heading":"Module structure","text":"Background context (5-10 minutes)Coding mechanics (10-20 minutes)Application code real problem (10-20 minutes)","code":""},{"path":"index.html","id":"additional-resources-to-be-provided-by-instructors","chapter":"Course overview","heading":"Additional resources to be provided by instructors","text":"Potential zoom “office hours” assist trainees coding data troubleshootingLinks vetted resources working R, RStudio, ecological models","code":""},{"path":"index.html","id":"course-subject-matter","chapter":"Course overview","heading":"Course subject matter","text":"training course include modules introduce users coding R, followed modules focus different ecological modeling topics. Users already proficient R RStudio wish learn ecological modeling may skip directly specific modules relevant goals.Modules basics using R RStudio adapted open-source course entitled Data Analysis Visualization R Ecologists1 created Carpentries, community instructors trainers teach foundational data science skills researchers. course designed ~6 hour workshop, also adapted multiple standalone videos (e.g., ); divide workshop materials four modules.","code":""},{"path":"required-set-up-for-the-course.html","id":"required-set-up-for-the-course","chapter":"Required set-up for the course","heading":"Required set-up for the course","text":"Authors: Data Carpentry contributors - Copyright (c) Data Carpentry; Ed Stowe (adaptation USACE)Last update: 2025-02-03Acknowledgements: bulk section adapted Data Carpentry course Data Analysis Visualization R Ecologists licensed CC-4.0 authors","code":""},{"path":"required-set-up-for-the-course.html","id":"preparations","chapter":"Required set-up for the course","heading":"0.1 Preparations","text":"modules designed learners carry computing follow material. Consequently, learners must R RStudio installed computers. also need able install number R packages, create directories, download files.avoid troubleshooting lesson, learners follow \ninstructions download install everything beforehand.USACE user’s ACE-machines may need assistance departments download R R Studio install packages. Therefore, give ample time get set-R R Studio potential training sessions.","code":""},{"path":"required-set-up-for-the-course.html","id":"install-r-and-rstudio","chapter":"Required set-up for the course","heading":"0.1.1 Install R and RStudio","text":"R RStudio two separate pieces software:R programming language software used run code written R.RStudio integrated development environment (IDE) makes using R easier. course use RStudio interact R.don’t already R RStudio installed, follow instructions operating system .\ninstall R install RStudio.","code":""},{"path":"required-set-up-for-the-course.html","id":"operating-system","chapter":"Required set-up for the course","heading":"0.1.1.1 Operating System","text":"","code":""},{"path":"required-set-up-for-the-course.html","id":"for-windows","chapter":"Required set-up for the course","heading":"0.1.1.1.1 For Windows","text":"Download R CRAN website.Run .exe file just downloadedGo RStudio download pageUnder Installers select RStudio x.yy.zzz - Windows Vista/7/8/10 (x, y, z represent version numbers)Double click file install itOnce ’s installed, open RStudio make sure works don’t get error messages.","code":""},{"path":"required-set-up-for-the-course.html","id":"for-macos","chapter":"Required set-up for the course","heading":"0.1.1.1.2 For MacOS","text":"Download R CRAN website.Select .pkg file latest R versionDouble click downloaded file install RIt also good idea install XQuartz (needed packages)Go RStudio download pageUnder Installers select RStudio x.yy.zzz - Mac OS X 10.6+ (64-bit) (x, y, z represent version numbers)Double click file install RStudioOnce ’s installed, open RStudio make sure works don’t get error messages.","code":""},{"path":"required-set-up-for-the-course.html","id":"for-linux","chapter":"Required set-up for the course","heading":"0.1.1.1.3 For Linux","text":"Download R CRAN website.Select .pkg file latest R versionDouble click downloaded file install RIt also good idea install XQuartz (needed packages)Go RStudio download pageUnder Installers select RStudio x.yy.zzz - Mac OS X 10.6+ (64-bit) (x, y, z represent version numbers)Double click file install RStudioOnce ’s installed, open RStudio make sure works don’t get error messages.","code":""},{"path":"required-set-up-for-the-course.html","id":"update-r-and-rstudio","chapter":"Required set-up for the course","heading":"0.1.2 Update R and RStudio","text":"already R RStudio installed, first check R version date:open RStudio R version printed console bottom left. Alternatively, can type sessionInfo() console. R version 4.0.0 later, don’t need update R lesson. version R older , download install latest version R R project website Windows, MacOS, LinuxIt necessary remove old versions R system, wish can check uninstall R?installing new version R, reinstall packages new version. Windows, package called installr can help upgrading R version migrate package library. similar package called pacman can help updating R packages across\nupdate RStudio latest version, open RStudio click \nHelp > Check Updates. new version available follow \ninstruction screen. default, RStudio also automatically notify \nnew versions every .","code":""},{"path":"required-set-up-for-the-course.html","id":"install-required-r-packages","chapter":"Required set-up for the course","heading":"0.1.3 Install required R packages","text":"training modules use various R packages, useful R code written people supplement code included base R language. Several modules use R packages specific analyses covered modules; instances, users prompted download load required packages. first four lessons covering introduction R, use packages tidyverse, lubridate, ratdat, learners planning complete intro R install packages.try install packages, open RStudio copy paste following command console window (look blinking cursor bottom left), press Enter (Windows Linux) Return (MacOS) execute command.Alternatively, can install packages using RStudio’s graphical user interface going Tools > Install Packages typing names packages separated comma.R tries download install packages machine.installation finished, can try load packages pasting following code console:see error like package called ‘...’ good go!","code":"\ninstall.packages(c(\"tidyverse\", \"lubridate\", \"ratdat\"))\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(ratdat)"},{"path":"required-set-up-for-the-course.html","id":"updating-r-packages","chapter":"Required set-up for the course","heading":"0.1.4 Updating R packages","text":"Generally, recommended keep R version packages date, new versions bring improvements important bugfixes. update packages installed, click Update Packages tab bottom right panel RStudio, go Tools > Check Package Updates...update packages required course, even installed relatively recently.","code":""},{"path":"required-set-up-for-the-course.html","id":"download-the-data","chapter":"Required set-up for the course","heading":"0.1.5 Download the data","text":"download data directly R modules. However, expecting problems network, may better download data beforehand store machine.data files lesson can downloaded manually:cleaned data andzip file raw data.","code":""},{"path":"introduction-to-r-and-rstudio.html","id":"introduction-to-r-and-rstudio","chapter":"1 Introduction to R and RStudio","heading":"1 Introduction to R and RStudio","text":"Authors: Data Carpentry contributors - Copyright (c) Data Carpentry; Ed Stowe (adaptation USACE, Getting R Help Section); Brian Breaker & Edmund Howe (Getting R Help Section)Last update: 2025-02-03Acknowledgements: bulk section adapted Data Carpentry course Data Analysis Visualization R Ecologists licensed CC-4.0 authors","code":""},{"path":"introduction-to-r-and-rstudio.html","id":"learning-objectives-1","chapter":"1 Introduction to R and RStudio","heading":"Learning Objectives:","text":"Understand R can benefit USACE biologistsUnderstand basic workflow using R RStudioUnderstand get help R problems several ways","code":""},{"path":"introduction-to-r-and-rstudio.html","id":"what-are-r-and-rstudio","chapter":"1 Introduction to R and RStudio","heading":"1.1 What are R and RStudio?","text":"R refers programming language well software runs R code.RStudio software interface can make easier write R scripts interact R software. ’s popular platform, RStudio also maintains tidyverse series packages use lesson.","code":""},{"path":"introduction-to-r-and-rstudio.html","id":"why-learn-r","chapter":"1 Introduction to R and RStudio","heading":"1.2 Why learn R?","text":"","code":""},{"path":"introduction-to-r-and-rstudio.html","id":"r-does-not-involve-lots-of-pointing-and-clicking-and-thats-a-good-thing","chapter":"1 Introduction to R and RStudio","heading":"1.2.1 R does not involve lots of pointing and clicking, and that’s a good thing","text":"Since R programming language, results analysis rely remembering succession pointing clicking, instead series written commands, ’s good thing! , want redo analysis collected data, don’t remember button clicked order obtain results; just run script .Working scripts makes steps used analysis clear, code write can inspected someone else can give feedback spot mistakes.Working scripts forces deeper understanding , facilitates learning comprehension methods use.","code":""},{"path":"introduction-to-r-and-rstudio.html","id":"r-code-is-great-for-reproducibility","chapter":"1 Introduction to R and RStudio","heading":"1.2.2 R code is great for reproducibility","text":"Reproducibility someone else (including future self) can obtain results dataset using analysis.R integrates tools generate manuscripts code. collect data, fix mistake dataset, figures statistical tests manuscript updated automatically.increasing number journals funding agencies expect analyses reproducible, knowing R give edge requirements.find running similar analyses many USACE projects, can simply adapt code earlier project analysis figures come together fraction time take analyze things scratch.","code":""},{"path":"introduction-to-r-and-rstudio.html","id":"r-is-interdisciplinary-and-extensible","chapter":"1 Introduction to R and RStudio","heading":"1.2.3 R is interdisciplinary and extensible","text":"tens thousands packages can installed extend capabilities, R provides framework allows combine statistical approaches many scientific disciplines best suit analytical framework need analyze data. instance, R packages image analysis, GIS, time series, including numerous packages highly relevant USACE biologists engineers (e.g., dataRetrieval package downloading USGS gage data directly R session).","code":""},{"path":"introduction-to-r-and-rstudio.html","id":"r-works-on-data-of-all-shapes-and-sizes","chapter":"1 Introduction to R and RStudio","heading":"1.2.4 R works on data of all shapes and sizes","text":"skills learn R scale easily size dataset. Whether dataset hundreds millions lines, won’t make much difference .R designed data analysis. comes special data structures data types make handling missing data statistical factors convenient.R can read data many different file types, including geospatial data, connect local remote databases.","code":""},{"path":"introduction-to-r-and-rstudio.html","id":"r-produces-high-quality-graphics","chapter":"1 Introduction to R and RStudio","heading":"1.2.5 R produces high-quality graphics","text":"R well-developed plotting capabilities, ggplot2 package one , powerful pieces plotting software available today. begin learning use ggplot2 next module.","code":""},{"path":"introduction-to-r-and-rstudio.html","id":"r-has-a-large-and-welcoming-community","chapter":"1 Introduction to R and RStudio","heading":"1.2.6 R has a large and welcoming community","text":"Thousands people use R daily. Many willing help mailing lists websites Stack Overflow, RStudio community.Since R popular among researchers, help communities learning materials aimed towards researchers. Python similar language R, can accomplish many tasks, widely used software developers software engineers, Python resources communities oriented towards researchers.","code":""},{"path":"introduction-to-r-and-rstudio.html","id":"not-only-is-r-free-but-it-is-also-open-source-and-cross-platform","chapter":"1 Introduction to R and RStudio","heading":"1.2.7 Not only is R free, but it is also open-source and cross-platform","text":"Anyone can inspect source code see R works. transparency, less chance mistakes, (someone else) find , can report fix bugs.","code":""},{"path":"introduction-to-r-and-rstudio.html","id":"navigating-rstudio","chapter":"1 Introduction to R and RStudio","heading":"1.3 Navigating RStudio","text":"use RStudio integrated development environment (IDE) write code scripts, run code R, navigate files computer, inspect objects create R, look plots make. RStudio many features can help things like version control, developing R packages, writing Shiny apps, won’t cover workshop.screenshot, can see 4 “panes” default layout:Top-Left: Source pane displays scripts files.\n3 panes, Console pane top left, press Shift+Cmd+N (Mac) Shift+Ctrl+N (Windows) open blank R script, make Source pane appear.\n3 panes, Console pane top left, press Shift+Cmd+N (Mac) Shift+Ctrl+N (Windows) open blank R script, make Source pane appear.Top-Right: Environment/History pane, shows objects current R session (Environment) command history (History)\ntabs , including Connections, Build, Tutorial, possibly Git\nwon’t cover tabs, RStudio lots useful features\ntabs , including Connections, Build, Tutorial, possibly Gitwe won’t cover tabs, RStudio lots useful featuresBottom-Left: Console pane, can interact directly R console, interprets R commands prints results\nalso tabs Terminal Jobs\nalso tabs Terminal JobsBottom-Right: Files/Plots/Help/Viewer pane navigate files view plots help pagesYou can customize layout panes, well many settings RStudio color scheme, font, even keyboard shortcuts. can access settings going menu bar, clicking Tools → Global Options.RStudio puts things need work R single window, also includes features like keyboard shortcuts, autocompletion code, syntax highlighting (different types code colored differently, making easier navigate code).","code":""},{"path":"introduction-to-r-and-rstudio.html","id":"getting-set-up-in-rstudio","chapter":"1 Introduction to R and RStudio","heading":"1.4 Getting set up in RStudio","text":"good practice organize projects self-contained folders right start, start building habit now. well-organized project easier navigate, reproducible, easier share others. project start top-level folder contains everything necessary project, including data, scripts, images, organized sub-folders.RStudio provides “Projects” feature can make easier work individual projects R. create project keep everything workshop.Start RStudio (see view similar screenshot ).top right, see blue 3D cube words “Project: (None)”. Click icon.Click New Project dropdown menu.Click New Directory, New Project.Type name project, recommend R-Ecology-Workshop.Put convenient location using “Create project subdirectory :” section. recommend Desktop. can always move project somewhere else later, self-contained.Click Create Project new project open.Next time open RStudio, can click 3D cube icon, see options open existing projects, like one just made.One benefits using RStudio Projects automatically set working directory top-level folder project. working directory folder R working, views location files (including data scripts) relative working directory. may come across scripts include something like setwd(\"/Users/YourUserName/MyCoolProject\"), directly sets working directory. usually much less portable, since specific directory might found someone else’s computer (probably don’t username ). Using RStudio Projects means don’t deal manually setting working directory.settings need adjust improve reproducibility work. Go menu bar, click Tools → Global Options open Options window.Make sure settings match highlighted yellow. don’t want RStudio store current status R session reload next time start R. might sound convenient, sake reproducibility, want start clean, empty R session every time work. means record everything scripts, save data need files, store outputs like images files. want get used everything generate single R session disposable. want scripts able regenerate things need, “raw materials” like data.","code":""},{"path":"introduction-to-r-and-rstudio.html","id":"organizing-your-project-directory","chapter":"1 Introduction to R and RStudio","heading":"1.5 Organizing your project directory","text":"Using consistent folder structure across new projects help keep growing project organized, make easy find files future. especially beneficial working multiple projects, since know look particular kinds files.use basic structure workshop, often good place start, can extended meet specific needs. diagram describing structure:Within project folder (R-Ecology-Workshop), first scripts folder hold scripts write. also data folder containing cleaned raw subfolders. general, want keep raw data completely untouched, put data folder, modify . Instead, read R, make modifications, write modified file cleaned folder. also images folder plots make, documents folder documents might produce.Let’s start making new folders. Go Files pane (bottom right), check current directory, highlighted yellow . directory project just made, case R-Ecology-Workshop. shouldn’t see folders yet.Next, click New Folder button, type scripts generate scripts folder. appear Files list now. Repeat process make data, images, documents folders. , click data folder Files pane. take data folder, empty. Use New Folder button create raw cleaned folders. return R-Ecology-Workshop folder, click file path, highlighted yellow previous image. ’s worth noting Files pane helps create, find, open files, moving files won’t change working directory project .","code":"R-Ecology-Workshop\n│\n└── scripts\n│\n└── data\n│    └── cleaned\n│    └── raw\n│\n└─── images\n│\n└─── documents"},{"path":"introduction-to-r-and-rstudio.html","id":"working-in-r-and-rstudio","chapter":"1 Introduction to R and RStudio","heading":"1.6 Working in R and RStudio","text":"basis programming write instructions computer follow, tell computer follow instructions. write instructions form code, common language understood computer humans (practice). call instructions commands, tell computer follow instructions running (also called executing) commands.","code":""},{"path":"introduction-to-r-and-rstudio.html","id":"console-vs.-script","chapter":"1 Introduction to R and RStudio","heading":"1.6.1 Console vs. script","text":"can run commands directly R console, can write R script. may help think working console vs. working script something like cooking. console like making new recipe, writing anything . can carry series steps produce nice, tasty dish end. However, didn’t write anything , ’s harder figure exactly , order.Writing script like taking nice notes cooking- can tweak edit recipe want, can come back 6 months try , don’t try remember went well didn’t. ’s actually even easier cooking, since can hit one button computer “cooks” whole recipe !additional benefit scripts can leave comments others read. Lines start # considered comments interpreted R code.","code":""},{"path":"introduction-to-r-and-rstudio.html","id":"console","chapter":"1 Introduction to R and RStudio","heading":"1.6.1.1 Console","text":"R console code run/executedThe prompt, > symbol, can type commandsBy pressing Enter, R execute commands print result.can work , history saved History pane, can’t access future","code":""},{"path":"introduction-to-r-and-rstudio.html","id":"script","chapter":"1 Introduction to R and RStudio","heading":"1.6.1.2 Script","text":"script record commands send R, preserved plain text file .R extensionYou can make new R script clicking File → New File → R Script, clicking green + button top left corner RStudio, pressing Shift+Cmd+N (Mac) Shift+Ctrl+N (Windows). unsaved, called “Untitled1”type lines R code script, can send R console evaluated\nCmd+Enter (Mac) Ctrl+Enter (Windows) run line code cursor \nhighlight multiple lines code, can run pressing Cmd+Enter (Mac) Ctrl+Enter (Windows)\npreserving commands script, can edit rerun quickly, save later, share others\ncan leave comments starting line #\nCmd+Enter (Mac) Ctrl+Enter (Windows) run line code cursor onIf highlight multiple lines code, can run pressing Cmd+Enter (Mac) Ctrl+Enter (Windows)preserving commands script, can edit rerun quickly, save later, share othersYou can leave comments starting line #","code":""},{"path":"introduction-to-r-and-rstudio.html","id":"example","chapter":"1 Introduction to R and RStudio","heading":"1.6.1.3 Example","text":"Let’s try running code console script. First, click Console pane, type 1+1. Hit Enter run code. see code echoed, value 2 returned.Now click blank script, type 1+1. cursor line, hit Cmd+Enter (Mac) Ctrl+Enter (Windows) run code. see code sent script console, returned value 2, just like ran code directly console.","code":""},{"path":"introduction-to-r-and-rstudio.html","id":"getting-help-with-r-coding","chapter":"1 Introduction to R and RStudio","heading":"1.7 Getting Help with R Coding","text":"able find help interpret probably one important skills learning new language, including R. Thankfully, many helpful resources learning R, several recent developments make easier use R code ever.","code":""},{"path":"introduction-to-r-and-rstudio.html","id":"help-from-within-rstudio","chapter":"1 Introduction to R and RStudio","heading":"1.7.1 Help from within RStudio","text":"Help functions: numerous ways get help R without leaving RStudio. First, help() function ? help operator (run either script consile) can used access documentation R functions packages. example, running commands help(print) ?print open documentation print() function, specifying help(package = \"dplyr\") provide info dplyr package. don’t know exact name function package can run apropos(\"print\") return available functions “print” name. Similarly, running ??print provides fuzzy matching search term returns function help pages, also vignettes R resources pertaining term.RStudio autocomplete: typing first letters function script console, can press Tab see autocomplete options. especially helpful don’t remember exact name function looking . Similarly, ’re trying remember specific arguments (.e., inputs) function, can type tab within parentheses function (e.g., pivot_wider()) order see arguments function looking . Finally, ’re working package, can also type name (already-installed) package followed two colons (e.g., dplyr::) Rstudio shows dropdown menu available functions package.","code":""},{"path":"introduction-to-r-and-rstudio.html","id":"r-task-views","chapter":"1 Introduction to R and RStudio","heading":"1.7.2 R Task Views","text":"Task views provide annotated list packages relevant particular field. Two useful ones USACE scientists/engineers :Environmetrics Task View: overview packages relevant ecological environmental research.Spatial Analysis Task View: relevant packages spatial analysis, GIS, Remote Sensing R.","code":""},{"path":"introduction-to-r-and-rstudio.html","id":"google-and-stackoverflow","chapter":"1 Introduction to R and RStudio","heading":"1.7.3 Google and StackOverflow","text":"Often quickest way get help Google. key searching R help try specific possible, include term R specific packages search. Many helpful responses can found site StackOverflow ‘r’ tag. StackOverflow discussion forum things related programming. can use tag search functions StackOverflow find answers almost anything can think .","code":""},{"path":"introduction-to-r-and-rstudio.html","id":"ai-chatbots","chapter":"1 Introduction to R and RStudio","heading":"1.7.4 AI chatbots","text":"Artificial intelligence chatbots (e.g., ChatGPT) often quite helpful code-related questions can generate code many specific tasks. However, careful using chatbots: sometimes generate inaccurate code even “hallucinate” code exist.","code":""},{"path":"introduction-to-r-and-rstudio.html","id":"package-vignettes","chapter":"1 Introduction to R and RStudio","heading":"1.7.5 Package vignettes","text":"R package developers often include vignettes packages. vignettes walk user common applications packages, can extremely instructive learning perform specific types analyses. vignette exists, usually found packages reference page CRAN2 , Vignette section. packages vignettes, hosted GitHub sites opposed CRAN.","code":""},{"path":"introduction-to-r-and-rstudio.html","id":"other-resources","chapter":"1 Introduction to R and RStudio","heading":"1.7.6 Other Resources","text":"many resources mention everyone favorites. just :R Data Science: best known guide using R tidy methods.Advanced R: follow-R Data Science book.CRAN Cheatsheets: good cheat sheet official source.RStudio Cheatsheets: Additional cheat sheets RStudio. data wrangling cheatsheet may especially useful.","code":""},{"path":"introduction-to-r-and-rstudio.html","id":"summary","chapter":"1 Introduction to R and RStudio","heading":"1.8 Summary","text":"R programming language software used run commands languageRStudio software make easier write run code RUse R Projects keep work organized self-containedWrite code scripts reproducibility portabilityLearning get R help important","code":""},{"path":"data-visualization-with-ggplot2.html","id":"data-visualization-with-ggplot2","chapter":"2 Data Visualization with ggplot2","heading":"2 Data Visualization with ggplot2","text":"module teach use ggplot package R efficiently generate customizable complex plots like :Authors: Data Carpentry contributors - Copyright (c) Data Carpentry; Ed Stowe (adaptation USACE)Last update: 2025-02-03Acknowledgements: bulk section adapted Data Carpentry course Data Analysis Visualization R Ecologists licensed CC-4.0 authorsthe ggplot() function initiates plot, geom_ functions add representations datause aes() mapping variable data part plotuse scale_ functions modify scales used represent variablesuse premade theme_ functions broadly change appearance, theme() function fine-tunestart simple build plots iteratively","code":""},{"path":"data-visualization-with-ggplot2.html","id":"learning-objectives-2","chapter":"2 Data Visualization with ggplot2","heading":"2.1 Learning objectives","text":"Produce scatter plots boxplots using ggplot2.Represent data variables plot components.Modify scales plot components.Iteratively build modify ggplot2 plots adding layers.Change appearance existing ggplot2 plots using premade customized themes.Describe faceting apply faceting ggplot2.Save plots image files.::::::::::::::::::::::::::::::::::::::::::::::::","code":""},{"path":"data-visualization-with-ggplot2.html","id":"setup","chapter":"2 Data Visualization with ggplot2","heading":"2.2 Setup","text":"going using functions ggplot2 package visualize data. Functions predefined bits code automate complicated actions. R many built-functions, can access many loading packages functions data R.don’t blank, untitled script open yet, go ahead open one Shift+Cmd+N (Mac) Shift+Ctrl+N (Windows). save file scripts/ folder, title workshop_code.R.Earlier, install tidyverse series packages, includes ggplot2 package running install.packages(\"tidyverse\"). installed package onto computer R can access . order use current session, load package using library() function.ggplot2 installed, can run install.packages(\"tidyverse\") console.good practice put install.packages() script. every time run whole script, package reinstalled, typically unnecessary. want install package computer , load library() script need use .Later learn read data external files R, now going use clean ready--use dataset provided ratdat data package. make dataset available, need load package .ratdat package contains data Portal Project, long-term dataset Portal, Arizona, Chihuahuan desert.using dataset called complete_old, contains older years mammal survey data. Let’s try learn little bit data. can use ? front name dataset, bring help page data.can read descriptions variable data.actually take look data, can use View() function open interactive viewer, behaves like simplified version spreadsheet program. ’s handy function, somewhat limited trying view large datasets.hover tab interactive View(), can click “x” appears, close tab.can find dataset using str() function examine structure data.str() tell us many observations/rows (obs) variables/columns , well information variables. see name variable (year), followed kind variable (int integer, chr character), first 10 entries variable. talk different data types structures later .","code":"\nlibrary(tidyverse)\nlibrary(ratdat)\n?complete_old\nView(complete_old)\nstr(complete_old)## tibble [16,878 × 13] (S3: tbl_df/tbl/data.frame)\n##  $ record_id      : int [1:16878] 1 2 3 4 5 6 7 8 9 10 ...\n##  $ month          : int [1:16878] 7 7 7 7 7 7 7 7 7 7 ...\n##  $ day            : int [1:16878] 16 16 16 16 16 16 16 16 16 16 ...\n##  $ year           : int [1:16878] 1977 1977 1977 1977 1977 1977 1977 1977 1977 1977 ...\n##  $ plot_id        : int [1:16878] 2 3 2 7 3 1 2 1 1 6 ...\n##  $ species_id     : chr [1:16878] \"NL\" \"NL\" \"DM\" \"DM\" ...\n##  $ sex            : chr [1:16878] \"M\" \"M\" \"F\" \"M\" ...\n##  $ hindfoot_length: int [1:16878] 32 33 37 36 35 14 NA 37 34 20 ...\n##  $ weight         : int [1:16878] NA NA NA NA NA NA NA NA NA NA ...\n##  $ genus          : chr [1:16878] \"Neotoma\" \"Neotoma\" \"Dipodomys\" \"Dipodomys\" ...\n##  $ species        : chr [1:16878] \"albigula\" \"albigula\" \"merriami\" \"merriami\" ...\n##  $ taxa           : chr [1:16878] \"Rodent\" \"Rodent\" \"Rodent\" \"Rodent\" ...\n##  $ plot_type      : chr [1:16878] \"Control\" \"Long-term Krat Exclosure\" \"Control\" \"Rodent Exclosure\" ..."},{"path":"data-visualization-with-ggplot2.html","id":"plotting-with-ggplot2","chapter":"2 Data Visualization with ggplot2","heading":"2.3 Plotting with ggplot2","text":"ggplot2 powerful package allows create complex plots tabular data (data table format rows columns). package uses consistent vocabulary create plots widely varying types. Therefore, need small changes code underlying data changes decide make box plot instead scatter plot. approach helps create publication-quality plots minimal adjusting tweaking.ggplot2 part tidyverse series packages, tend like data “long” “tidy” format, means column represents single variable, row represents single observation. Well-structured data save lots time making figures ggplot2. now, use data already format. start learning R using ggplot2 relies concepts need talk data transformation next lessons.ggplot plots built step step adding new layers, allows extensive flexibility customization plots.languages, like Python, require certain spacing indentation code run properly. isn’t case R, see spaces indentation code lesson, improve readability.build plot, use basic template can used different types plots:use ggplot() function create plot. order tell data use, need specify data argument. argument input function takes, set arguments using = sign.get blank plot haven’t told ggplot() variables want correspond parts plot. can specify “mapping” variables plot elements, x/y coordinates, size, shape, using aes() function. ’ll also add comment, line starting #. ’s good idea use comments organize code clarify .Now ’ve got plot x y axes corresponding variables complete_old. However, haven’t specified want data displayed. using geom_ functions, specify type geometry want, points, lines, bars. can add geom_point() layer plot using + sign. indent onto new line make easier read, end first line + sign.may notice warning missing values removed. variable necessary make plot missing given row data (case, hindfoot_length weight), can’t plotted. ggplot2 just uses warning message let us know rows couldn’t plotted.Warning messages one ways R communicate . Warnings can thought “heads ”. Nothing necessarily went wrong, author function wanted draw attention something. case, ’s worth knowing rows data plotted missing data.serious type message error. ’s example:can see, get error message, plot, something actually gone wrong. particular error message fairly common, happened misspelled point poit. function named geom_poit(), R tells us can’t find function name.","code":"ggplot(data = <DATA>, mapping = aes(<MAPPINGS>)) + <GEOM_FUNCTION>()\nggplot(data = complete_old)\n# adding a mapping to x and y axes\nggplot(data = complete_old, mapping = aes(x = weight, y = hindfoot_length))\nggplot(data = complete_old, mapping = aes(x = weight, y = hindfoot_length)) +\n  geom_point()## Warning: Removed 3081 rows containing missing values or values outside the\n## scale range (`geom_point()`).\nggplot(data = complete_old, mapping = aes(x = weight, y = hindfoot_length)) +\n  geom_poit()## Error in geom_poit(): could not find function \"geom_poit\""},{"path":"data-visualization-with-ggplot2.html","id":"changing-aesthetics","chapter":"2 Data Visualization with ggplot2","heading":"2.4 Changing aesthetics","text":"Building ggplot plots often iterative process, ’ll continue developing scatter plot just made. may noticed parts scatter plot many overlapping points, making difficult see data. can adjust transparency points using alpha argument, takes value 0 1:can also change color points:Two common issues might run working R forgetting closing bracket closing quote. Let’s take look one .Try running following code:see + appear console. R telling expects input order finish running code. missing closing bracket end geom_point function call. can hit Esc console reset .Something similar happen run following code:missing quote end blue means rest code treated part quote, bit easier see since RStudio displays character strings different color.get different error message run following code:time extra closing ), R doesn’t know . tells unexpected ), doesn’t pinpoint exactly . enough time working R, get better spotting mismatched brackets.","code":"\nggplot(data = complete_old, mapping = aes(x = weight, y = hindfoot_length)) +\n  geom_point(alpha = 0.2)\nggplot(data = complete_old, mapping = aes(x = weight, y = hindfoot_length)) +\n  geom_point(alpha = 0.2, color = \"blue\")ggplot(data = complete_old, mapping = aes(x = weight, y = hindfoot_length)) +\n  geom_point(color = \"blue\", alpha = 0.2ggplot(data = complete_old, mapping = aes(x = weight, y = hindfoot_length)) +\n  geom_point(color = \"blue, alpha = 0.2)ggplot(data = complete_old, mapping = aes(x = weight, y = hindfoot_length)) +\n  geom_point(color = \"blue\", alpha = 0.2))"},{"path":"data-visualization-with-ggplot2.html","id":"adding-another-variable","chapter":"2 Data Visualization with ggplot2","heading":"2.4.1 Adding another variable","text":"Let’s try coloring points according sampling plot type (plot refers physical area rodents sampled nothing making graphs). Since ’re now mapping variable (plot_type) component ggplot2 plot (color), need put argument inside aes():","code":"\nggplot(data = complete_old, mapping = aes(x = weight, y = hindfoot_length, color = plot_type)) +\n  geom_point(alpha = 0.2)"},{"path":"data-visualization-with-ggplot2.html","id":"challenge-1-modifying-plots","chapter":"2 Data Visualization with ggplot2","heading":"2.5 Challenge 1: Modifying plots","text":"Try modifying plot shape point varies sex. set shape way set color.think good way represent sex data?Solution. Now try changing plot color points vary year. notice difference color scale compared changing color plot type? think happened?Solution. Part 2, color scale different compared using color = plot_type plot_type year different variable types. plot_type categorical variable, ggplot2 defaults use discrete color scale, whereas year numeric variable, ggplot2 uses continuous color scale.","code":"\nggplot(data = complete_old, \n       mapping = aes(x = weight, y = hindfoot_length, shape = sex)) +\n  geom_point(alpha = 0.2)\nggplot(data = complete_old, \n       mapping = aes(x = weight, y = hindfoot_length, color = year)) +\n  geom_point(alpha = 0.2)"},{"path":"data-visualization-with-ggplot2.html","id":"changing-scales","chapter":"2 Data Visualization with ggplot2","heading":"2.6 Changing scales","text":"default discrete color scale isn’t always ideal: isn’t friendly viewers colorblindness doesn’t translate well grayscale. However, ggplot2 comes quite color scales, including fantastic viridis scales, designed colorblind grayscale friendly. can change scales adding scale_ functions plots:Scales don’t just apply colors- plot component put inside aes() can modified scale_ functions. Just modified scale used map plot_type color, can modify way weight mapped x axis using scale_x_log10() function:One nice thing ggplot tidyverse general groups functions similar things given similar names. function modifies ggplot scale starts scale_, making easier search right function.","code":"\nggplot(data = complete_old, mapping = aes(x = weight, y = hindfoot_length, color = plot_type)) +\n  geom_point(alpha = 0.2) +\n  scale_color_viridis_d()\nggplot(data = complete_old, mapping = aes(x = weight, y = hindfoot_length, color = plot_type)) +\n  geom_point(alpha = 0.2) +\n  scale_x_log10()"},{"path":"data-visualization-with-ggplot2.html","id":"boxplot","chapter":"2 Data Visualization with ggplot2","heading":"2.7 Boxplot","text":"Let’s try making different type plot altogether. ’ll start basic building blocks using ggplot() aes().time, let’s try making boxplot, plot_type x axis hindfoot_length y axis. can adding geom_boxplot() ggplot():Just colored points , can color boxplot plot_type well:looks like color affected outlines boxplot, rectangular portions. color impacts 1-dimensional parts ggplot: points lines. change color 2-dimensional parts plot, use fill:One thing may notice axis labels overlapping , depending wide plot viewer . One way help make legible wrap text. can modifying labels x axis scale.use scale_x_discrete() function discrete axis, modify labels argument. function label_wrap_gen() wrap text labels make legible.","code":"\nggplot(data = complete_old, mapping = aes(x = plot_type, y = hindfoot_length))\nggplot(data = complete_old, mapping = aes(x = plot_type, y = hindfoot_length)) +\n  geom_boxplot()## Warning: Removed 2733 rows containing non-finite outside the scale range\n## (`stat_boxplot()`).\nggplot(data = complete_old, mapping = aes(x = plot_type, y = hindfoot_length, color = plot_type)) +\n  geom_boxplot()\nggplot(data = complete_old, mapping = aes(x = plot_type, y = hindfoot_length, fill = plot_type)) +\n  geom_boxplot()\nggplot(data = complete_old, mapping = aes(x = plot_type, y = hindfoot_length, fill = plot_type)) +\n  geom_boxplot() +\n  scale_x_discrete(labels = label_wrap_gen(width = 10))"},{"path":"data-visualization-with-ggplot2.html","id":"adding-geoms","chapter":"2 Data Visualization with ggplot2","heading":"2.8 Adding geoms","text":"One powerful aspects ggplot way can add components plot successive layers. boxplots can useful summarizing data, often helpful show raw data well. ggplot, can easily add another geom_ plot show raw data.Let’s add geom_point() visualize raw data. modify alpha argument help overplotting.Uh oh… points given x axis category fall exactly line, isn’t useful. can shift using geom_jitter(), add points bit random noise added positions prevent happening.may noticed data points now appearing plot twice: outliers plotted black points geom_boxplot(), also plotted geom_jitter(). Since don’t want represent data multiple times form (points), can stop geom_boxplot() plotting . setting outlier.shape argument NA, means outliers don’t shape plotted.Just , can map plot_type color putting inside aes().Notice color points color boxplot lines changed. time specify aes() mapping inside initial ggplot() function, mapping apply geoms.want limit mapping single geom, can put mapping specific geom_ function, like :Now points colored according plot_type, boxplots color. One thing might notice even alpha = 0.2, points obscure parts boxplot. geom_point() layer comes geom_boxplot() layer, means points plotted top boxes. put boxplots top, switch order layers:Now opposite problem! white fill boxplots completely obscures points. address problem, can remove fill boxplots altogether, leaving black lines. , set fill NA:Now can see raw data boxplots top.","code":"\nggplot(data = complete_old, mapping = aes(x = plot_type, y = hindfoot_length)) +\n  geom_boxplot() +\n  geom_point(alpha = 0.2)\nggplot(data = complete_old, mapping = aes(x = plot_type, y = hindfoot_length)) +\n  geom_boxplot() +\n  geom_jitter(alpha = 0.2)\nggplot(data = complete_old, mapping = aes(x = plot_type, y = hindfoot_length)) +\n  geom_boxplot(outlier.shape = NA) +\n  geom_jitter(alpha = 0.2)\nggplot(data = complete_old, mapping = aes(x = plot_type, y = hindfoot_length, color = plot_type)) +\n  geom_boxplot(outlier.shape = NA) +\n  geom_jitter(alpha = 0.2)\nggplot(data = complete_old, mapping = aes(x = plot_type, y = hindfoot_length)) +\n  geom_boxplot(outlier.shape = NA) +\n  geom_jitter(aes(color = plot_type), alpha = 0.2)\nggplot(data = complete_old, mapping = aes(x = plot_type, y = hindfoot_length)) +\n  geom_jitter(aes(color = plot_type), alpha = 0.2) +\n  geom_boxplot(outlier.shape = NA)\nggplot(data = complete_old, mapping = aes(x = plot_type, y = hindfoot_length)) +\n  geom_jitter(aes(color = plot_type), alpha = 0.2) +\n  geom_boxplot(outlier.shape = NA, fill = NA)"},{"path":"data-visualization-with-ggplot2.html","id":"challenge-2-change-geoms","chapter":"2 Data Visualization with ggplot2","heading":"2.9 Challenge 2: Change geoms","text":"Violin plots similar boxplots- try making one using plot_type hindfoot_length x y variables. Remember geom functions start geom_, followed type geom.might also place test search engine skills. often useful search R package_name stuff want search. example might search R ggplot2 violin plot.Solution. extra challenge, , make color points outlines violins vary plot_type, set fill violins white. Try playing order layers see looks best.Solution. ","code":"\nggplot(data = complete_old, \n       mapping = aes(x = plot_type, \n                     y = hindfoot_length,\n                     color = plot_type)) +\n  geom_jitter(alpha = 0.2) +\n  geom_violin(fill = \"white\")\nggplot(data = complete_old, \n       mapping = aes(x = plot_type, \n                     y = hindfoot_length,\n                     color = plot_type)) +\n  geom_jitter(alpha = 0.2) +\n  geom_violin(fill = \"white\")"},{"path":"data-visualization-with-ggplot2.html","id":"changing-themes","chapter":"2 Data Visualization with ggplot2","heading":"2.10 Changing themes","text":"far ’ve changing appearance parts plot related data geom_ functions, can also change many non-data components plot.point, pretty happy basic layout plot, can assign plot named object. using assignment arrow <-. taking result code right side arrow, assigning object whose name left side arrow.create object called myplot. run name ggplot2 object, show plot, just like ran code .process assigning something object specific ggplot2, rather general feature R. using lot rest lesson. can now work myplot object block ggplot2 code, means can use + add new components .can change overall appearance using theme_ functions. Let’s try black--white theme adding theme_bw() plot:can see, number parts plot changed. theme_ functions usually control many aspects plot’s appearance , sake convenience. individually change parts plot, can use theme() function, can take many different arguments change things text, grid lines, background color, . Let’s try changing size text axis titles. can specifying axis.title element_text() size set 14.Another change might want make remove vertical grid lines. Since x axis categorical, grid lines aren’t useful. , inside theme(), change panel.grid.major.x element_blank().Another useful change might remove color legend, since information already x axis. one, set legend.position “none”.many possible arguments theme() function, can sometimes hard find right one. tips figuring modify plot element:type theme(), put cursor parentheses, hit Tab bring list arguments\ncan scroll arguments, start typing, shorten list potential matches\ncan scroll arguments, start typing, shorten list potential matcheslike many things tidyverse, similar argument start similar names\naxis, legend, panel, plot, strip arguments\naxis, legend, panel, plot, strip argumentsarguments hierarchy\ntext controls text whole plot\naxis.title controls text axis titles\naxis.title.x controls text x axis title\ntext controls text whole plotaxis.title controls text axis titlesaxis.title.x controls text x axis titleYou may noticed used 3 different approaches getting rid something ggplot:outlier.shape = NA remove outliers boxplotpanel.grid.major.x = element_blank() remove x grid lineslegend.position = \"none\" remove legendWhy many ways seems like thing?? common frustration working R, programming language. couple reasons :Different people contribute different packages functions, may choose things differently.Code may appear thing, details actually quite different. inner workings ggplot2 actually quite complex, since turns making plots complicated process! , things seem (removing parts plot), may actually operating different components stages final plot.Developing packages highly iterative process, sometimes things change. However, changing much stuff can make old code break. Let’s say removing legend introduced feature ggplot2, lot time passed someone added feature letting remove outliers geom_boxplot(). Changing way remove legend, ’s boxplot approach, break code written meantime, developers may opt keep old approach place.","code":"\nmyplot <- ggplot(data = complete_old, mapping = aes(x = plot_type, y = hindfoot_length)) +\n  geom_jitter(aes(color = plot_type), alpha = 0.2) +\n  geom_boxplot(outlier.shape = NA, fill = NA)\n\nmyplot## Warning: Removed 2733 rows containing non-finite outside the scale range\n## (`stat_boxplot()`).## Warning: Removed 2733 rows containing missing values or values outside the\n## scale range (`geom_point()`).\nmyplot + theme_bw()\nmyplot +\n  theme_bw() +\n  theme(axis.title = element_text(size = 14))\nmyplot +\n  theme_bw() +\n  theme(axis.title = element_text(size = 14), \n        panel.grid.major.x = element_blank())\nmyplot +\n  theme_bw() +\n  theme(axis.title = element_text(size = 14), \n        panel.grid.major.x = element_blank(), \n        legend.position = \"none\")"},{"path":"data-visualization-with-ggplot2.html","id":"changing-labels","chapter":"2 Data Visualization with ggplot2","heading":"2.11 Changing labels","text":"plot really shaping now. However, probably want make axis titles nicer, perhaps add main title plot. can using labs() function:removed legend plot, can also change titles various legends using labs(). example, labs(color = \"Plot type\") change title color scale legend “Plot type”.","code":"\nmyplot +\n  theme_bw() +\n  theme(axis.title = element_text(size = 14), \n        legend.position = \"none\") +\n  labs(title = \"Rodent size by plot type\",\n       x = \"Plot type\",\n       y = \"Hindfoot length (mm)\")"},{"path":"data-visualization-with-ggplot2.html","id":"challenge-3-customizing-a-plot","chapter":"2 Data Visualization with ggplot2","heading":"2.12 Challenge 3: Customizing a plot","text":"Modify previous plot adding descriptive subtitle. Increase font size plot title make bold.Hint: “bold” referred font “face”Solution. ","code":"\nmyplot +\n  theme_bw() +\n  theme(axis.title = element_text(size = 14), legend.position = \"none\",\n        plot.title = element_text(face = \"bold\", size = 20)) +\n  labs(title = \"Rodent size by plot type\",\n       subtitle = \"Long-term dataset from Portal, AZ\",\n       x = \"Plot type\",\n       y = \"Hindfoot length (mm)\")"},{"path":"data-visualization-with-ggplot2.html","id":"faceting","chapter":"2 Data Visualization with ggplot2","heading":"2.13 Faceting","text":"One powerful features ggplot ability quickly split plot multiple smaller plots based categorical variable, called faceting.far ’ve mapped variables x axis, y axis, color, trying add 4th variable becomes difficult. Changing shape point might work, categories, even , can hard tell differences shapes small points.Instead cramming one variable single plot, use facet_wrap() function generate series smaller plots, split sex. also use ncol specify want arranged single column:Faceting comes handy many scenarios. can useful :categorical variable many levels differentiate color (dataset 20 countries)data overlap heavily, obscuring categoriesyou want show 3 variables onceyou want see category isolation allowing general comparisons categories","code":"\nmyplot +\n  theme_bw() +\n  theme(axis.title = element_text(size = 14), \n        legend.position = \"none\", \n        panel.grid.major.x = element_blank()) +\n  labs(title = \"Rodent size by plot type\",\n       x = \"Plot type\",\n       y = \"Hindfoot length (mm)\",\n       color = \"Plot type\") +\n  facet_wrap(vars(sex), ncol = 1)"},{"path":"data-visualization-with-ggplot2.html","id":"exporting-plots","chapter":"2 Data Visualization with ggplot2","heading":"2.14 Exporting plots","text":"happy final plot, can assign whole thing new object, can call finalplot., can run ggsave() save plot. first argument give path file want save, including correct file extension. code make image called rodent_size_plots.jpg images/ folder current project. making .jpg, can save .pdf, .tiff, file formats. Next, tell name plot object want save. can also specify things like width height plot inches.","code":"\nfinalplot <- myplot +\n  theme_bw() +\n  theme(axis.title = element_text(size = 14), \n        legend.position = \"none\", \n        panel.grid.major.x = element_blank()) +\n  labs(title = \"Rodent size by plot type\",\n       x = \"Plot type\",\n       y = \"Hindfoot length (mm)\",\n       color = \"Plot type\") +\n  facet_wrap(vars(sex), ncol = 1)\n\n\n\ncomplete_old %>%\nfilter(sex %in% c(\"M\", \"F\")) %>%\nmutate(sex = case_when(sex == \"M\" ~ \"Male\", TRUE ~ \"Female\")) %>%\nggplot(mapping = aes(x = plot_type, y = hindfoot_length)) +\n  geom_jitter(aes(color = plot_type), alpha = 0.2) +\n  geom_boxplot(outlier.shape = NA, fill = NA) +\n  theme_bw() +\n  theme(axis.title = element_text(size = 14), \n        legend.position = \"none\", \n        panel.grid.major.x = element_blank()) +\n  labs(title = \"Rodent size by plot type\",\n       x = \"Plot type\",\n       y = \"Hindfoot length (mm)\",\n       color = \"Plot type\") +\n  facet_wrap(vars(sex), ncol = 1)## Warning: Removed 1460 rows containing non-finite outside the scale range\n## (`stat_boxplot()`).## Warning: Removed 1460 rows containing missing values or values outside the\n## scale range (`geom_point()`).\nggsave(filename = \"images/rodent_size_plots.jpg\",\n       height = 6, width = 8)"},{"path":"data-visualization-with-ggplot2.html","id":"challenge-4-make-your-own-plot","chapter":"2 Data Visualization with ggplot2","heading":"2.15 Challenge 4: Make your own plot","text":"Try making plot! can run str(complete_old) ?complete_old explore variables might use new plot. Feel free use variables already seen, haven’t explored yet.couple ideas get started:make histogram one numeric variablestry using different color scale_try changing size points thickness lines geom","code":""},{"path":"exploring-and-understanding-data.html","id":"exploring-and-understanding-data","chapter":"3 Exploring and understanding data","heading":"3 Exploring and understanding data","text":"Authors: Data Carpentry contributors - Copyright (c) Data Carpentry; Ed Stowe (adaptation USACE)Last update: 2025-02-03Acknowledgements: bulk section adapted Data Carpentry course Data Analysis Visualization R Ecologists licensed CC-4.0 authorsCoercion something often intentionally; rather, combining vectors reading data R, stray character missed may change entire numeric vector character vector. good idea check class() results frequently, particularly running confusing error messages.Understanding ’s going help avoid lot confusion working R. assign something object, first thing happens righthand side gets evaluated. thing happens run something console: type x console hit Enter, R returns value x. first ran line y <- x, x first gets evaluated value 5, gets assigned y. objects x y actually linked way, change value x 10, y unaffected.also means can run multiple nested operations, store intermediate values separate objects, overwrite values:naming objects R, common naming rules conventions:make names clear without long\nwkg probably short\nweight_in_kilograms probably long\nweight_kg good\nwkg probably shortweight_in_kilograms probably longweight_kg goodnames start numbernames case sensitiveyou use names fundamental functions R, like , else, \ngeneral, avoid using names common functions like c, mean, etc.\ngeneral, avoid using names common functions like c, mean, etc.avoid dots . names, special meaning R, may confusing otherstwo common formats snake_case camelCasebe consistent, least within script, ideally within whole projectyou can use style guide like Google’s \ntidyverse’sfunctions like head(), str(), summary() useful exploring data.framesmost things R vectors, vectors stitched together, functionsmake sure use class() check vector types, especially using new functionsfactors can useful, behave differently character vectors","code":"\nx <- 5\n\n# first, x gets evaluated to 5\n# then 5/2 gets evaluated to 2.5\n# then sqrt(2.5) is evaluated\nsqrt(x/2)## [1] 1.581139\n# we can also store the evaluated value of x/2 \n# in an object y before passing it to sqrt()\ny <- x/2\n\nsqrt(y)## [1] 1.581139\n# first, the x on the righthand side gets evaluated to 5\n# then 5 gets squared\n# then the resulting value is assigned to the object x\n\nx <- x^2\n\nx## [1] 25"},{"path":"exploring-and-understanding-data.html","id":"learning-objectives-3","chapter":"3 Exploring and understanding data","heading":"3.1 Learning Objectives","text":"Explore structure content data.framesUnderstand vector types missing dataUse vectors function argumentsCreate convert factorsUnderstand R assigns values objects","code":""},{"path":"exploring-and-understanding-data.html","id":"setup-1","chapter":"3 Exploring and understanding data","heading":"3.2 Setup","text":"","code":"\nlibrary(tidyverse)\nlibrary(ratdat)"},{"path":"exploring-and-understanding-data.html","id":"the-data.frame","chapter":"3 Exploring and understanding data","heading":"3.3 The data.frame","text":"just spent quite bit time learning create visualizations complete_old data, talk much complete_old thing . ’s important understand R thinks , represents, stores data order us productive working relationship R.complete_old data stored R data.frame, common way R represents tabular data (data can stored table format, like spreadsheet). can check complete_old using class() function:can view first rows head() function, last rows tail() function:used functions just one argument, object complete_old, didn’t give argument name, like often ggplot2. R, function’s arguments come particular order, put correct order, don’t need name . case, name argument x, can name want, since know ’s first argument, don’t need .learn function, can type ? front name function, bring official documentation function:Function documentation written authors functions, can vary pretty widely style readability. first section, Description, gives concise description function , may always enough. Arguments section defines arguments function usually worth reading thoroughly. Finally, Examples section end often helpful examples can run get sense function .Another great source information package vignettes. Many packages vignettes, like tutorials introduce package, specific functions, general methods. can run vignette(package = \"package_name\") see list vignettes package. name, can run vignette(\"vignette_name\", \"package_name\") view vignette. can also use web browser go https://cran.r-project.org/web/packages/package_name/vignettes/ find list links vignette. packages websites, often nicely formatted vignettes tutorials.Finally, learning search help probably useful skill R user. key skill figuring actually search . ’s often good idea start search R R programming. name package want use, start R package_name.Many answers find website called Stack Overflow, people ask programming questions others provide answers. generally poor form ask duplicate questions, decide post , thorough searching see answered (likely ). decide post question Stack Overflow, help forum, want create reproducible example reprex. asking complicated question requiring data whole bunch code, people probably won’t able willing help . However, can hone specific thing want help , create minimal example using smaller, fake data, much easier others help . search make reproducible example R, find great resources help .arguments optional. example, n argument head() specifies number rows print. defaults 6, can override specifying different number:order correctly, don’t name either:Additionally, name , can put order want:Generally, ’s good practice start required arguments, like data.frame whose rows want see, name optional arguments. ever unsure, never hurts explicitly name argument.Let’s get back investigating complete_old data.frame. can get useful summaries variable using summary() function:, already done, can use str() look structure object:get quite bit useful information . First, told data.frame 16878 observations, rows, 13 variables, columns.Next, get bit information variable, including type (int chr) quick peek first 10 values. might ask $ front variable. $ operator allows us select individual columns data.frame.$ operator also allows use tab-completion quickly select variable want given data.frame. example, get year variable, can type complete_old$ hit Tab. get list variables can move arrow keys. Hit Enter reach year, finish code:get back whole bunch numbers, entries year column printed order.","code":"\nclass(complete_old)## [1] \"tbl_df\"     \"tbl\"        \"data.frame\"\nhead(complete_old)## # A tibble: 6 × 13\n##   record_id month   day  year plot_id species_id sex   hindfoot_length weight\n##       <int> <int> <int> <int>   <int> <chr>      <chr>           <int>  <int>\n## 1         1     7    16  1977       2 NL         M                  32     NA\n## 2         2     7    16  1977       3 NL         M                  33     NA\n## 3         3     7    16  1977       2 DM         F                  37     NA\n## 4         4     7    16  1977       7 DM         M                  36     NA\n## 5         5     7    16  1977       3 DM         M                  35     NA\n## 6         6     7    16  1977       1 PF         M                  14     NA\n## # ℹ 4 more variables: genus <chr>, species <chr>, taxa <chr>, plot_type <chr>\ntail(complete_old)## # A tibble: 6 × 13\n##   record_id month   day  year plot_id species_id sex   hindfoot_length weight\n##       <int> <int> <int> <int>   <int> <chr>      <chr>           <int>  <int>\n## 1     16873    12     5  1989       8 DO         M                  37     51\n## 2     16874    12     5  1989      16 RM         F                  18     15\n## 3     16875    12     5  1989       5 RM         M                  17      9\n## 4     16876    12     5  1989       4 DM         M                  37     31\n## 5     16877    12     5  1989      11 DM         M                  37     50\n## 6     16878    12     5  1989       8 DM         F                  37     42\n## # ℹ 4 more variables: genus <chr>, species <chr>, taxa <chr>, plot_type <chr>\n?head\nhead(complete_old, n = 10)## # A tibble: 10 × 13\n##    record_id month   day  year plot_id species_id sex   hindfoot_length weight\n##        <int> <int> <int> <int>   <int> <chr>      <chr>           <int>  <int>\n##  1         1     7    16  1977       2 NL         M                  32     NA\n##  2         2     7    16  1977       3 NL         M                  33     NA\n##  3         3     7    16  1977       2 DM         F                  37     NA\n##  4         4     7    16  1977       7 DM         M                  36     NA\n##  5         5     7    16  1977       3 DM         M                  35     NA\n##  6         6     7    16  1977       1 PF         M                  14     NA\n##  7         7     7    16  1977       2 PE         F                  NA     NA\n##  8         8     7    16  1977       1 DM         M                  37     NA\n##  9         9     7    16  1977       1 DM         F                  34     NA\n## 10        10     7    16  1977       6 PF         F                  20     NA\n## # ℹ 4 more variables: genus <chr>, species <chr>, taxa <chr>, plot_type <chr>\nhead(complete_old, 10)## # A tibble: 10 × 13\n##    record_id month   day  year plot_id species_id sex   hindfoot_length weight\n##        <int> <int> <int> <int>   <int> <chr>      <chr>           <int>  <int>\n##  1         1     7    16  1977       2 NL         M                  32     NA\n##  2         2     7    16  1977       3 NL         M                  33     NA\n##  3         3     7    16  1977       2 DM         F                  37     NA\n##  4         4     7    16  1977       7 DM         M                  36     NA\n##  5         5     7    16  1977       3 DM         M                  35     NA\n##  6         6     7    16  1977       1 PF         M                  14     NA\n##  7         7     7    16  1977       2 PE         F                  NA     NA\n##  8         8     7    16  1977       1 DM         M                  37     NA\n##  9         9     7    16  1977       1 DM         F                  34     NA\n## 10        10     7    16  1977       6 PF         F                  20     NA\n## # ℹ 4 more variables: genus <chr>, species <chr>, taxa <chr>, plot_type <chr>\nhead(n = 10, x = complete_old)## # A tibble: 10 × 13\n##    record_id month   day  year plot_id species_id sex   hindfoot_length weight\n##        <int> <int> <int> <int>   <int> <chr>      <chr>           <int>  <int>\n##  1         1     7    16  1977       2 NL         M                  32     NA\n##  2         2     7    16  1977       3 NL         M                  33     NA\n##  3         3     7    16  1977       2 DM         F                  37     NA\n##  4         4     7    16  1977       7 DM         M                  36     NA\n##  5         5     7    16  1977       3 DM         M                  35     NA\n##  6         6     7    16  1977       1 PF         M                  14     NA\n##  7         7     7    16  1977       2 PE         F                  NA     NA\n##  8         8     7    16  1977       1 DM         M                  37     NA\n##  9         9     7    16  1977       1 DM         F                  34     NA\n## 10        10     7    16  1977       6 PF         F                  20     NA\n## # ℹ 4 more variables: genus <chr>, species <chr>, taxa <chr>, plot_type <chr>\nsummary(complete_old)##    record_id         month             day            year         plot_id     \n##  Min.   :    1   Min.   : 1.000   Min.   : 1.0   Min.   :1977   Min.   : 1.00  \n##  1st Qu.: 4220   1st Qu.: 3.000   1st Qu.: 9.0   1st Qu.:1981   1st Qu.: 5.00  \n##  Median : 8440   Median : 6.000   Median :15.0   Median :1983   Median :11.00  \n##  Mean   : 8440   Mean   : 6.382   Mean   :15.6   Mean   :1984   Mean   :11.47  \n##  3rd Qu.:12659   3rd Qu.: 9.000   3rd Qu.:23.0   3rd Qu.:1987   3rd Qu.:17.00  \n##  Max.   :16878   Max.   :12.000   Max.   :31.0   Max.   :1989   Max.   :24.00  \n##                                                                                \n##   species_id            sex            hindfoot_length     weight      \n##  Length:16878       Length:16878       Min.   : 6.00   Min.   :  4.00  \n##  Class :character   Class :character   1st Qu.:21.00   1st Qu.: 24.00  \n##  Mode  :character   Mode  :character   Median :35.00   Median : 42.00  \n##                                        Mean   :31.98   Mean   : 53.22  \n##                                        3rd Qu.:37.00   3rd Qu.: 53.00  \n##                                        Max.   :70.00   Max.   :278.00  \n##                                        NA's   :2733    NA's   :1692    \n##     genus             species              taxa            plot_type        \n##  Length:16878       Length:16878       Length:16878       Length:16878      \n##  Class :character   Class :character   Class :character   Class :character  \n##  Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n##                                                                             \n##                                                                             \n##                                                                             \n## \nstr(complete_old)## tibble [16,878 × 13] (S3: tbl_df/tbl/data.frame)\n##  $ record_id      : int [1:16878] 1 2 3 4 5 6 7 8 9 10 ...\n##  $ month          : int [1:16878] 7 7 7 7 7 7 7 7 7 7 ...\n##  $ day            : int [1:16878] 16 16 16 16 16 16 16 16 16 16 ...\n##  $ year           : int [1:16878] 1977 1977 1977 1977 1977 1977 1977 1977 1977 1977 ...\n##  $ plot_id        : int [1:16878] 2 3 2 7 3 1 2 1 1 6 ...\n##  $ species_id     : chr [1:16878] \"NL\" \"NL\" \"DM\" \"DM\" ...\n##  $ sex            : chr [1:16878] \"M\" \"M\" \"F\" \"M\" ...\n##  $ hindfoot_length: int [1:16878] 32 33 37 36 35 14 NA 37 34 20 ...\n##  $ weight         : int [1:16878] NA NA NA NA NA NA NA NA NA NA ...\n##  $ genus          : chr [1:16878] \"Neotoma\" \"Neotoma\" \"Dipodomys\" \"Dipodomys\" ...\n##  $ species        : chr [1:16878] \"albigula\" \"albigula\" \"merriami\" \"merriami\" ...\n##  $ taxa           : chr [1:16878] \"Rodent\" \"Rodent\" \"Rodent\" \"Rodent\" ...\n##  $ plot_type      : chr [1:16878] \"Control\" \"Long-term Krat Exclosure\" \"Control\" \"Rodent Exclosure\" ...\ncomplete_old$year##   [1] 1977 1977 1977 1977 1977 1977 1977 1977 1977 1977 1977 1977 1977 1977 1977\n##  [16] 1977 1977 1977 1977 1977 1977 1977 1977 1977 1977 1977 1977 1977 1977 1977\n##  [31] 1977 1977 1977 1977 1977 1977 1977 1977 1977 1977 1977 1977 1977 1977 1977\n##  [46] 1977 1977 1977 1977 1977 1977 1977 1977 1977 1977 1977 1977 1977 1977 1977\n##  [61] 1977 1977 1977 1977 1977 1977 1977 1977 1977 1977 1977 1977 1977 1977 1977\n##  [76] 1977 1977 1977 1977 1977 1977 1977 1977 1977 1977 1977 1977 1977 1977 1977\n##  [91] 1977 1977 1977 1977 1977 1977 1977 1977 1977 1977\n##  [ reached getOption(\"max.print\") -- omitted 16778 entries ]"},{"path":"exploring-and-understanding-data.html","id":"vectors-the-building-block-of-data","chapter":"3 Exploring and understanding data","heading":"3.4 Vectors: the building block of data","text":"might noticed last result looked different printed complete_old data.frame . ’s data.frame, vector. vector 1-dimensional series values, case vector numbers representing years.Data.frames made vectors; column data.frame vector. Vectors basic building blocks data R. Basically, everything R vector, bunch vectors stitched together way, function. Understanding vectors work crucial understanding R treats data, spend time learning .4 main types vectors (also known atomic vectors):\"character\" strings characters, like genus sex columns. entry character vector wrapped quotes. programming languages, type data may referred “strings”.\"character\" strings characters, like genus sex columns. entry character vector wrapped quotes. programming languages, type data may referred “strings”.\"integer\" integers. numeric values complete_old integers. may sometimes see integers represented like 2L 20L. L indicates R integer, instead next data type, \"numeric\".\"integer\" integers. numeric values complete_old integers. may sometimes see integers represented like 2L 20L. L indicates R integer, instead next data type, \"numeric\".\"numeric\", aka \"double\", vectors can contain numbers including decimals. languages may refer “float” “floating point” numbers.\"numeric\", aka \"double\", vectors can contain numbers including decimals. languages may refer “float” “floating point” numbers.\"logical\" TRUE FALSE, can also represented T F. contexts, may referred “Boolean” data.\"logical\" TRUE FALSE, can also represented T F. contexts, may referred “Boolean” data.Vectors can single type. Since column data.frame vector, means accidental character following number, like 29, can change type whole vector. Mixing vector types one common mistakes R, can tricky figure . ’s often useful check types vectors.create vector scratch, can use c() function, putting values inside, separated commas.can see, values get printed console, just like complete_old$year. store vector can continue work , need assign object.can check kind object num class() function.see num numeric vector.Let’s try making character vector:Remember entry, like \"apple\", needs surrounded quotes, entries separated commas. something like \"apple, pear, grape\", single entry containing whole string.Finally, let’s make logical vector:","code":"\nc(1, 2, 5, 12, 4)## [1]  1  2  5 12  4\nnum <- c(1, 2, 5, 12, 4)\nclass(num)## [1] \"numeric\"\nchar <- c(\"apple\", \"pear\", \"grape\")\nclass(char)## [1] \"character\"\nlogi <- c(TRUE, FALSE, TRUE, TRUE)\nclass(logi)## [1] \"logical\""},{"path":"exploring-and-understanding-data.html","id":"challenge-1-coercion","chapter":"3 Exploring and understanding data","heading":"3.5 Challenge 1: Coercion","text":"Since vectors can hold one type data, something done try combine different types data one vector.type vectors ? Try guess without running code first, run code use class() verify answers.Solution. R automatically convert values vector type, process called coercion.many values combined_logical \"TRUE\" (character)?Solution. one value \"TRUE\". Coercion happens vector created, TRUE num_logi becomes 1, TRUE char_logi becomes \"TRUE\". two vectors combined, R doesn’t remember 1 num_logi used TRUE, just coerce 1 \"1\".Now ’ve seen examples coercion, might started see rules types get converted. hierarchy coercion. Can draw diagram represents hierarchy types get converted types?Solution. logical → integer → numeric → characterLogical vectors can take two values: TRUE FALSE. Integer vectors can contain integers, TRUE FALSE can coerced 1 0. Numeric vectors can contain numbers decimals, integers can coerced , say, 6 6.0 (though R still display numeric 6 6.). Finally, string characters can represented character vector, types can coerced character vector.","code":"\nnum_logi <- c(1, 4, 6, TRUE)\nnum_char <- c(1, 3, \"10\", 6)\nchar_logi <- c(\"a\", \"b\", TRUE)\n\n\ntricky <- c(\"a\", \"b\", \"1\", FALSE)\nclass(num_logi)## [1] \"numeric\"\nclass(num_char)## [1] \"character\"\nclass(char_logi)## [1] \"character\"\nclass(tricky)## [1] \"character\"\ncombined_logical <- c(num_logi, char_logi)\ncombined_logical## [1] \"1\"    \"4\"    \"6\"    \"1\"    \"a\"    \"b\"    \"TRUE\"\nclass(combined_logical)## [1] \"character\""},{"path":"exploring-and-understanding-data.html","id":"missing-data","chapter":"3 Exploring and understanding data","heading":"3.6 Missing data","text":"One great things R handles missing data, can tricky programming languages. R represents missing data NA, without quotes, vectors type. Let’s make numeric vector NA value:R doesn’t make assumptions want handle missing data, pass vector numeric function like min(), won’t know , returns NA:good thing, since won’t accidentally forget consider missing data. decide exclude missing values, many basic math functions argument remove :","code":"\nweights <- c(25, 34, 12, NA, 42)\nmin(weights)## [1] NA\nmin(weights, na.rm = TRUE)## [1] 12"},{"path":"exploring-and-understanding-data.html","id":"vectors-as-arguments","chapter":"3 Exploring and understanding data","heading":"3.7 Vectors as arguments","text":"common reason create vector scratch use function argument. quantile() function calculate quantile given vector numeric values. set quantile using probs argument. also need set na.rm = TRUE, since NA values weight column.Now get back 25% quantile value weights. However, often want know one quantile. Luckily, probs argument vectorized, meaning can take whole vector values. Let’s try getting 25%, 50% (median), 75% quantiles .c() function flexible, doesn’t necessarily scale well. want generate long vector scratch, probably don’t want type everything manually. functions can help generate vectors.First, putting : two numbers generate vector integers starting first number ending last. seq() function allows generate similar sequences, changing amount.Finally, rep() function allows repeat value, even whole vector, many times want, works type vector.","code":"\nquantile(complete_old$weight, probs = 0.25, na.rm = TRUE)## 25% \n##  24\nquantile(complete_old$weight, probs = c(0.25, 0.5, 0.75), na.rm = TRUE)## 25% 50% 75% \n##  24  42  53\n# generates a sequence of integers\n1:10##  [1]  1  2  3  4  5  6  7  8  9 10\n# with seq() you can generate sequences with a combination of:\n# from: starting value\n# to: ending value\n# by: how much should each entry increase\n# length.out: how long should the resulting vector be\nseq(from = 0, to = 1, by = 0.1)##  [1] 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0\nseq(from = 0, to = 1, length.out = 50)##  [1] 0.00000000 0.02040816 0.04081633 0.06122449 0.08163265 0.10204082\n##  [7] 0.12244898 0.14285714 0.16326531 0.18367347 0.20408163 0.22448980\n## [13] 0.24489796 0.26530612 0.28571429 0.30612245 0.32653061 0.34693878\n## [19] 0.36734694 0.38775510 0.40816327 0.42857143 0.44897959 0.46938776\n## [25] 0.48979592 0.51020408 0.53061224 0.55102041 0.57142857 0.59183673\n## [31] 0.61224490 0.63265306 0.65306122 0.67346939 0.69387755 0.71428571\n## [37] 0.73469388 0.75510204 0.77551020 0.79591837 0.81632653 0.83673469\n## [43] 0.85714286 0.87755102 0.89795918 0.91836735 0.93877551 0.95918367\n## [49] 0.97959184 1.00000000\nseq(from = 0, by = 0.01, length.out = 20)##  [1] 0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.10 0.11 0.12 0.13 0.14\n## [16] 0.15 0.16 0.17 0.18 0.19\n# repeats \"a\" 12 times\nrep(\"a\", times = 12)##  [1] \"a\" \"a\" \"a\" \"a\" \"a\" \"a\" \"a\" \"a\" \"a\" \"a\" \"a\" \"a\"\n# repeats this whole sequence 4 times\nrep(c(\"a\", \"b\", \"c\"), times = 4)##  [1] \"a\" \"b\" \"c\" \"a\" \"b\" \"c\" \"a\" \"b\" \"c\" \"a\" \"b\" \"c\"\n# repeats each value 4 times\nrep(1:10, each = 4)##  [1]  1  1  1  1  2  2  2  2  3  3  3  3  4  4  4  4  5  5  5  5  6  6  6  6  7\n## [26]  7  7  7  8  8  8  8  9  9  9  9 10 10 10 10"},{"path":"exploring-and-understanding-data.html","id":"challenge-2-creating-sequences","chapter":"3 Exploring and understanding data","heading":"3.8 Challenge 2: Creating sequences","text":"Write code generate following vector:Solution. Calculate quantiles complete_old hindfoot lengths every 5% level (0%, 5%, 10%, 15%, etc.)Solution. ","code":"##  [1] -3 -2 -1  0  1  2  3 -3 -2 -1  0  1  2  3 -3 -2 -1  0  1  2  3\nrep(-3:3, 3)##  [1] -3 -2 -1  0  1  2  3 -3 -2 -1  0  1  2  3 -3 -2 -1  0  1  2  3\n# this also works\nrep(seq(from = -3, to = 3, by = 1), 3)##  [1] -3 -2 -1  0  1  2  3 -3 -2 -1  0  1  2  3 -3 -2 -1  0  1  2  3\n# you might also store the sequence as an intermediate vector\n\nmy_seq <- seq(from = -3, to = 3, by = 1)\nrep(my_seq, 3)##  [1] -3 -2 -1  0  1  2  3 -3 -2 -1  0  1  2  3 -3 -2 -1  0  1  2  3\nquantile(complete_old$hindfoot_length, \n         probs = seq(from = 0, to = 1, by = 0.05),\n         na.rm = T)##   0%   5%  10%  15%  20%  25%  30%  35%  40%  45%  50%  55%  60%  65%  70%  75% \n##    6   16   17   19   20   21   22   31   33   34   35   35   36   36   36   37 \n##  80%  85%  90%  95% 100% \n##   37   39   49   51   70"},{"path":"exploring-and-understanding-data.html","id":"building-with-vectors","chapter":"3 Exploring and understanding data","heading":"3.9 Building with vectors","text":"now seen vectors different forms: columns data.frame single vectors. However, can manipulated lots shapes forms. common forms :matrices\n2-dimensional numeric representations\n2-dimensional numeric representationsarrays\nmany-dimensional numeric\nmany-dimensional numericlists\nlists flexible ways store vectors\nlist can contain vectors many different types lengths\nentry list can another list, lists can get deeply nested\ndata.frame type list column individual vector vector length, since data.frame entry every column row\nlists flexible ways store vectorsa list can contain vectors many different types lengthsan entry list can another list, lists can get deeply nesteda data.frame type list column individual vector vector length, since data.frame entry every column rowfactors\nway represent categorical data\nfactors can ordered unordered\noften look like character vectors, behave differently\nhood, integers character labels, called levels, integer\nway represent categorical datafactors can ordered unorderedthey often look like character vectors, behave differentlyunder hood, integers character labels, called levels, integer","code":""},{"path":"exploring-and-understanding-data.html","id":"factors","chapter":"3 Exploring and understanding data","heading":"3.9.1 Factors","text":"spend bit time talking factors, since often challenging type data work . can create factor scratch putting character vector made using c() factor() function:can inspect levels factor using levels() function:forcats package tidyverse lot convenient functions working factors. show common operations, forcats package many useful functions.general, good practice leave categorical data character vector need use factor. reasons might need factor:Another function requires use factorYou plotting categorical data want control ordering categories plotSince factors can behave differently character vectors, always good idea check type data ’re working . might use new function first time confused results, realize later produced factor output, thought character vector.fairly straightforward convert factor character vector:However, need careful ’re somehow working factor numbers levels:","code":"\nsex <- factor(c(\"male\", \"female\", \"female\", \"male\", \"female\", NA))\n\nsex## [1] male   female female male   female <NA>  \n## Levels: female male\nlevels(sex)## [1] \"female\" \"male\"\nlibrary(forcats)\n\n# change the order of the levels\nfct_relevel(sex, c(\"male\", \"female\"))## [1] male   female female male   female <NA>  \n## Levels: male female\n# change the names of the levels\nfct_recode(sex, \"M\" = \"male\", \"F\" = \"female\")## [1] M    F    F    M    F    <NA>\n## Levels: F M\n# turn NAs into an actual factor level (useful for including NAs in plots)\nfct_na_value_to_level(sex, \"(Missing)\")## [1] male      female    female    male      female    (Missing)\n## Levels: female male (Missing)\nas.character(sex)## [1] \"male\"   \"female\" \"female\" \"male\"   \"female\" NA\nf_num <- factor(c(1990, 1983, 1977, 1998, 1990))\n\n# this will pull out the underlying integers, not the levels\nas.numeric(f_num)## [1] 3 2 1 4 3\n# if we first convert to characters, we can then convert to numbers\nas.numeric(as.character(f_num))## [1] 1990 1983 1977 1998 1990"},{"path":"exploring-and-understanding-data.html","id":"assignment-objects-and-values","chapter":"3 Exploring and understanding data","heading":"3.10 Assignment, objects, and values","text":"’ve already created quite objects R using <- assignment arrow, finer details worth talking . First, let’s start quick challenge.","code":""},{"path":"exploring-and-understanding-data.html","id":"challenge-3-assignments-and-objects","chapter":"3 Exploring and understanding data","heading":"3.11 Challenge 3: Assignments and objects","text":"value y running following code?Solution. ","code":"\nx <- 5\ny <- x\nx <- 10\nx <- 5\ny <- x\nx <- 10\ny## [1] 5"},{"path":"manipulating-tabular-data.html","id":"manipulating-tabular-data","chapter":"4 Manipulating Tabular Data","heading":"4 Manipulating Tabular Data","text":"Authors: Data Carpentry contributors - Copyright (c) Data Carpentry; Ed Stowe (adaptation USACE); Brian Breaker (loop function code)Last update: 2025-02-12Acknowledgements: bulk section adapted Data Carpentry course Data Analysis Visualization R Ecologists licensed CC-4.0 authors","code":""},{"path":"manipulating-tabular-data.html","id":"learning-objectives-4","chapter":"4 Manipulating Tabular Data","heading":"4.1 Learning Objectives","text":"Import CSV data R.Understand difference base R tidyverse approaches.Subset rows columns data.frames.Use pipes link steps together pipelines.Create new data.frame columns using existing columns.Utilize concept split-apply-combine data analysis.Reshape data wide long formats.Export data CSV file.","code":"\nlibrary(tidyverse)"},{"path":"manipulating-tabular-data.html","id":"importing-data","chapter":"4 Manipulating Tabular Data","heading":"4.2 Importing data","text":"point, working complete_old dataframe contained ratdat package. However, typically won’t access data R package; much common access data files stored somewhere computer. going download CSV file containing surveys data computer, read R.Click link download file: https://datacarpentry.org/R-ecology-lesson/data/cleaned/surveys_complete_77_89.csv.prompted save file computer somewhere. Save inside cleaned data folder, data folder R-Ecology-Workshop folder. ’s inside project, able point R towards .","code":""},{"path":"manipulating-tabular-data.html","id":"file-paths","chapter":"4 Manipulating Tabular Data","heading":"4.2.0.1 File paths","text":"reference files R script, need give R precise instructions files . using something called file path. looks something like : \"Documents/Manuscripts/Chapter_2.txt\". path tell computer get whatever folder contains Documents folder way .txt file.two kinds paths: absolute relative. Absolute paths specific particular computer, whereas relative paths relative certain folder. keeping work R-Ecology-Workshop folder, paths can relative folder.Now, let’s read CSV file R store object named surveys. use read_csv function tidyverse’s readr package, argument give relative path CSV file.Typing paths can error prone, can utilize keyboard shortcut. Inside parentheses read_csv(), type pair quotes put cursor . hit Tab. small menu showing folders files show . can use ↑ ↓ keys move options, start typing narrow . can hit Enter select file folder, hit Tab continue building file path. might take bit getting used , get hang , speed writing file paths reduce number mistakes make.may noticed bit feedback R ran last line code. got useful information CSV file read . can see:number rows columnsthe delimiter file, values separated, comma \",\"set columns parsed various vector types\nfile 6 character columns 7 numeric columns\ncan see names columns type\nfile 6 character columns 7 numeric columnswe can see names columns typeWhen working output new function, ’s often good idea check class():Whoa! thing? multiple classes? Well, ’s called tibble, tidyverse version data.frame. data.frame, added perks. prints little nicely, highlights NA values negative values red, generally communicate (terms warnings errors, good thing).tidyverse vs. base RAs begin delve deeply tidyverse, briefly pause mention reasons focusing tidyverse set tools. R, often many ways get job done, approaches can accomplish tasks similar tidyverse.phrase base R used refer approaches utilize functions contained R’s default packages. already used base R functions, str(), head(), mean(), using scattered throughout lesson. However, key base R approaches teaching. include square bracket subsetting base plotting. may come across code written people looks like surveys[1:10, 2] plot(surveys$weight, surveys$hindfoot_length), base R commands. ’re interested learning approaches, can check Carpentries lessons like Software Carpentry Programming R lesson.choose teach tidyverse set packages share similar syntax philosophy, making consistent producing highly readable code. also flexible powerful, growing number packages designed according similar principles work well rest packages. tidyverse packages tend clear documentation wide array learning materials tend written novice users mind. Finally, tidyverse continued grow, strong support RStudio, implies approaches relevant future.","code":"\nsurveys <- read_csv(\"data/surveys_complete_77_89.csv\")## Rows: 16878 Columns: 13\n## ── Column specification ────────────────────────────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## chr (6): species_id, sex, genus, species, taxa, plot_type\n## dbl (7): record_id, month, day, year, plot_id, hindfoot_length, weight\n## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nclass(surveys)## [1] \"spec_tbl_df\" \"tbl_df\"      \"tbl\"         \"data.frame\""},{"path":"manipulating-tabular-data.html","id":"manipulating-data","chapter":"4 Manipulating Tabular Data","heading":"4.3 Manipulating data","text":"One important skills working data R ability manipulate, modify, reshape data. dplyr tidyr packages tidyverse provide series powerful functions many common data manipulation tasks.’ll start two commonly used dplyr functions: select(), selects certain columns data.frame, filter(), filters rows according certain criteria.select() filter(), can hard remember operates columns operates rows. select() c columns filter() r rows.","code":""},{"path":"manipulating-tabular-data.html","id":"select","chapter":"4 Manipulating Tabular Data","heading":"4.3.0.1 select()","text":"use select() function, first argument name data.frame, rest arguments unquoted names columns want:columns arranged order specified inside select().select columns except specific columns, put - front column want exclude:select() also works numeric vectors order columns. select 3rd, 4th, 5th, 10th columns, run following code:careful using method, since less explicit columns want. However, can useful data.frame many columns don’t want type many names.Finally, can select columns based whether match certain criteria using () function. want numeric columns, can ask select columns class numeric:Instead giving names positions columns, instead pass () function name another function inside , case .numeric(), get columns function returns TRUE.can use select columns NA values :","code":"\nselect(surveys, plot_id, species_id, hindfoot_length)## # A tibble: 16,878 × 3\n##    plot_id species_id hindfoot_length\n##      <dbl> <chr>                <dbl>\n##  1       2 NL                      32\n##  2       3 NL                      33\n##  3       2 DM                      37\n##  4       7 DM                      36\n##  5       3 DM                      35\n##  6       1 PF                      14\n##  7       2 PE                      NA\n##  8       1 DM                      37\n##  9       1 DM                      34\n## 10       6 PF                      20\n## # ℹ 16,868 more rows\nselect(surveys, -record_id, -year)## # A tibble: 16,878 × 11\n##    month   day plot_id species_id sex   hindfoot_length weight genus     species\n##    <dbl> <dbl>   <dbl> <chr>      <chr>           <dbl>  <dbl> <chr>     <chr>  \n##  1     7    16       2 NL         M                  32     NA Neotoma   albigu…\n##  2     7    16       3 NL         M                  33     NA Neotoma   albigu…\n##  3     7    16       2 DM         F                  37     NA Dipodomys merria…\n##  4     7    16       7 DM         M                  36     NA Dipodomys merria…\n##  5     7    16       3 DM         M                  35     NA Dipodomys merria…\n##  6     7    16       1 PF         M                  14     NA Perognat… flavus \n##  7     7    16       2 PE         F                  NA     NA Peromysc… eremic…\n##  8     7    16       1 DM         M                  37     NA Dipodomys merria…\n##  9     7    16       1 DM         F                  34     NA Dipodomys merria…\n## 10     7    16       6 PF         F                  20     NA Perognat… flavus \n## # ℹ 16,868 more rows\n## # ℹ 2 more variables: taxa <chr>, plot_type <chr>\nselect(surveys, c(3:5, 10))## # A tibble: 16,878 × 4\n##      day  year plot_id genus      \n##    <dbl> <dbl>   <dbl> <chr>      \n##  1    16  1977       2 Neotoma    \n##  2    16  1977       3 Neotoma    \n##  3    16  1977       2 Dipodomys  \n##  4    16  1977       7 Dipodomys  \n##  5    16  1977       3 Dipodomys  \n##  6    16  1977       1 Perognathus\n##  7    16  1977       2 Peromyscus \n##  8    16  1977       1 Dipodomys  \n##  9    16  1977       1 Dipodomys  \n## 10    16  1977       6 Perognathus\n## # ℹ 16,868 more rows\nselect(surveys, where(is.numeric))## # A tibble: 16,878 × 7\n##    record_id month   day  year plot_id hindfoot_length weight\n##        <dbl> <dbl> <dbl> <dbl>   <dbl>           <dbl>  <dbl>\n##  1         1     7    16  1977       2              32     NA\n##  2         2     7    16  1977       3              33     NA\n##  3         3     7    16  1977       2              37     NA\n##  4         4     7    16  1977       7              36     NA\n##  5         5     7    16  1977       3              35     NA\n##  6         6     7    16  1977       1              14     NA\n##  7         7     7    16  1977       2              NA     NA\n##  8         8     7    16  1977       1              37     NA\n##  9         9     7    16  1977       1              34     NA\n## 10        10     7    16  1977       6              20     NA\n## # ℹ 16,868 more rows\nselect(surveys, where(anyNA))## # A tibble: 16,878 × 7\n##    species_id sex   hindfoot_length weight genus       species  taxa  \n##    <chr>      <chr>           <dbl>  <dbl> <chr>       <chr>    <chr> \n##  1 NL         M                  32     NA Neotoma     albigula Rodent\n##  2 NL         M                  33     NA Neotoma     albigula Rodent\n##  3 DM         F                  37     NA Dipodomys   merriami Rodent\n##  4 DM         M                  36     NA Dipodomys   merriami Rodent\n##  5 DM         M                  35     NA Dipodomys   merriami Rodent\n##  6 PF         M                  14     NA Perognathus flavus   Rodent\n##  7 PE         F                  NA     NA Peromyscus  eremicus Rodent\n##  8 DM         M                  37     NA Dipodomys   merriami Rodent\n##  9 DM         F                  34     NA Dipodomys   merriami Rodent\n## 10 PF         F                  20     NA Perognathus flavus   Rodent\n## # ℹ 16,868 more rows"},{"path":"manipulating-tabular-data.html","id":"filter","chapter":"4 Manipulating Tabular Data","heading":"4.3.0.2 filter()","text":"filter() function used select rows meet certain criteria. get rows value year equal 1985, run following:== sign means “equal ”. several operators can use: >, >=, <, <=, != (equal ). Another useful operator %%, asks value lefthand side found anywhere vector righthand side. example, get rows specific species_id values, run:can also use multiple conditions one filter() statement. get rows year less equal 1988 whose hindfoot length values NA. ! .na() function means “”.","code":"\nfilter(surveys, year == 1985)## # A tibble: 1,438 × 13\n##    record_id month   day  year plot_id species_id sex   hindfoot_length weight\n##        <dbl> <dbl> <dbl> <dbl>   <dbl> <chr>      <chr>           <dbl>  <dbl>\n##  1      9790     1    19  1985      16 RM         F                  16      4\n##  2      9791     1    19  1985      17 OT         F                  20     16\n##  3      9792     1    19  1985       6 DO         M                  35     48\n##  4      9793     1    19  1985      12 DO         F                  35     40\n##  5      9794     1    19  1985      24 RM         M                  16      4\n##  6      9795     1    19  1985      12 DO         M                  34     48\n##  7      9796     1    19  1985       6 DM         F                  37     35\n##  8      9797     1    19  1985      14 DM         M                  36     45\n##  9      9798     1    19  1985       6 DM         F                  36     38\n## 10      9799     1    19  1985      19 RM         M                  16      4\n## # ℹ 1,428 more rows\n## # ℹ 4 more variables: genus <chr>, species <chr>, taxa <chr>, plot_type <chr>\nfilter(surveys, species_id %in% c(\"RM\", \"DO\"))## # A tibble: 2,835 × 13\n##    record_id month   day  year plot_id species_id sex   hindfoot_length weight\n##        <dbl> <dbl> <dbl> <dbl>   <dbl> <chr>      <chr>           <dbl>  <dbl>\n##  1        68     8    19  1977       8 DO         F                  32     52\n##  2       292    10    17  1977       3 DO         F                  36     33\n##  3       294    10    17  1977       3 DO         F                  37     50\n##  4       311    10    17  1977      19 RM         M                  18     13\n##  5       317    10    17  1977      17 DO         F                  32     48\n##  6       323    10    17  1977      17 DO         F                  33     31\n##  7       337    10    18  1977       8 DO         F                  35     41\n##  8       356    11    12  1977       1 DO         F                  32     44\n##  9       378    11    12  1977       1 DO         M                  33     48\n## 10       397    11    13  1977      17 RM         F                  16      7\n## # ℹ 2,825 more rows\n## # ℹ 4 more variables: genus <chr>, species <chr>, taxa <chr>, plot_type <chr>\nfilter(surveys, year <= 1988 & !is.na(hindfoot_length))## # A tibble: 12,779 × 13\n##    record_id month   day  year plot_id species_id sex   hindfoot_length weight\n##        <dbl> <dbl> <dbl> <dbl>   <dbl> <chr>      <chr>           <dbl>  <dbl>\n##  1         1     7    16  1977       2 NL         M                  32     NA\n##  2         2     7    16  1977       3 NL         M                  33     NA\n##  3         3     7    16  1977       2 DM         F                  37     NA\n##  4         4     7    16  1977       7 DM         M                  36     NA\n##  5         5     7    16  1977       3 DM         M                  35     NA\n##  6         6     7    16  1977       1 PF         M                  14     NA\n##  7         8     7    16  1977       1 DM         M                  37     NA\n##  8         9     7    16  1977       1 DM         F                  34     NA\n##  9        10     7    16  1977       6 PF         F                  20     NA\n## 10        11     7    16  1977       5 DS         F                  53     NA\n## # ℹ 12,769 more rows\n## # ℹ 4 more variables: genus <chr>, species <chr>, taxa <chr>, plot_type <chr>"},{"path":"manipulating-tabular-data.html","id":"challenge-1-filtering-and-selecting","chapter":"4 Manipulating Tabular Data","heading":"4.4 Challenge 1: Filtering and selecting","text":"Use surveys data make data.frame data years 1980 1985.Solution. Use surveys data make data.frame following columns, order: year, month, species_id, plot_id.Solution. ","code":"\nsurveys_filtered <- filter(surveys, year >= 1980 & year <= 1985)\nsurveys_selected <- select(surveys, year, month, species_id, plot_id)"},{"path":"manipulating-tabular-data.html","id":"the-pipe","chapter":"4 Manipulating Tabular Data","heading":"4.5 The pipe: %>%","text":"happens want select() filter() data? couple options. First, use nested functions:R evaluate statements inside . First, select() operate surveys data.frame, removing column day. resulting data.frame used first argument filter(), selects rows month greater equal 7.Nested functions can difficult read functions, nearly impossible many functions done . alternative approach create intermediate objects:approach easier read, since can see steps order, enough steps, left cluttered mess intermediate objects, often confusing names.elegant solution problem operator called pipe, looks like %>%. can insert using keyboard shortcut Shift+Cmd+M (Mac) Shift+Ctrl+M (Windows). ’s use pipe select filter one step:take thing lefthand side insert first argument function righthand side. putting functions onto new line, can build nice, readable pipeline. can useful think little assembly line data. starts top gets piped select() function, comes modified somewhat. gets sent filter() function, modified, final product gets printed console. can also helpful think %>% meaning “”. Since many tidyverse functions verbs names, pipeline can read like sentence.’s worth showing learners can run pipeline without highlighting whole thing. cursor line pipeline, running line run whole thing.can also show highlighting section pipeline, can run first X steps .want store final product object, use assignment arrow start:good approach build pipeline step step prior assignment. add functions pipeline go, results printing console view. ’re satisfied final result, go back add assignment arrow statement start. approach interactive, allowing see results step build pipeline, produces nicely readable code.","code":"\nfilter(select(surveys, -day), month >= 7)## # A tibble: 8,244 × 12\n##    record_id month  year plot_id species_id sex   hindfoot_length weight genus  \n##        <dbl> <dbl> <dbl>   <dbl> <chr>      <chr>           <dbl>  <dbl> <chr>  \n##  1         1     7  1977       2 NL         M                  32     NA Neotoma\n##  2         2     7  1977       3 NL         M                  33     NA Neotoma\n##  3         3     7  1977       2 DM         F                  37     NA Dipodo…\n##  4         4     7  1977       7 DM         M                  36     NA Dipodo…\n##  5         5     7  1977       3 DM         M                  35     NA Dipodo…\n##  6         6     7  1977       1 PF         M                  14     NA Perogn…\n##  7         7     7  1977       2 PE         F                  NA     NA Peromy…\n##  8         8     7  1977       1 DM         M                  37     NA Dipodo…\n##  9         9     7  1977       1 DM         F                  34     NA Dipodo…\n## 10        10     7  1977       6 PF         F                  20     NA Perogn…\n## # ℹ 8,234 more rows\n## # ℹ 3 more variables: species <chr>, taxa <chr>, plot_type <chr>\nsurveys_noday <- select(surveys, -day)\nfilter(surveys_noday, month >= 7)## # A tibble: 8,244 × 12\n##    record_id month  year plot_id species_id sex   hindfoot_length weight genus  \n##        <dbl> <dbl> <dbl>   <dbl> <chr>      <chr>           <dbl>  <dbl> <chr>  \n##  1         1     7  1977       2 NL         M                  32     NA Neotoma\n##  2         2     7  1977       3 NL         M                  33     NA Neotoma\n##  3         3     7  1977       2 DM         F                  37     NA Dipodo…\n##  4         4     7  1977       7 DM         M                  36     NA Dipodo…\n##  5         5     7  1977       3 DM         M                  35     NA Dipodo…\n##  6         6     7  1977       1 PF         M                  14     NA Perogn…\n##  7         7     7  1977       2 PE         F                  NA     NA Peromy…\n##  8         8     7  1977       1 DM         M                  37     NA Dipodo…\n##  9         9     7  1977       1 DM         F                  34     NA Dipodo…\n## 10        10     7  1977       6 PF         F                  20     NA Perogn…\n## # ℹ 8,234 more rows\n## # ℹ 3 more variables: species <chr>, taxa <chr>, plot_type <chr>\nsurveys %>% \n  select(-day) %>% \n  filter(month >= 7)## # A tibble: 8,244 × 12\n##    record_id month  year plot_id species_id sex   hindfoot_length weight genus  \n##        <dbl> <dbl> <dbl>   <dbl> <chr>      <chr>           <dbl>  <dbl> <chr>  \n##  1         1     7  1977       2 NL         M                  32     NA Neotoma\n##  2         2     7  1977       3 NL         M                  33     NA Neotoma\n##  3         3     7  1977       2 DM         F                  37     NA Dipodo…\n##  4         4     7  1977       7 DM         M                  36     NA Dipodo…\n##  5         5     7  1977       3 DM         M                  35     NA Dipodo…\n##  6         6     7  1977       1 PF         M                  14     NA Perogn…\n##  7         7     7  1977       2 PE         F                  NA     NA Peromy…\n##  8         8     7  1977       1 DM         M                  37     NA Dipodo…\n##  9         9     7  1977       1 DM         F                  34     NA Dipodo…\n## 10        10     7  1977       6 PF         F                  20     NA Perogn…\n## # ℹ 8,234 more rows\n## # ℹ 3 more variables: species <chr>, taxa <chr>, plot_type <chr>\nsurveys_sub <- surveys %>% \n  select(-day) %>% \n  filter(month >= 7)"},{"path":"manipulating-tabular-data.html","id":"challenge-2-using-pipes","chapter":"4 Manipulating Tabular Data","heading":"4.6 Challenge 2: Using pipes","text":"Use surveys data make data.frame columns record_id, month, species_id, data year 1988. Use pipe function calls.Solution. Make sure filter() select(). need use year column filtering rows, discarded select() step. also need make sure use == instead = filtering rows year equal 1988.","code":"\nsurveys_1988 <- surveys %>%\n  filter(year == 1988) %>%\n  select(record_id, month, species_id)"},{"path":"manipulating-tabular-data.html","id":"making-new-columns-with-mutate","chapter":"4 Manipulating Tabular Data","heading":"4.7 Making new columns with mutate()","text":"Another common task creating new column based values existing columns. example, add new column weight kilograms instead grams:can create multiple columns one mutate() call, get created order write . means can even reference first new column second new column:can also use multiple columns create single column. example, ’s often good practice keep components date separate columns necessary, ’ve done . programs like Excel can automatic things dates way reproducible sometimes hard notice. However, now working R, can safely put together date column.put together columns something looks like date, can use paste() function, takes arguments items paste together, well argument sep, character used separate items.Since new column gets moved way end, doesn’t end printing . can use relocate() function put year column:Now can see character column contains date string. However, ’s truly date column. Dates type numeric variable defined, ordered scale. turn column proper date, use function tidyverse’s lubridate package, lots useful functions working dates. function ymd() parse date string order year-month-day. Let’s load package use ymd().Now can see date column type date well. example, created column two separate lines mutate(), can combine one:","code":"\nsurveys %>% \n  mutate(weight_kg = weight / 1000)## # A tibble: 16,878 × 14\n##    record_id month   day  year plot_id species_id sex   hindfoot_length weight\n##        <dbl> <dbl> <dbl> <dbl>   <dbl> <chr>      <chr>           <dbl>  <dbl>\n##  1         1     7    16  1977       2 NL         M                  32     NA\n##  2         2     7    16  1977       3 NL         M                  33     NA\n##  3         3     7    16  1977       2 DM         F                  37     NA\n##  4         4     7    16  1977       7 DM         M                  36     NA\n##  5         5     7    16  1977       3 DM         M                  35     NA\n##  6         6     7    16  1977       1 PF         M                  14     NA\n##  7         7     7    16  1977       2 PE         F                  NA     NA\n##  8         8     7    16  1977       1 DM         M                  37     NA\n##  9         9     7    16  1977       1 DM         F                  34     NA\n## 10        10     7    16  1977       6 PF         F                  20     NA\n## # ℹ 16,868 more rows\n## # ℹ 5 more variables: genus <chr>, species <chr>, taxa <chr>, plot_type <chr>,\n## #   weight_kg <dbl>\nsurveys %>% \n  mutate(weight_kg = weight / 1000,\n         weight_lbs = weight_kg * 2.2)## # A tibble: 16,878 × 15\n##    record_id month   day  year plot_id species_id sex   hindfoot_length weight\n##        <dbl> <dbl> <dbl> <dbl>   <dbl> <chr>      <chr>           <dbl>  <dbl>\n##  1         1     7    16  1977       2 NL         M                  32     NA\n##  2         2     7    16  1977       3 NL         M                  33     NA\n##  3         3     7    16  1977       2 DM         F                  37     NA\n##  4         4     7    16  1977       7 DM         M                  36     NA\n##  5         5     7    16  1977       3 DM         M                  35     NA\n##  6         6     7    16  1977       1 PF         M                  14     NA\n##  7         7     7    16  1977       2 PE         F                  NA     NA\n##  8         8     7    16  1977       1 DM         M                  37     NA\n##  9         9     7    16  1977       1 DM         F                  34     NA\n## 10        10     7    16  1977       6 PF         F                  20     NA\n## # ℹ 16,868 more rows\n## # ℹ 6 more variables: genus <chr>, species <chr>, taxa <chr>, plot_type <chr>,\n## #   weight_kg <dbl>, weight_lbs <dbl>\nsurveys %>% \n  mutate(date = paste(year, month, day, sep = \"-\"))## # A tibble: 16,878 × 14\n##    record_id month   day  year plot_id species_id sex   hindfoot_length weight\n##        <dbl> <dbl> <dbl> <dbl>   <dbl> <chr>      <chr>           <dbl>  <dbl>\n##  1         1     7    16  1977       2 NL         M                  32     NA\n##  2         2     7    16  1977       3 NL         M                  33     NA\n##  3         3     7    16  1977       2 DM         F                  37     NA\n##  4         4     7    16  1977       7 DM         M                  36     NA\n##  5         5     7    16  1977       3 DM         M                  35     NA\n##  6         6     7    16  1977       1 PF         M                  14     NA\n##  7         7     7    16  1977       2 PE         F                  NA     NA\n##  8         8     7    16  1977       1 DM         M                  37     NA\n##  9         9     7    16  1977       1 DM         F                  34     NA\n## 10        10     7    16  1977       6 PF         F                  20     NA\n## # ℹ 16,868 more rows\n## # ℹ 5 more variables: genus <chr>, species <chr>, taxa <chr>, plot_type <chr>,\n## #   date <chr>\nsurveys %>% \n  mutate(date = paste(year, month, day, sep = \"-\")) %>% \n  relocate(date, .after = year)## # A tibble: 16,878 × 14\n##    record_id month   day  year date     plot_id species_id sex   hindfoot_length\n##        <dbl> <dbl> <dbl> <dbl> <chr>      <dbl> <chr>      <chr>           <dbl>\n##  1         1     7    16  1977 1977-7-…       2 NL         M                  32\n##  2         2     7    16  1977 1977-7-…       3 NL         M                  33\n##  3         3     7    16  1977 1977-7-…       2 DM         F                  37\n##  4         4     7    16  1977 1977-7-…       7 DM         M                  36\n##  5         5     7    16  1977 1977-7-…       3 DM         M                  35\n##  6         6     7    16  1977 1977-7-…       1 PF         M                  14\n##  7         7     7    16  1977 1977-7-…       2 PE         F                  NA\n##  8         8     7    16  1977 1977-7-…       1 DM         M                  37\n##  9         9     7    16  1977 1977-7-…       1 DM         F                  34\n## 10        10     7    16  1977 1977-7-…       6 PF         F                  20\n## # ℹ 16,868 more rows\n## # ℹ 5 more variables: weight <dbl>, genus <chr>, species <chr>, taxa <chr>,\n## #   plot_type <chr>\nlibrary(lubridate)\n\nsurveys %>% \n  mutate(date = paste(year, month, day, sep = \"-\"),\n         date = ymd(date)) %>% \n  relocate(date, .after = year)## # A tibble: 16,878 × 14\n##    record_id month   day  year date       plot_id species_id sex  \n##        <dbl> <dbl> <dbl> <dbl> <date>       <dbl> <chr>      <chr>\n##  1         1     7    16  1977 1977-07-16       2 NL         M    \n##  2         2     7    16  1977 1977-07-16       3 NL         M    \n##  3         3     7    16  1977 1977-07-16       2 DM         F    \n##  4         4     7    16  1977 1977-07-16       7 DM         M    \n##  5         5     7    16  1977 1977-07-16       3 DM         M    \n##  6         6     7    16  1977 1977-07-16       1 PF         M    \n##  7         7     7    16  1977 1977-07-16       2 PE         F    \n##  8         8     7    16  1977 1977-07-16       1 DM         M    \n##  9         9     7    16  1977 1977-07-16       1 DM         F    \n## 10        10     7    16  1977 1977-07-16       6 PF         F    \n## # ℹ 16,868 more rows\n## # ℹ 6 more variables: hindfoot_length <dbl>, weight <dbl>, genus <chr>,\n## #   species <chr>, taxa <chr>, plot_type <chr>\nsurveys %>% \n  mutate(date = paste(year, month, day, sep = \"-\"),\n         date = as.Date(date)) %>% \n  relocate(date, .after = year)## # A tibble: 16,878 × 14\n##    record_id month   day  year date       plot_id species_id sex  \n##        <dbl> <dbl> <dbl> <dbl> <date>       <dbl> <chr>      <chr>\n##  1         1     7    16  1977 1977-07-16       2 NL         M    \n##  2         2     7    16  1977 1977-07-16       3 NL         M    \n##  3         3     7    16  1977 1977-07-16       2 DM         F    \n##  4         4     7    16  1977 1977-07-16       7 DM         M    \n##  5         5     7    16  1977 1977-07-16       3 DM         M    \n##  6         6     7    16  1977 1977-07-16       1 PF         M    \n##  7         7     7    16  1977 1977-07-16       2 PE         F    \n##  8         8     7    16  1977 1977-07-16       1 DM         M    \n##  9         9     7    16  1977 1977-07-16       1 DM         F    \n## 10        10     7    16  1977 1977-07-16       6 PF         F    \n## # ℹ 16,868 more rows\n## # ℹ 6 more variables: hindfoot_length <dbl>, weight <dbl>, genus <chr>,\n## #   species <chr>, taxa <chr>, plot_type <chr>\n# using nested functions\nsurveys %>% \n  mutate(date = ymd(paste(year, month, day, sep = \"-\"))) %>% \n  relocate(date, .after = year)## # A tibble: 16,878 × 14\n##    record_id month   day  year date       plot_id species_id sex  \n##        <dbl> <dbl> <dbl> <dbl> <date>       <dbl> <chr>      <chr>\n##  1         1     7    16  1977 1977-07-16       2 NL         M    \n##  2         2     7    16  1977 1977-07-16       3 NL         M    \n##  3         3     7    16  1977 1977-07-16       2 DM         F    \n##  4         4     7    16  1977 1977-07-16       7 DM         M    \n##  5         5     7    16  1977 1977-07-16       3 DM         M    \n##  6         6     7    16  1977 1977-07-16       1 PF         M    \n##  7         7     7    16  1977 1977-07-16       2 PE         F    \n##  8         8     7    16  1977 1977-07-16       1 DM         M    \n##  9         9     7    16  1977 1977-07-16       1 DM         F    \n## 10        10     7    16  1977 1977-07-16       6 PF         F    \n## # ℹ 16,868 more rows\n## # ℹ 6 more variables: hindfoot_length <dbl>, weight <dbl>, genus <chr>,\n## #   species <chr>, taxa <chr>, plot_type <chr>\n# using a pipe *inside* mutate()\nsurveys %>% \n  mutate(date = paste(year, month, day, \n                      sep = \"-\") %>% ymd()) %>% \n  relocate(date, .after = year)## # A tibble: 16,878 × 14\n##    record_id month   day  year date       plot_id species_id sex  \n##        <dbl> <dbl> <dbl> <dbl> <date>       <dbl> <chr>      <chr>\n##  1         1     7    16  1977 1977-07-16       2 NL         M    \n##  2         2     7    16  1977 1977-07-16       3 NL         M    \n##  3         3     7    16  1977 1977-07-16       2 DM         F    \n##  4         4     7    16  1977 1977-07-16       7 DM         M    \n##  5         5     7    16  1977 1977-07-16       3 DM         M    \n##  6         6     7    16  1977 1977-07-16       1 PF         M    \n##  7         7     7    16  1977 1977-07-16       2 PE         F    \n##  8         8     7    16  1977 1977-07-16       1 DM         M    \n##  9         9     7    16  1977 1977-07-16       1 DM         F    \n## 10        10     7    16  1977 1977-07-16       6 PF         F    \n## # ℹ 16,868 more rows\n## # ℹ 6 more variables: hindfoot_length <dbl>, weight <dbl>, genus <chr>,\n## #   species <chr>, taxa <chr>, plot_type <chr>"},{"path":"manipulating-tabular-data.html","id":"challenge-3-plotting-date","chapter":"4 Manipulating Tabular Data","heading":"4.8 Challenge 3: Plotting date","text":"ggplot() function takes data first argument, can actually pipe data straight ggplot(). Try building pipeline creates date column plots weight across date.Solution. isn’t necessarily useful plot, learn techniques help produce nice time series plots","code":"\nsurveys %>% \n  mutate(date = ymd(paste(year, month, day, sep = \"-\"))) %>% \n  ggplot(aes(x = date, y = weight)) +\n  geom_jitter(alpha = 0.1)## Warning: Removed 1692 rows containing missing values or values outside the\n## scale range (`geom_point()`)."},{"path":"manipulating-tabular-data.html","id":"the-split-apply-combine-approach","chapter":"4 Manipulating Tabular Data","heading":"4.9 The split-apply-combine approach","text":"Many data analysis tasks can achieved using split-apply-combine approach: split data groups, apply analysis group, combine results way. dplyr convenient functions enable approach, main two group_by() summarize().group_by() takes data.frame name one columns categorical values define groups. summarize() collapses group one-row summary group, giving back data.frame one row per group. syntax summarize() similar mutate(), define new columns based values columns. Let’s try calculating mean weight animals sex.can see mean weight males slightly higher females, animals whose sex unknown much higher weights. probably due small sample size, check sure. Like mutate(), can define multiple columns one summarize() call. function n() count number rows group.often want create groups based multiple columns. example, might interested mean weight every species + sex combination. add another column group_by() call.resulting data.frame much larger, since greater number groups. also see strange value showing mean_weight column: NaN. stands “Number”, often results trying operation vector zero entries. can vector zero entries? Well, particular group (like AB species ID + NA sex group) NA values weight, na.rm = T argument mean() remove values prior calculating mean. result value NaN. Since particularly interested values, let’s add step pipeline remove rows weight NA steps. means groups NA values disappear data.frame formally create groups group_by().looks better! ’s often useful take look results order, like lowest mean weight highest. can use arrange() function :want reverse order, can wrap column name desc():may seen several messages saying summarise() grouped output 'species_id'. can override using .groups argument. warning resulting data.frame retained group structure, means subsequent operations data.frame happen group level. look resulting data.frame printed console, see lines:tell us data.frame 46 rows, 4 columns, group variable species_id, 18 groups. see something similar use group_by() alone:get back entire surveys data.frame, grouping variables added: 67 groups species_id + sex combinations. Groups often maintained throughout pipeline, assign resulting data.frame new object, also groups. can lead confusing results forget grouping want carry operations whole data.frame, group. Therefore, good habit remove groups end pipeline containing group_by():Now data.frame just says # tibble: 46 × 4 top, groups.common want get one-row-per-group summary summarise() provides, times want calculate per-group value keep rows data.frame. example, might want know mean weight species ID + sex combination, might want know far mean value observation group . , can use group_by() mutate() together:Since get columns back, new columns end don’t print console. Let’s use select() just look columns interest. Inside select() can use contains() function get column containing word “weight” name:happens group_by() + mutate() combination similar using summarize(): group, mean weight calculated. However, instead reporting one row per group, mean weight group added row group. row group (like DM species ID + M sex), see value mean_weight.","code":"\nsurveys %>% \n  group_by(sex) %>% \n  summarize(mean_weight = mean(weight, na.rm = T))## # A tibble: 3 × 2\n##   sex   mean_weight\n##   <chr>       <dbl>\n## 1 F            53.1\n## 2 M            53.2\n## 3 <NA>         74.0\nsurveys %>% \n  group_by(sex) %>% \n  summarize(mean_weight = mean(weight, na.rm = T),\n            n = n())## # A tibble: 3 × 3\n##   sex   mean_weight     n\n##   <chr>       <dbl> <int>\n## 1 F            53.1  7318\n## 2 M            53.2  8260\n## 3 <NA>         74.0  1300\nsurveys %>% \n  group_by(species_id, sex) %>% \n  summarize(mean_weight = mean(weight, na.rm = T),\n            n = n())## `summarise()` has grouped output by 'species_id'. You can override using the\n## `.groups` argument.## # A tibble: 67 × 4\n## # Groups:   species_id [36]\n##    species_id sex   mean_weight     n\n##    <chr>      <chr>       <dbl> <int>\n##  1 AB         <NA>        NaN     223\n##  2 AH         <NA>        NaN     136\n##  3 BA         M             7       3\n##  4 CB         <NA>        NaN      23\n##  5 CM         <NA>        NaN      13\n##  6 CQ         <NA>        NaN      16\n##  7 CS         <NA>        NaN       1\n##  8 CV         <NA>        NaN       1\n##  9 DM         F            40.7  2522\n## 10 DM         M            44.0  3108\n## # ℹ 57 more rows\nsurveys %>% \n  filter(!is.na(weight)) %>% \n  group_by(species_id, sex) %>% \n  summarize(mean_weight = mean(weight),\n            n = n())## `summarise()` has grouped output by 'species_id'. You can override using the\n## `.groups` argument.## # A tibble: 46 × 4\n## # Groups:   species_id [18]\n##    species_id sex   mean_weight     n\n##    <chr>      <chr>       <dbl> <int>\n##  1 BA         M             7       3\n##  2 DM         F            40.7  2460\n##  3 DM         M            44.0  3013\n##  4 DM         <NA>         37       8\n##  5 DO         F            48.4   679\n##  6 DO         M            49.3   748\n##  7 DO         <NA>         44       1\n##  8 DS         F           118.   1055\n##  9 DS         M           123.   1184\n## 10 DS         <NA>        121.     16\n## # ℹ 36 more rows\nsurveys %>% \n  filter(!is.na(weight)) %>% \n  group_by(species_id, sex) %>% \n  summarize(mean_weight = mean(weight),\n            n = n()) %>% \n  arrange(mean_weight)## `summarise()` has grouped output by 'species_id'. You can override using the\n## `.groups` argument.## # A tibble: 46 × 4\n## # Groups:   species_id [18]\n##    species_id sex   mean_weight     n\n##    <chr>      <chr>       <dbl> <int>\n##  1 PF         <NA>         6        2\n##  2 BA         M            7        3\n##  3 PF         F            7.09   215\n##  4 PF         M            7.10   296\n##  5 RM         M            9.92   678\n##  6 RM         <NA>        10.4      7\n##  7 RM         F           10.7    629\n##  8 RF         M           12.4     16\n##  9 RF         F           13.7     46\n## 10 PP         <NA>        15        2\n## # ℹ 36 more rows\nsurveys %>% \n  filter(!is.na(weight)) %>% \n  group_by(species_id, sex) %>% \n  summarize(mean_weight = mean(weight),\n            n = n()) %>% \n  arrange(desc(mean_weight))## `summarise()` has grouped output by 'species_id'. You can override using the\n## `.groups` argument.## # A tibble: 46 × 4\n## # Groups:   species_id [18]\n##    species_id sex   mean_weight     n\n##    <chr>      <chr>       <dbl> <int>\n##  1 NL         M           168.    355\n##  2 NL         <NA>        164.      9\n##  3 NL         F           151.    460\n##  4 SS         M           130       1\n##  5 DS         M           123.   1184\n##  6 DS         <NA>        121.     16\n##  7 DS         F           118.   1055\n##  8 SH         F            79.2    61\n##  9 SH         M            67.6    34\n## 10 SF         F            58.3     3\n## # ℹ 36 more rows# A tibble: 46 × 4\n# Groups:   species_id [18]\nsurveys %>% \n  group_by(species_id, sex)## # A tibble: 16,878 × 13\n## # Groups:   species_id, sex [67]\n##    record_id month   day  year plot_id species_id sex   hindfoot_length weight\n##        <dbl> <dbl> <dbl> <dbl>   <dbl> <chr>      <chr>           <dbl>  <dbl>\n##  1         1     7    16  1977       2 NL         M                  32     NA\n##  2         2     7    16  1977       3 NL         M                  33     NA\n##  3         3     7    16  1977       2 DM         F                  37     NA\n##  4         4     7    16  1977       7 DM         M                  36     NA\n##  5         5     7    16  1977       3 DM         M                  35     NA\n##  6         6     7    16  1977       1 PF         M                  14     NA\n##  7         7     7    16  1977       2 PE         F                  NA     NA\n##  8         8     7    16  1977       1 DM         M                  37     NA\n##  9         9     7    16  1977       1 DM         F                  34     NA\n## 10        10     7    16  1977       6 PF         F                  20     NA\n## # ℹ 16,868 more rows\n## # ℹ 4 more variables: genus <chr>, species <chr>, taxa <chr>, plot_type <chr>\nsurveys %>% \n  filter(!is.na(weight)) %>% \n  group_by(species_id, sex) %>% \n  summarize(mean_weight = mean(weight),\n            n = n()) %>% \n  arrange(desc(mean_weight)) %>% \n  ungroup()## `summarise()` has grouped output by 'species_id'. You can override using the\n## `.groups` argument.## # A tibble: 46 × 4\n##    species_id sex   mean_weight     n\n##    <chr>      <chr>       <dbl> <int>\n##  1 NL         M           168.    355\n##  2 NL         <NA>        164.      9\n##  3 NL         F           151.    460\n##  4 SS         M           130       1\n##  5 DS         M           123.   1184\n##  6 DS         <NA>        121.     16\n##  7 DS         F           118.   1055\n##  8 SH         F            79.2    61\n##  9 SH         M            67.6    34\n## 10 SF         F            58.3     3\n## # ℹ 36 more rows\nsurveys %>% \n  filter(!is.na(weight)) %>% \n  group_by(species_id, sex) %>% \n  mutate(mean_weight = mean(weight),\n            weight_diff = weight - mean_weight)## # A tibble: 15,186 × 15\n## # Groups:   species_id, sex [46]\n##    record_id month   day  year plot_id species_id sex   hindfoot_length weight\n##        <dbl> <dbl> <dbl> <dbl>   <dbl> <chr>      <chr>           <dbl>  <dbl>\n##  1        63     8    19  1977       3 DM         M                  35     40\n##  2        64     8    19  1977       7 DM         M                  37     48\n##  3        65     8    19  1977       4 DM         F                  34     29\n##  4        66     8    19  1977       4 DM         F                  35     46\n##  5        67     8    19  1977       7 DM         M                  35     36\n##  6        68     8    19  1977       8 DO         F                  32     52\n##  7        69     8    19  1977       2 PF         M                  15      8\n##  8        70     8    19  1977       3 OX         F                  21     22\n##  9        71     8    19  1977       7 DM         F                  36     35\n## 10        74     8    19  1977       8 PF         M                  12      7\n## # ℹ 15,176 more rows\n## # ℹ 6 more variables: genus <chr>, species <chr>, taxa <chr>, plot_type <chr>,\n## #   mean_weight <dbl>, weight_diff <dbl>\nsurveys %>% \n  filter(!is.na(weight)) %>% \n  group_by(species_id, sex) %>% \n  mutate(mean_weight = mean(weight),\n            weight_diff = weight - mean_weight) %>% \n  select(species_id, sex, contains(\"weight\"))## # A tibble: 15,186 × 5\n## # Groups:   species_id, sex [46]\n##    species_id sex   weight mean_weight weight_diff\n##    <chr>      <chr>  <dbl>       <dbl>       <dbl>\n##  1 DM         M         40       44.0      -4.00  \n##  2 DM         M         48       44.0       4.00  \n##  3 DM         F         29       40.7     -11.7   \n##  4 DM         F         46       40.7       5.28  \n##  5 DM         M         36       44.0      -8.00  \n##  6 DO         F         52       48.4       3.63  \n##  7 PF         M          8        7.10      0.902 \n##  8 OX         F         22       21         1     \n##  9 DM         F         35       40.7      -5.72  \n## 10 PF         M          7        7.10     -0.0980\n## # ℹ 15,176 more rows"},{"path":"manipulating-tabular-data.html","id":"challenge-4-making-a-time-series","chapter":"4 Manipulating Tabular Data","heading":"4.10 Challenge 4: Making a time series","text":"Use split-apply-combine approach make data.frame counts total number animals sex caught day surveys data.Solution. Now use data.frame just made plot daily number animals sex caught time. ’s geom use, line plot might good choice. also think differentiate data corresponds sex.Solution. ","code":"\nsurveys_daily_counts <- surveys %>% \n  mutate(date = ymd(paste(year, month, day, sep = \"-\"))) %>% \n  group_by(date, sex) %>% \n  summarize(n = n())## `summarise()` has grouped output by 'date'. You can override using the\n## `.groups` argument.\n# shorter approach using count()\nsurveys_daily_counts <- surveys %>% \n  mutate(date = ymd(paste(year, month, day, sep = \"-\"))) %>% \n  count(date, sex)\nsurveys_daily_counts %>% \n  ggplot(aes(x = date, y = n, color = sex)) +\n  geom_line()"},{"path":"manipulating-tabular-data.html","id":"reshaping-data-with-tidyr","chapter":"4 Manipulating Tabular Data","heading":"4.11 Reshaping data with tidyr","text":"Let’s say interested comparing mean weights species across different plots. can begin process using group_by() + summarize() approach:looks great, bit difficult compare values across plots. nice reshape data.frame make comparisons easier. Well, tidyr package tidyverse pair functions allow reshape data pivoting : pivot_wider() pivot_longer(). pivot_wider() make data wider, means increasing number columns reducing number rows. pivot_longer() opposite, reducing number columns increasing number rows.case, might nice create data.frame species row, plot column containing mean weight given species. use pivot_wider() reshape data way. takes 3 arguments:name data.framenames_from: column used generate names new columns?values_from: column used fill values new columns?columns used names_from values_from pivoted.{alt=‘Diagram depicting behavior pivot_wider() small tabular dataset.’; width = 100%}case, want new columns named plot_id column, values coming mean_weight column. can pipe data.frame right pivot_wider() add two arguments:Now ’ve got reshaped data.frame. things notice. First, new column plot_id value. one old column left data.frame: species_id. wasn’t used pivot_wider(), stays, now contains single entry unique species_id value.Finally, lot NAs appeared. species aren’t found every plot, data.frame value every row every column, NA inserted. can double-check verify going .Looking new pivoted data.frame, can see NA value species BA plot 1. Let’s take sp_by_plot data.frame look mean_weight species + plot combination.get back 0 rows. mean_weight species BA plot 1. either happened BA ever caught plot 1, every BA caught plot 1 NA weight value rows got removed used filter(!.na(weight)) process making sp_by_plot. rows species + plot combination, pivoted data.frame, value gets filled NA.another pivot_ function opposite, moving data wide long format, called pivot_longer(). takes 3 arguments: cols columns want pivot, names_to name new column contain old column names, values_to name new column contain old values.{alt=‘Diagram depicting behavior pivot_longer() small tabular dataset.’; width = 100%}can pivot new wide data.frame long format using pivot_longer(). want pivot columns except species_id, use PLOT new column plot IDs, MEAN_WT new column mean weight values.One thing notice NA values got generated pivoted wider. However, can filter , gets us back data sp_by_plot, pivoted wider.Data often recorded spreadsheets wider format, lots tidyverse tools, especially ggplot2, like data longer format, pivot_longer() often useful.","code":"\nsp_by_plot <- surveys %>% \n  filter(!is.na(weight)) %>% \n  group_by(species_id, plot_id) %>% \n  summarise(mean_weight = mean(weight)) %>% \n  arrange(species_id, plot_id)## `summarise()` has grouped output by 'species_id'. You can override using the\n## `.groups` argument.\nsp_by_plot## # A tibble: 300 × 3\n## # Groups:   species_id [18]\n##    species_id plot_id mean_weight\n##    <chr>        <dbl>       <dbl>\n##  1 BA               3         8  \n##  2 BA              21         6.5\n##  3 DM               1        42.7\n##  4 DM               2        42.6\n##  5 DM               3        41.2\n##  6 DM               4        41.9\n##  7 DM               5        42.6\n##  8 DM               6        42.1\n##  9 DM               7        43.2\n## 10 DM               8        43.4\n## # ℹ 290 more rows\nsp_by_plot_wide <- sp_by_plot %>% \n  pivot_wider(names_from = plot_id, \n              values_from = mean_weight)\n\nsp_by_plot_wide## # A tibble: 18 × 25\n## # Groups:   species_id [18]\n##    species_id    `3`   `21`    `1`    `2`    `4`   `5`    `6`   `7`    `8`\n##    <chr>       <dbl>  <dbl>  <dbl>  <dbl>  <dbl> <dbl>  <dbl> <dbl>  <dbl>\n##  1 BA           8      6.5   NA     NA     NA     NA    NA     NA    NA   \n##  2 DM          41.2   41.5   42.7   42.6   41.9   42.6  42.1   43.2  43.4 \n##  3 DO          42.7   NA     50.1   50.3   46.8   50.4  49.0   52    49.2 \n##  4 DS         128.    NA    129.   125.   118.   111.  114.   126.  128.  \n##  5 NL         171.   136.   154.   171.   164.   192.  176.   170.  134.  \n##  6 OL          32.1   28.6   35.5   34     33.0   32.6  31.8   NA    30.3 \n##  7 OT          24.1   24.1   23.7   24.9   26.5   23.6  23.5   22    24.1 \n##  8 OX          22     NA     NA     22     NA     20    NA     NA    NA   \n##  9 PE          22.7   19.6   21.6   22.0   NA     21    21.6   22.8  19.4 \n## 10 PF           7.12   7.23   6.57   6.89   6.75   7.5   7.54   7     6.78\n## 11 PH          28     31     NA     NA     NA     29    NA     NA    NA   \n## 12 PM          20.1   23.6   23.7   23.9   NA     23.7  22.3   23.4  23   \n## 13 PP          17.1   13.6   14.3   16.4   14.8   19.8  16.8   NA    13.9 \n## 14 RF          14.8   17     NA     16     NA     14    12.1   13    NA   \n## 15 RM          10.3    9.89  10.9   10.6   10.4   10.8  10.6   10.7   9   \n## 16 SF          NA     49     NA     NA     NA     NA    NA     NA    NA   \n## 17 SH          76.0   79.9   NA     88     NA     82.7  NA     NA    NA   \n## 18 SS          NA     NA     NA     NA     NA     NA    NA     NA    NA   \n## # ℹ 15 more variables: `9` <dbl>, `10` <dbl>, `11` <dbl>, `12` <dbl>,\n## #   `13` <dbl>, `14` <dbl>, `15` <dbl>, `16` <dbl>, `17` <dbl>, `18` <dbl>,\n## #   `19` <dbl>, `20` <dbl>, `22` <dbl>, `23` <dbl>, `24` <dbl>\nsp_by_plot %>% \n  filter(species_id == \"BA\" & plot_id == 1)## # A tibble: 0 × 3\n## # Groups:   species_id [0]\n## # ℹ 3 variables: species_id <chr>, plot_id <dbl>, mean_weight <dbl>\nsp_by_plot_wide %>% \n  pivot_longer(cols = -species_id, names_to = \"PLOT\", values_to = \"MEAN_WT\")## # A tibble: 432 × 3\n## # Groups:   species_id [18]\n##    species_id PLOT  MEAN_WT\n##    <chr>      <chr>   <dbl>\n##  1 BA         3         8  \n##  2 BA         21        6.5\n##  3 BA         1        NA  \n##  4 BA         2        NA  \n##  5 BA         4        NA  \n##  6 BA         5        NA  \n##  7 BA         6        NA  \n##  8 BA         7        NA  \n##  9 BA         8        NA  \n## 10 BA         9        NA  \n## # ℹ 422 more rows\nsp_by_plot_wide %>% \n  pivot_longer(cols = -species_id, names_to = \"PLOT\", values_to = \"MEAN_WT\") %>% \n  filter(!is.na(MEAN_WT))## # A tibble: 300 × 3\n## # Groups:   species_id [18]\n##    species_id PLOT  MEAN_WT\n##    <chr>      <chr>   <dbl>\n##  1 BA         3         8  \n##  2 BA         21        6.5\n##  3 DM         3        41.2\n##  4 DM         21       41.5\n##  5 DM         1        42.7\n##  6 DM         2        42.6\n##  7 DM         4        41.9\n##  8 DM         5        42.6\n##  9 DM         6        42.1\n## 10 DM         7        43.2\n## # ℹ 290 more rows"},{"path":"manipulating-tabular-data.html","id":"other-key-data-tools","chapter":"4 Manipulating Tabular Data","heading":"4.12 Other key data tools","text":"","code":""},{"path":"manipulating-tabular-data.html","id":"for-loops","chapter":"4 Manipulating Tabular Data","heading":"4.12.1 For-loops","text":"LoopingA loop allows repeat code. specify variable range values loop runs \ncode value range. ways repeat code (e.g. apply suite functions), \ngoing discuss loops (R world think loops bad since R optimized working\nvectors, concept useful). basic structure looks like:Loops cycle “index” variable, changes iteration. must give variable name\n(often people use “” index), tell values cycle . Let’s look loop \nprints value index variable time.’ll see value printed loop iteration, changes based values given \n. Let’s try one simple example, give nonconsecutive looping values.just illustrates can use manye different vector types looping vector. loop \nalways use looping_vector[1] first value index, looping_vector[2], \ngets last value looping vector. Now, let’s something little useful inside loop.bit silly example since looping list values summing .\nreality just use sum() cumsum(). also highlights fact loops R can slow\ncompared vector operations /primitive operations (see Hadley Wickham’s section Primitive functions).kind thing doesn’t come easily many languages. need program using\nloop. sake argument (practice), let’s try loop version R. ’ll notice \nset <- NULL beginning. adding values using loop, object must\nexist first. Thus, make NULL can use <- c() know vector., thing, big deal. big though look timing two. Let’s\ncreate two large vectors see happens.Quite difference time! examples like lead talk around \nR slow looping. general agree obvious vectorized/base\nsolution (case simply using ifelse) use . said, isn’t always\nobvious faster solution . ChatGPT AI programs helpful\nsuggesting solutions.short, obvious vector primitive solution exists, use . aren’t\nclear need use loop, don’t afraid use one. plenty \nexamples vectorized solution exists loop, may difficult code\nunderstand. Personally, think possible go far vectorized\npath. makes sense, otherwise take advantage loop! can\nalways try speed things got code working first time.","code":"for(a_name in a_range) {\n  code you want to run\n  may or may not use a_name\n}\nfor(i in 1:3){\n  print(i)\n}## [1] 1\n## [1] 2\n## [1] 3\nloopVec <- c('a', 'vector', 'of', 'character', 'values', 'works', 'too!')\n\nfor(word in loopVec){\n  print(word)\n}## [1] \"a\"\n## [1] \"vector\"\n## [1] \"of\"\n## [1] \"character\"\n## [1] \"values\"\n## [1] \"works\"\n## [1] \"too!\"\n# sequentially increase the value of some number\nvec <- 1:10\nj <- 0 # variables used in loops must exist already - so we initialize them\nfor(i in vec) {\n  j <- i + j\n  print(j)\n}## [1] 1\n## [1] 3\n## [1] 6\n## [1] 10\n## [1] 15\n## [1] 21\n## [1] 28\n## [1] 36\n## [1] 45\n## [1] 55\n# A simple vectorized operation\nx <- 1:100\ny <- 100:1\nz <- x + y\nz##   [1] 101 101 101 101 101 101 101 101 101 101 101 101 101 101 101 101 101 101\n##  [19] 101 101 101 101 101 101 101 101 101 101 101 101 101 101 101 101 101 101\n##  [37] 101 101 101 101 101 101 101 101 101 101 101 101 101 101 101 101 101 101\n##  [55] 101 101 101 101 101 101 101 101 101 101 101 101 101 101 101 101 101 101\n##  [73] 101 101 101 101 101 101 101 101 101 101 101 101 101 101 101 101 101 101\n##  [91] 101 101 101 101 101 101 101 101 101 101\n# We will assume vectors of the same length...\nout <- NULL \nfor(i in 1:length(x)) {\n  out[i] <- x[i] + y[i]\n}\n# This happens almost instantaneously. This creates a new columns where there is a NA value \n# if the Flow_cd is \"A e\", otherwise it uses the value stored in the Flow column\nintroDFRev <- dplyr::mutate(introDF, Flow_rev = ifelse(Flow_cd == \"A e\", NA, Flow))\n\n\n# Now if we looped through to check that we have to wait for this loop (~20 seconds) - Don't try the whole dataframe. It will take forever. \nfor(i in 1:1000) { \n  if(introDFRev$Flow_cd[i] == \"A e\") { \n       introDFRev$Flow_rev2[i] <-  NA \n    } else { \n      introDFRev$Flow_rev2[i] <- introDFRev$Flow[i] \n      }\n} #end loop"},{"path":"manipulating-tabular-data.html","id":"functions","chapter":"4 Manipulating Tabular Data","heading":"4.12.2 Functions","text":"point pretty well versed using functions. name arguments, \nsomething. return value, don’t. short form basic structure R. One cool\nthings R (programming general), stuck functions provided us. can\ndevelop , often want things repeatedly, slightly different contexts.\nCreating function deal fact helps us great deal repeat , \ncan just use already written. Creating function really easy. use function() function.\nbasic structure :Well ’s nice. really useful, shows main components, function(), {} \nreally new things.bit better flexible. can can specify arguments\nuse within body function. example,Functions useful want repeat general procedure different specifics time. Since\nworking recently creating plots, imagine us wanting create plot similar\nlayout, different source data save plot file, single function call. \nmight look like:Nice… function something usefulreturn()last control structure going talk return(). return() provide result\nfunction terminates function. may asking , didn’t terminate get value\nfunctions just created? , return() mandatory R functions return\nlast calculation. people argue using return() good practice allows us \nexplicit. Others argue concise beautiful ’s hard see implicitly returned.\nThink learn R. see return() can used, let’s take look \nodd_even() sum_vec() functions make simple changes take advantage return().First, oddEven().now, sumVec()","code":"\nfunction_name <- function(arguments) {\n  codeGoesHere\n  useArgumentsAsNeeded\n} #ends function\n\n## So a real example with no arguments might look like:\n\nsay_hi <- function() {\n  print(\"Hello, World!\")\n}\n\nsay_hi()## [1] \"Hello, World!\"\nprint_twice <- function(my_text) {\n  print(my_text)\n  print(my_text)\n}\n\nprint_twice(\"Hello, World!\")## [1] \"Hello, World!\"\n## [1] \"Hello, World!\"\nprint_twice(\"Hola, mundo\")## [1] \"Hola, mundo\"\n## [1] \"Hola, mundo\"\nprint_twice(\"Howdy, Texas\")## [1] \"Howdy, Texas\"\n## [1] \"Howdy, Texas\"\ngetPlot <- function(x, y, file) {\n  myDat <- data.frame(X = x, Y = y) #creating at dataframe\n  myP <- ggplot(myDat, aes(x = X, y = Y)) + \n    geom_point() + \n    stat_smooth(method = \"lm\")\n  ggsave(file, plot = myP)\n  return(myP) # to get the plot out of the function\n}\n\n# call the function using siteDF\nlibrary(ggplot2)\ngetPlot(siteDF$flowX, siteDF$flowY, \"flowPlot.png\")\noddEven <- function(num) {\n  if(num %% 2 == 0){\n    return(\"EVEN\")\n  } \n  return(\"ODD\")\n}\nsumVec <- function(vec) {\n  j <- 0\n  for(i in vec) {\n    j <- i + j\n  }\n  return(j)\n}"},{"path":"manipulating-tabular-data.html","id":"exporting-data","chapter":"4 Manipulating Tabular Data","heading":"4.13 Exporting data","text":"Let’s say want send wide version sb_by_plot data.frame colleague doesn’t use R. case, might want save CSV file.First, might want modify names columns, since right now bare numbers, aren’t informative. Luckily, pivot_wider() argument names_prefix allow us add “plot_” start column.looks better! Let’s save data.frame new object.Now can save data.frame CSV using write_csv() function readr package. first argument name data.frame, second path new file want create, including file extension .csv.go look data/cleaned_data folder, see new CSV file.","code":"\nsp_by_plot %>% \n  pivot_wider(names_from = plot_id, values_from = mean_weight,\n              names_prefix = \"plot_\")## # A tibble: 18 × 25\n## # Groups:   species_id [18]\n##    species_id plot_3 plot_21 plot_1 plot_2 plot_4 plot_5 plot_6 plot_7 plot_8\n##    <chr>       <dbl>   <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>\n##  1 BA           8       6.5   NA     NA     NA      NA    NA      NA    NA   \n##  2 DM          41.2    41.5   42.7   42.6   41.9    42.6  42.1    43.2  43.4 \n##  3 DO          42.7    NA     50.1   50.3   46.8    50.4  49.0    52    49.2 \n##  4 DS         128.     NA    129.   125.   118.    111.  114.    126.  128.  \n##  5 NL         171.    136.   154.   171.   164.    192.  176.    170.  134.  \n##  6 OL          32.1    28.6   35.5   34     33.0    32.6  31.8    NA    30.3 \n##  7 OT          24.1    24.1   23.7   24.9   26.5    23.6  23.5    22    24.1 \n##  8 OX          22      NA     NA     22     NA      20    NA      NA    NA   \n##  9 PE          22.7    19.6   21.6   22.0   NA      21    21.6    22.8  19.4 \n## 10 PF           7.12    7.23   6.57   6.89   6.75    7.5   7.54    7     6.78\n## 11 PH          28      31     NA     NA     NA      29    NA      NA    NA   \n## 12 PM          20.1    23.6   23.7   23.9   NA      23.7  22.3    23.4  23   \n## 13 PP          17.1    13.6   14.3   16.4   14.8    19.8  16.8    NA    13.9 \n## 14 RF          14.8    17     NA     16     NA      14    12.1    13    NA   \n## 15 RM          10.3     9.89  10.9   10.6   10.4    10.8  10.6    10.7   9   \n## 16 SF          NA      49     NA     NA     NA      NA    NA      NA    NA   \n## 17 SH          76.0    79.9   NA     88     NA      82.7  NA      NA    NA   \n## 18 SS          NA      NA     NA     NA     NA      NA    NA      NA    NA   \n## # ℹ 15 more variables: plot_9 <dbl>, plot_10 <dbl>, plot_11 <dbl>,\n## #   plot_12 <dbl>, plot_13 <dbl>, plot_14 <dbl>, plot_15 <dbl>, plot_16 <dbl>,\n## #   plot_17 <dbl>, plot_18 <dbl>, plot_19 <dbl>, plot_20 <dbl>, plot_22 <dbl>,\n## #   plot_23 <dbl>, plot_24 <dbl>\nsurveys_sp <- sp_by_plot %>% \n  pivot_wider(names_from = plot_id, values_from = mean_weight,\n              names_prefix = \"plot_\")\n\nsurveys_sp## # A tibble: 18 × 25\n## # Groups:   species_id [18]\n##    species_id plot_3 plot_21 plot_1 plot_2 plot_4 plot_5 plot_6 plot_7 plot_8\n##    <chr>       <dbl>   <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>\n##  1 BA           8       6.5   NA     NA     NA      NA    NA      NA    NA   \n##  2 DM          41.2    41.5   42.7   42.6   41.9    42.6  42.1    43.2  43.4 \n##  3 DO          42.7    NA     50.1   50.3   46.8    50.4  49.0    52    49.2 \n##  4 DS         128.     NA    129.   125.   118.    111.  114.    126.  128.  \n##  5 NL         171.    136.   154.   171.   164.    192.  176.    170.  134.  \n##  6 OL          32.1    28.6   35.5   34     33.0    32.6  31.8    NA    30.3 \n##  7 OT          24.1    24.1   23.7   24.9   26.5    23.6  23.5    22    24.1 \n##  8 OX          22      NA     NA     22     NA      20    NA      NA    NA   \n##  9 PE          22.7    19.6   21.6   22.0   NA      21    21.6    22.8  19.4 \n## 10 PF           7.12    7.23   6.57   6.89   6.75    7.5   7.54    7     6.78\n## 11 PH          28      31     NA     NA     NA      29    NA      NA    NA   \n## 12 PM          20.1    23.6   23.7   23.9   NA      23.7  22.3    23.4  23   \n## 13 PP          17.1    13.6   14.3   16.4   14.8    19.8  16.8    NA    13.9 \n## 14 RF          14.8    17     NA     16     NA      14    12.1    13    NA   \n## 15 RM          10.3     9.89  10.9   10.6   10.4    10.8  10.6    10.7   9   \n## 16 SF          NA      49     NA     NA     NA      NA    NA      NA    NA   \n## 17 SH          76.0    79.9   NA     88     NA      82.7  NA      NA    NA   \n## 18 SS          NA      NA     NA     NA     NA      NA    NA      NA    NA   \n## # ℹ 15 more variables: plot_9 <dbl>, plot_10 <dbl>, plot_11 <dbl>,\n## #   plot_12 <dbl>, plot_13 <dbl>, plot_14 <dbl>, plot_15 <dbl>, plot_16 <dbl>,\n## #   plot_17 <dbl>, plot_18 <dbl>, plot_19 <dbl>, plot_20 <dbl>, plot_22 <dbl>,\n## #   plot_23 <dbl>, plot_24 <dbl>\nwrite_csv(surveys_sp, \"data/surveys_meanweight_species_plot.csv\")"},{"path":"manipulating-tabular-data.html","id":"summary-1","chapter":"4 Manipulating Tabular Data","heading":"4.14 Summary","text":"use filter() subset rows select() subset columnsbuild pipelines one step time assigning resultit often best keep components dates separate needed, use mutate() make date columngroup_by() can used summarize() collapse rows mutate() keep number rowspivot_wider() pivot_longer() powerful reshaping data, plan use thoughtfully","code":""},{"path":"conceptual-ecological-models.html","id":"conceptual-ecological-models","chapter":"5 Conceptual Ecological Models","heading":"5 Conceptual Ecological Models","text":"","code":""},{"path":"conceptual-ecological-models.html","id":"what-they-are-not","chapter":"5 Conceptual Ecological Models","heading":"5.1 What they are not","text":"truth\nComprehensive\nFinal","code":""},{"path":"conceptual-ecological-models.html","id":"misconceptions","chapter":"5 Conceptual Ecological Models","heading":"5.2 Misconceptions","text":"Incomplete understanding","code":""},{"path":"conceptual-ecological-models.html","id":"storytelling","chapter":"5 Conceptual Ecological Models","heading":"5.3 Storytelling","text":"audience\ncharacters\nplot?\nsetting?","code":""},{"path":"conceptual-ecological-models.html","id":"art-of-conceptual-modeling-is-iterating","chapter":"5 Conceptual Ecological Models","heading":"5.4 Art of conceptual modeling is iterating","text":"Iterate simple “complex” right level complexity","code":""},{"path":"background-on-habitat-models-in-usace.html","id":"background-on-habitat-models-in-usace","chapter":"6 Background on habitat models in USACE","heading":"6 Background on habitat models in USACE","text":"INTRO HABITAT MODELS GENERAL; CONSIDER WHETHER PARE REMOVEAcross business lines, U.S. Army Corps Engineers (USACE) engages large variety decisions affect multitude ecological outcomes (e.g., ecosystem restoration oyster reefs, environmental flows imperiled fishes, bird breeding grounds impacted dredge material management). numerous aquatic, riparian, terrestrial analytical tools exist, ecological models typically easily accessible usable form field practitioners often require significant data modeling expertise effectively employ. However, ecological models must used quantify environmental impacts benefits throughout project life-cycle planning, engineering, construction, operations, maintenance.common approach ecological modeling environmental impacts benefits based quantity quality habitat. “index” models (Swannack et al. 2012) originally developed species-specific applications (e.g., slider turtles), general approach also adapted guilds (e.g., salmonids), communities (e.g., floodplain vegetation), ecosystem processes (e.g., Hydrogeomorphic Method). standard platform exists computing outcomes index models, users often develop ad hoc spreadsheet models, highly prone numerical errors (McKay 2009). common quantity-quality structure index models provides opportunity develop consistent, error-checked index modeling calculator adaptable variety applications across Corps.Furthermore, ecological modeling often seeks inform trade-offs monetary assessment social benefits costs (e.g., restoration investment cost economic damages avoided) non-monetary assessment environmental benefits costs (e.g., habitat gains restoration impact imperiled taxa’s habitat). Cost-effectiveness incremental cost analyses (CEICA) provide useful set techniques comparing non-monetary monetary costs benefits management actions (Robinson et al. 1995). CEICA commonly applied planning designing ecosystem restoration projects often coupled index models inform management decisions.","code":""},{"path":"background-on-habitat-models-in-usace.html","id":"index-based-ecological-models","chapter":"6 Background on habitat models in USACE","heading":"6.0.0.1 2.1. Index-Based Ecological Models","text":"Ecological models become common tools informing decisions related management complex ecological processes. Models span breadth potential ecological management applications seafood harvest limits, transport nutrients freshwaters, bioaccumulation contaminants, management imperiled taxa, wetland impact assessment. addition diverse outcomes, ecological models often take variety theoretical constructs ranging theoretical, analytical models statistical correlations variables agent-based simulations animal movement (See Swannack et al. 2012 review ecological model types). diversity ecological endpoints model constructs led wide array tools applicable ecosystem management restoration.Index models family techniques commonly applied planning ecosystem restoration projects. Briefly, index models quantitatively translate multiple features processes relative assessment habitat suitability given organism relative assessment ecosystem condition (Tirpak et al. 2009, Swannack et al. 2012). specifically, index models combine assessments habitat ecosystem quality quantity overarching metric assessing relative condition site (e.g., “habitat unit” “functional capacity unit”). Quantity commonly expressed metric area acres hectares; however, metrics may appropriate specific applications river length lake volume. Quality assessed identifying key variables correlated habitat ecosystem condition. variable translated suitability index curve, transforms dimensional quantities flow velocity dimensionless values quality (0 1 0 unsuitable/low condition 1 suitable/ideal condition). Multiple suitability curves combined various equational forms overarching assessment habitat quality (e.g., “habitat suitability index” “functional capacity index”).Index models may derived variety methods resources. Like models, many index model developers emphasized algorithms simplify complex ecosystems, thus, tools considered adaptable hypotheses rather mechanistic, cause-effect relationships. value index models lies utility quantitatively comparing relative merits alternative management actions testing hypotheses. following represent common sources index models applied USACE ecosystem restoration projects:U.S. Fish Wildlife Service (USFWS) Habitat Suitability Index (HSI) Models: HSI approach quantitatively relates potential species presence habitat characteristics. Species complex relationships environment, HSI models provide simple method characterizing potential habitat support target species across landscape. USFWS led development 500+ HSI models 1970-1980s support environmental management decisions nationwide (https://www.nwrc.usgs.gov/wdb/pub/hsi/hsiindex.htm). quantitative relationships species habitat generally based combination literature, field studies, expert opinion, reports colloquially referred “blue books” blue covers binding original publications.Hydrogeomorphic Method (HGM) Wetland Assessment: Similarly, HGM approach common technique rapidly assessing wetland function. HGM methodology model development thoroughly documented (Brinson 1993, Smith et al. 1995), models available 30+ wetland types nationwide (https://wetlands.el.erdc.dren.mil/guidebooks.cfm).Literature-Based Index Models: Index models also appear within peer-reviewed grey literature variety formats. original HSI models subsequently adapted local conditions updated new data became available (e.g., bluegill model Stuber et al. 1982 adapted Palesh Anderson 1990). Models also appear peer-reviewed literature new data become available, recent revision oyster suitability models expands breadth applicability (Swannack et al. 2014).Project- Objective-Centric Index Models: Specific restoration projects may build models unique set project objectives (McKay et al. 2019), can evolve project proceeds preliminary screening detailed alternatives analysis (e.g., McKay et al. 2018ab). recently, Carrillo et al. (2020) proposed Toolkit interActive Modeling (TAM), facilitates development index models real-time mediated modeling workshop settings (Herman et al. 2019).","code":""},{"path":"background-on-habitat-models-in-usace.html","id":"cost-effectiveness-and-incremental-cost-analysis-ceica","chapter":"6 Background on habitat models in USACE","heading":"6.0.0.2 2.2. Cost-Effectiveness and Incremental Cost Analysis (CEICA)","text":"USACE ecosystem restoration mission first authorized Water Resources Development Act 1986 stated purpose “…restore significant structure, function dynamic processes degraded” (USACE 1999, ER 1165-2-501). Given goal, USACE programs emphasize ecological outcomes (opposed social economic outcomes). Generally, ecological resources may quantified variety ways ranging habitat suitability focal taxa (e.g., endangered species) changes physical processes (e.g., sediment delivery geomorphic change) changes biological processes (e.g., carbon uptake storage). USACE business lines (e.g., navigation), costs benefits actions compared monetary terms, benefit-cost ratio serves crucial decision metric. However, outputs restoration typically monetized, different set methods required inform restoration decision-making address issue “ecosystem restoration worth Federal investment?” particular, cost-effectiveness incremental cost analyses provide techniques comparing non-monetary ecological benefits relative monetary costs restoration actions (Robinson et al. 1995).Cost-effectiveness incremental cost analyses (CEICA) analytical tools assessing relative benefits costs ecosystem restoration actions informing decisions. Benefits costs assessed prior analyses using ecological models (e.g., index models) cost engineering methods, respectively. CEICA may conducted site scale compare alternatives single location (e.g., action vs. dam removal vs. fish ladder) system scale compare relative merits multiple sites (e.g., sites vs. Site-vs. Site-B vs. Site-Site-B). Within USACE, Institute Water Resources provided toolkit conducting CEICA, IWR Planning Suite (http://www.iwr.usace.army.mil/Missions/Economics/IWR-Planning-Suite/).Cost-effectiveness analysis provides mechanism examining efficiency alternative actions. given level investment, agency wants identify plan return--investment (.e., environmental benefits), given level environmental benefits, agency wants plan least cost. “efficiency frontier” identifies plans efficiently provide benefits per cost basis (.e., cost-effective plans). “non-dominated” alternatives compose Pareto-optimal frontier.Incremental cost analysis conducted set cost-effective plans. technique sequentially compares plan higher cost plans reveal changes unit cost output levels increase eliminates plans efficiently provide benefits per unit cost basis. Specifically, analysis examines slope cost-effectiveness frontier isolate incremental unit cost ($/unit) increases magnitude environmental benefit increases. Incremental cost analysis ultimately intended inform decision-makers consequences increasing unit cost increasing benefits (.e., unit becomes expensive). Plans emerging incremental cost analysis efficiently accomplish objective relative unit costs typically referred “best buys”. Importantly, “best buys” cost-effective, cost-effective plans best buys.","code":""},{"path":"background-on-habitat-models-in-usace.html","id":"include-tam-model-here","chapter":"6 Background on habitat models in USACE","heading":"6.0.1 Include TAM model here?","text":"","code":""},{"path":"habitat-suitability-index-models-with-ecorest.html","id":"habitat-suitability-index-models-with-ecorest","chapter":"7 Habitat Suitability Index models with ecorest","heading":"7 Habitat Suitability Index models with ecorest","text":"module teach use ecorest package R efficiently reproducibly carry HSI modeling generate useful output like :Authors: Darixa Hernandez-Abrams (writing, code), Kyle McKay (writing, code), Ed Stowe (writing, code, editing), ; Background draft ecorest tech report, authors: Kyle McKay, Darixa Hernández-Abrams, Rachel Nifong, Todd SwannackLast update: 2025-02-06Acknowledgements: ","code":""},{"path":"habitat-suitability-index-models-with-ecorest.html","id":"learning-objectives-5","chapter":"7 Habitat Suitability Index models with ecorest","heading":"7.1 Learning objectives","text":"Understand benefits HSI modeling programmatically ecorestLearn use ecorest conduct Blue book HSI modelBuild custom HSI model using ecorest","code":""},{"path":"habitat-suitability-index-models-with-ecorest.html","id":"background-on-hsi-modeling-with-ecorest","chapter":"7 Habitat Suitability Index models with ecorest","heading":"7.2 Background on HSI modeling with ecorest","text":"ecorest, standard platform computing outcomes habitat index models. now, users often develop ad hoc spreadsheet models, can highly prone numerical errors (McKay 2009). common quantity-quality structure index models, however, provided opportunity develop consistent, error-checked index modeling calculator adaptable variety applications across USACE. tool ecorest, can greatly increase efficiency performing habitat modeling well decrease likelihood computational errors.ecorest package can used quickly apply 350 HSI models developed U.S. Fish Wildlife Service, often referred “bluebook” models, flexible enough carry user-defined HSI model, including ones appear literature custom models developed specific projects.tutorial, demonstrate ecorest package can used two different HSI modeling scenarios, one relies bluebook model, another uses custom HSI model.","code":""},{"path":"habitat-suitability-index-models-with-ecorest.html","id":"run-an-hsi-analysis-with-ecorest-using-a-bluebook-model","chapter":"7 Habitat Suitability Index models with ecorest","heading":"7.3 Run an HSI analysis with ecorest using a bluebook model","text":"scenario, use ecorest carry modeling analysis originally done Sacramento District part Delta Islands Levees Feasibility Study. study, Marsh Wren HSI model used examine ecosystem effects intertidal marsh restoration. overall Feasibility Study can found , appendix details habitat evaluation procedure can found .conduct HSI modeling scenario, first need add necessary packages R session.ecorest package contains list object called HSImodels. 351 elements list data frame representing one U.S. Fish Wildlife Service Habitat suitability index (HSI) models. HSI model consists multiple independent variables (e.g., percent canopy cover) associated habitat suitability values ranging 0 1. Categorical input variables coded letters. models can called R corresponding model name using HSImodels$modelname (e.g., HSImodels$barredowl).look HSI models available, run following code:Another important object contained within package HSImetadata dataframe. contains important information HSI model, including different suitability variables within model aggregated calculate overall HSI value project alternative. following code generates small subset variables rows dataframe.","code":"\nlibrary(ecorest)\nlibrary(tidyverse)\nnames(HSImodels)##   [1] \"alewifeJuv\"                             \"alewifeJuvAndSAEL\"                     \n##   [3] \"alewifeSAEL\"                            \"americanalligatorNontidal\"             \n##   [5] \"americanalligatorTidal\"                 \"americanblackduckWinteringEVegWetland\" \n##   [7] \"americanblackduckWinteringNCapeCod\"     \"americanblackduckWinteringSCapeCod\"    \n##   [9] \"americancoot\"                           \"americaneiderBreeding\"                 \n##  [11] \"americanoysterGulfofMexModifier\"        \"americanoysterGulfofMexTypical\"        \n##  [13] \"americanshadEstu\"                       \"americanshadRiv\"                       \n##  [15] \"americanwoodcockWinteringForestedDry\"   \"americanwoodcockWinteringForestedMoist\"\n##  [17] \"americanwoodcockWinteringForestedWet\"   \"americanwoodcockWinteringShrubDry\"     \n##  [19] \"americanwoodcockWinteringShrubMoist\"    \"americanwoodcockWinteringShrubWet\"     \n##  [21] \"arcticgrayling\"                         \"arcticgraylingAdultJuv\"                \n##  [23] \"arcticgraylingSEF\"                      \"atlanticcroakerLATideLt0.5m\"           \n##  [25] \"atlanticcroakerLATideMt0.5m\"            \"atlanticcroakerOtherTideLt0.5m\"        \n##  [27] \"atlanticcroakerOtherTideMt0.5m\"         \"atlanticcroakerWetlandLATideLt0.5m\"    \n##  [29] \"atlanticcroakerWetlandLATideMt0.5m\"     \"atlanticcroakerWetlandOtherTideLt0.5m\" \n##  [31] \"atlanticcroakerWetlandOtherTideMt0.5m\"  \"bairdssparrow\"                         \n##  [33] \"baldeagleBreeding\"                      \"barredowl\"                             \n##  [35] \"beaverLacAreaLt8ha\"                     \"beaverLacAreaMtoe8ha\"                  \n##  [37] \"beaverPalu\"                             \"beaverRiv\"                             \n##  [39] \"beltedkingfishLenticConstWave\"          \"beltedkingfishLenticNoConstWave\"       \n##  [41] \"beltedkingfishLotic\"                    \"bigmouthbuffaloLacNoSal\"               \n##  [43] \"bigmouthbuffaloLacSal\"                  \"bigmouthbuffaloRivNoSal\"               \n##  [45] \"bigmouthbuffaloRivSal\"                  \"blackbear\"                             \n##  [47] \"blackbelliedwhistlingduck\"              \"blackbrant\"                            \n##  [49] \"blackbullheadLac\"                       \"blackbullheadRiv\"                      \n##  [51] \"blackcappedchickadeeFoodCanH\"           \"blackcappedchickadeeFoodCanVol\"        \n##  [53] \"blackcrappieLacNoSal\"                   \"blackcrappieLacSal\"                    \n##  [55] \"blackcrappieRivNoSal\"                   \"blackcrappieRivSal\"                    \n##  [57] \"blacknosedaceLac\"                       \"blacknosedaceRiv\"                      \n##  [59] \"blackshoulderedkite\"                    \"blacktailedprairiedog\"                 \n##  [61] \"bluegillLac\"                            \"bluegillRiv\"                           \n##  [63] \"bluegrouse\"                             \"blueherringJuv\"                        \n##  [65] \"blueherringJuvAndSAEL\"                  \"blueherringSAEL\"                       \n##  [67] \"bluewingedtealBreeding\"                 \"bobcatLt4ha\"                           \n##  [69] \"bobcatMtoe4ha\"                          \"brewerssparrow\"                        \n##  [71] \"brooktroutLacAllLtoe15C\"                \"brooktroutLacAllMt15C\"                 \n##  [73] \"brooktroutRivAllLtoe15CLtoe5mEC\"        \"brooktroutRivAllLtoe15CMt5mEC\"         \n##  [75] \"brooktroutRivAllMt15CLtoe5mEC\"          \"brooktroutRivAllMt15CMt5mEC\"           \n##  [77] \"brownshrimpNGulfofMex\"                  \"brownthrasher\"                         \n##  [79] \"browntroutCompLtoe10C\"                  \"browntroutCompMt10C\"                   \n##  [81] \"browntroutLimitLtoe10C\"                 \"browntroutLimitMt10C\"                  \n##  [83] \"bullfrog\"                               \"cactuswren\"                            \n##  [85] \"canvasbackBreeding\"                     \"channelcatfishLac\"                     \n##  [87] \"channelcatfishRiv\"                      \"chinooksalmonComp5to10CSand\"           \n##  [89] \"chinooksalmonComp5to10CSilt\"            \"chinooksalmonCompLtoe5CSand\"           \n##  [91] \"chinooksalmonCompLtoe5CSilt\"            \"chinooksalmonCompMt10CSand\"            \n##  [93] \"chinooksalmonCompMt10CSilt\"             \"chinooksalmonLimit5to10CSand\"          \n##  [95] \"chinooksalmonLimit5to10CSilt\"           \"chinooksalmonLimitLtoe5CSand\"          \n##  [97] \"chinooksalmonLimitLtoe5CSilt\"           \"chinooksalmonLimitMt10CSand\"           \n##  [99] \"chinooksalmonLimitMt10CSilt\"            \"chumsalmonAlevin\"                      \n##  [ reached getOption(\"max.print\") -- omitted 251 entries ]\nHSImetadata %>%\n  select(model, submodel, Eqtn) %>% # Select which columns to display\n  slice(1,4:6) # Select which rows to display##                                   model                     submodel            Eqtn\n## 1                            alewifeJuv    Juvenile life stage model    min(CF, CWQ)\n## 2             americanalligatorNontidal             Nontidal wetland (CCB*CCN)^(1/2)\n## 3                americanalligatorTidal               Tidal wetlands (CCB*CCN)^(1/2)\n## 4 americanblackduckWinteringEVegWetland Estuarine vegetated wetlands              CF"},{"path":"habitat-suitability-index-models-with-ecorest.html","id":"explore-the-marsh-wren-hsi-model","chapter":"7 Habitat Suitability Index models with ecorest","heading":"7.3.1 Explore the Marsh Wren HSI model","text":"use Marsh Wren HSI model, extract HSImodels object.Print Marsh Wren model console look structure:can see, ’s series suitability curves ordered parameter breakpoints associated suitability indices parameter. column environmental variable (e.g., emerg.hydrophytes.class) followed column (whose name ends .SIV) suitability index values associated environmental predictor.marsh wren HSI model can also viewed graphically. HSIplotter function creates JPEG image file images folder working directory. user selects name file, keep “.jpg” file extension.","code":"\nwren_hsi <- HSImodels$marshwren\nprint(wren_hsi)##   emerg.hydrophytes.class emerg.hydrophytes.SIV emerg.herb.can.cov.pct emerg.herb.can.cov.SIV\n## 1                       a                   1.0                      0                    0.0\n## 2                       b                   0.5                     50                    0.1\n## 3                       c                   0.1                     80                    1.0\n## 4                       d                   0.0                    100                    1.0\n##   avg.wtr.d.cm avg.wtr.d.cm.SIV woody.can.cov.pct woody.can.cov.SIV\n## 1            0                0                 0                 1\n## 2           15                1               100                 0\n## 3           40                1                NA                NA\n## 4           NA               NA                NA                NA\nHSIplotter(wren_hsi, \"images/marshwren_hsi.jpg\")"},{"path":"habitat-suitability-index-models-with-ecorest.html","id":"calculate-suitability-values-from-environmental-variables","chapter":"7 Habitat Suitability Index models with ecorest","heading":"7.3.2 Calculate suitability values from environmental variables","text":"calculate acres habitat project produce, need estimates environmental variables make chosen HSI model. Marsh Wren, variables include:Growth form emergent hydrophytesCanopy cover emergent herbaceous vegetationMean water depthCanopy cover woody vegetationValues variables can imported .csv file, can entered manually . take values row table corresponds target year 1 (TY1). Environmental variables must order appear HSI model. Categorical variables, emergent hydrophyte growth form, coded letters.order calculate habitat units, also need provide project area acres. provide area listed TY1 .Next, use SIcalc function calculate suitability value variable model. SIcalc function computes suitability indices using two inputs: suitability model (wren_hsi) project-specific environmental variables ’ve provided (`env_vars).","code":"\nenv_vars <- tibble(v1 = \"a\",\n                   v2 = 4,\n                   v3 = 99.1,\n                   v4 = 0)\narea = 34\nsi_vars <- SIcalc(wren_hsi, env_vars)\nprint(si_vars)## [1] 1.000 0.008 1.000 1.000"},{"path":"habitat-suitability-index-models-with-ecorest.html","id":"aggregate-hsi-value-and-habitat-units","chapter":"7 Habitat Suitability Index models with ecorest","heading":"7.3.3 Aggregate HSI value and Habitat Units","text":"crucial part HSI modeling determining SI values different variables combined create overarching HSI score. SI values can combined numerous ways (e.g., geometric mean, minimum value, etc.). bluebook models, however, identify species/submodel-specific way combine SI values, according specific attributes species. Therefore, bluebook model used, HSIeqtn function used calculate cummulative HSI score using appropriate equation. function requires name HSI model, variables individual metrics ’ve calculated, HSImetadata file, euqation stored.calculated cummulative HSI score separate SI values, can lastly calculate Habitat Units associated given project.","code":"\ntotal_HSI <- HSIeqtn(\"marshwren\", si_vars, HSImetadata)\nprint(total_HSI)## [1] 0.2\nTotal_hab <- total_HSI*area\nprint(Total_hab)## [1] 6.8"},{"path":"habitat-suitability-index-models-with-ecorest.html","id":"habitat-units-for-multiple-years-or-alternative-projects","chapter":"7 Habitat Suitability Index models with ecorest","heading":"7.3.4 Habitat Units for multiple years or alternative projects","text":"multiple project alternatives exist, multiple years environmental data, can perform operations using code. , -loop allows us iterate scenarios/years calculate SI values habitat unitsTable 7.1: Habitat Suitability Index (HSI) Variables \nResulting Outputs Marsh Wren Model -Project.results ’ve generated using ecorest package identical ones report, can see:\nbeauty carrying R using script can change input value reproduce results matter seconds re-running code, instead go steps manually. using ecorest package means instant access hundreds quality-controlled bluebook models.","code":"\n# Create dataframe of environmental variables for 4 scenarios/years\nnew_env_vars <- tibble(V1 = rep(\"a\", 4),\n                       V2 = c(5, 8.5, 12, 16),\n                       V3 = c(94.5, 89.9, 85.3, 80.8),\n                       V4 = c(0, 0, 0, 0))\n\nnyears <- nrow(new_env_vars) # Number of years/alternatives\nnvars <- ncol(new_env_vars) # Number of environmental variables\n\nnew_area = c(68, 102, 136, 170)\n\n# Create an empty matrix and vector to hold the results of the SI and HSI calculations, respectively\nnew_si_vars <- matrix(0, nrow=nyears, ncol=nvars)\nnew_total_HSI <- rep(NA, 4)\n\n# Use a for-loop to calculate SI values and HSI scores for each year/scenario\nfor (i in 1:nyears) {\n  new_si_vars[i, ] <- SIcalc(wren_hsi, new_env_vars[i, ])\n  new_total_HSI[i] <- HSIeqtn(\"marshwren\", new_si_vars[i, ], HSImetadata)\n}\n\n# Create a dataframe of the new SI values\nnew_si_df <- as_tibble(new_si_vars) %>%\n  set_names(c(\"SI_V1\",\"SI_V2\",\"SI_V3\",\"SI_V4\"))\n\n# Create an output table with all of the input and output\noutput_tab_wren <- new_env_vars %>%\n  bind_cols(new_si_df) %>%\n  mutate(output_HSI = round(new_total_HSI, 2),\n         total_acres = new_area,\n         HUs = output_HSI*total_acres)\n\n# Print the table using the knitr package\nknitr::kable(output_tab_wren, \n             digits = 2, \n             padding = 2,\n             caption = \"Habitat Suitability Index (HSI) Variables and \n             Resulting Outputs for the Marsh Wren Model With-Project.\")"},{"path":"habitat-suitability-index-models-with-ecorest.html","id":"hsi-modeling-with-custom-model","chapter":"7 Habitat Suitability Index models with ecorest","heading":"7.4 HSI Modeling with custom model","text":"Sometimes, project use custom index model instead bluebook model calculate habitat units. example, South San Francisco Bay Shoreline Study, USACE-SPN collaborated ERDC’s Coastal Ecology Integrated Ecological Modeling, develop quantitative model tidal marsh restoration subsided former baylands sensitive multiple restoration measures, including import beneficially used material bathymetry lifts, ecotones refugia.used ecorest package habitat model caclulations.","code":""},{"path":"habitat-suitability-index-models-with-ecorest.html","id":"explore-model-and-data","chapter":"7 Habitat Suitability Index models with ecorest","heading":"7.4.1 Explore model and data","text":"Import .csv marsh model, print names variables, plot model. Note: plotting function work, model must read using base R function read.csv instead Tidyverse function read_csvNext, read environmental data associated alternative project designs.","code":"\n# Import marsh HSI model\nmarsh_model<-read.csv(\"data/SF-TM-HEI_InputData.csv\")\nprint(names(marsh_model))## [1] \"percent.tidal.range\"                                     \n## [2] \"percent.tidal.range.SIV\"                                 \n## [3] \"Average.Marsh.Age\"                                       \n## [4] \"Average.Marsh.Age.SIV\"                                   \n## [5] \"Percent.Marsh.within.500.or.200\"                         \n## [6] \"Percent.Marsh.within.500.or.200.SIV\"                     \n## [7] \"Percent.Marsh.Shoreward.Edge.with.Ecotone.Transition\"    \n## [8] \"Percent.Marsh.Shoreward.Edge.with.Ecotone.Transition.SIV\"\nHSIplotter(marsh_model,\"images/marsh_model.jpg\")\n# Dataset of environmental conditions associated with project alternatives\nmarsh_alts <- read.csv(\"data/SF-TM-HEI_FieldData.csv\")\n\n# marsh_alts %>% \n#   pivot_longer(V1:V4, names_to = \"variable\", values_to = \"value\") %>%\n#   ggplot() +\n#   geom_point(aes(variable, value))+\n#   facet_wrap(~alt, nrow = 15)\n\nprint(marsh_alts)##      alt  V1  V2  V3  V4\n## 1   FWOP   0   0   0   0\n## 2  Alt1a  20 100   0   0\n## 3  Alt1b  50 100   0   0\n## 4  Alt1c 100 100   0   0\n## 5  Alt2a  20 100  20 100\n## 6  Alt2b  50 100  20 100\n## 7  Alt2c 100 100  20 100\n## 8  Alt3a  20 100 100 100\n## 9  Alt3b  50 100 100 100\n## 10 Alt3c 100 100 100 100\n## 11 Alt3d 100  20 100 100\n## 12 Alt3e 100  10 100 100\n## 13 Alt3f 100   1 100 100\n## 14 Alt3g 100   0 100 100\n## 15 Alt3h 100   0   0   0"},{"path":"habitat-suitability-index-models-with-ecorest.html","id":"calculate-individual-suitability-indices","chapter":"7 Habitat Suitability Index models with ecorest","heading":"7.4.2 Calculate individual suitability indices","text":", create empty matrix store results calculate SI values parameter using SICalc().","code":"\n # Create a dataframe of just environmental variables associated with each alternative\nenv_marsh <- select(marsh_alts, -alt)\nnalternatives<-length(marsh_alts$alt) # Number of alternatives\n\n# Create empty matrix to store SI values for each alternative\nsi_marsh <- matrix(NA, nrow=nalternatives, ncol=4)\n\n# Calculate the SI values for each alternative using a for-loop\nfor(i in 1:nalternatives) {\n  si_marsh[i, ] <- SIcalc(marsh_model, env_marsh[i, ])\n}\n\n# Convert matrix of SI values to dataframe and name the columns\nsi_marsh_df <- as_tibble(si_marsh) %>%\n  set_names(paste0(\"SI_\", names(env_marsh)))"},{"path":"habitat-suitability-index-models-with-ecorest.html","id":"calculate-the-total-hsi-score","chapter":"7 Habitat Suitability Index models with ecorest","heading":"7.4.3 Calculate the Total HSI score","text":", analysis team developed custom formula combine different HSI scores:\nCube root((Tidal connectivity* Marsh Age)*(Ecotone + Refugia)/2)R syntax, code looks like \n((((SIV2*SIV1)(SIV4+SIV3)/2))^(1/3))calculate cummulative score, ’ll use mutate function create new column dataframe suitability index values, calculate total HSI values columns.","code":"\n# Calculate overall HSI from individual SI values\nsi_total <- si_marsh_df %>%\n  mutate(total_hsi = ((((SI_V4+SI_V3)/2)*(SI_V2*SI_V1))^(1/3)))"},{"path":"habitat-suitability-index-models-with-ecorest.html","id":"cummulative-hsi-and-habitat-units","chapter":"7 Habitat Suitability Index models with ecorest","heading":"7.4.4 Cummulative HSI and Habitat Units","text":"Next, ’ll calculate habitat units associated project, assuming total project area 300 acres.Finally, can create ouput table data.Table 7.2: SF Salt Marsh HSI Values Marsh Units.","code":"\n# Calculate habitat units by multiplying total HSI by project area (300 acres)\narea_marsh <- 300\n\nsi_total_hu<-si_total %>%\n  mutate(HUs = area_marsh*total_hsi)\noutput_tab <- bind_cols(marsh_alts, si_total_hu) %>%\n  set_names(c(\"Alternative\", \"PTR\", \"MA\", \"HTR\", \"E\", \"PTR-SI\", \"MA-SI\", \n              \"HTR-SI\", \"E-SI\", \"HSI-Total\", \"Marsh Units\"))\n\nknitr::kable(output_tab, caption=\"SF Salt Marsh HSI Values and Marsh Units.\", \n             align=\"c\", \n             digits = 2)"},{"path":"habitat-suitability-index-models-with-ecorest.html","id":"linking-steps-using-pipes","chapter":"7 Habitat Suitability Index models with ecorest","heading":"7.4.5 Linking steps using pipes","text":"last several operations conducted can done single “tidy” way using pipe operator %>%.final line code checks demonstrates two output dataframes identical.","code":"\narea_marsh = 300\n\noutput_tab_2 <- as_tibble(si_marsh) %>%\n  set_names(paste0(\"SI_\", names(env_marsh))) %>%\n  mutate(total_hsi = ((((SI_V4+SI_V3)/2)*(SI_V2*SI_V1))^(1/3)),\n         HUs = area_marsh*total_hsi) %>%\n  bind_cols(marsh_alts,.) %>% # This code is slightly different so that marsh_alts data will come first; the \".\" symbol indicates everything that has come before this function\n  set_names(c(\"Alternative\", \"PTR\", \"MA\", \"HTR\", \"E\", \"PTR-SI\", \"MA-SI\", \n              \"HTR-SI\", \"E-SI\", \"HSI-Total\", \"Marsh Units\"))\n\nall.equal(output_tab, output_tab_2)## [1] TRUE"},{"path":"habitat-suitability-index-models-with-ecorest.html","id":"summary-2","chapter":"7 Habitat Suitability Index models with ecorest","heading":"7.5 Summary","text":"ecorest represents flexible, reproducible tool carrying habitat modeling R environment, countless advantages ad-hoc spreadsheet approachesThe ecorest pacakge can instantly utilize > 350 USFWS Blue Book modelsecorest also enables users quickly implement custom HSI modelsUsing ecorest HSI modeling can greatly increase efficiency decrease likelihood errors habitat modeling","code":""},{"path":"ecorest-web-app.html","id":"ecorest-web-app","chapter":"8 Ecorest Web App","heading":"8 Ecorest Web App","text":"Authors: Colton Shaw (writing), Darixa Hernandez-Abrams (writing), Kyle McKay (writing), Ed Stowe (Adapting, editing)Last update: 2025-02-03Acknowledgements: material adapted ecorest webapp ","code":""},{"path":"ecorest-web-app.html","id":"background","chapter":"8 Ecorest Web App","heading":"8.1 Background","text":"previous module, learned build HSI models R using ecorest package. However, also webapp using ecorest package, tutorial, users can learn use webapp.ecorest webapp combines HSI CEICA capabilities, package ecorest, single graphical user interface. Users can access ecosrest data functions without need use R programming language. instructions conduct HSI CEICA using ecorest webapp.","code":""},{"path":"ecorest-web-app.html","id":"model-selection","chapter":"8 Ecorest Web App","heading":"8.2 Model Selection","text":"first step conduct Habitat Suitability Index analysis selecting appropriate model(s) study. Model Selection tab includes subtabs Choose model Visualize model.","code":""},{"path":"ecorest-web-app.html","id":"choose-model","chapter":"8 Ecorest Web App","heading":"8.2.1 Choose Model","text":"section allows user select model either existing USFWS HSI “Bluebook Model” (left panel) “User-Specified Model” (right panel).","code":""},{"path":"ecorest-web-app.html","id":"bluebook-model-left-panel","chapter":"8 Ecorest Web App","heading":"8.2.1.1 Bluebook model (left panel):","text":"“Bluebook Model”, click dropdown button select model choice.","code":""},{"path":"ecorest-web-app.html","id":"user-specified-model-right-panel","chapter":"8 Ecorest Web App","heading":"8.2.1.2 User-specified model (right panel):","text":"Check box upper-right side indicate user specified model used.Download example file guidance metadata formatting.\nmetadata include information model variable names used (SIVs).\nHeaders metadata belong first row, starting first column\nVariables headers values ignored\nVariables headers values allowed\nOmitting model name allowed\n\nInformation model belongs second row denoted columns.\ninformation available one metadata (e.g., submodel),leave cells header empty.\n\nInsert variable names SIV headers end names “.SIV”.\nmodel suitability curves example, add headers “SIV4”, “SIV5”,etc.\nmetadata include information model variable names used (SIVs).Headers metadata belong first row, starting first column\nVariables headers values ignored\nVariables headers values allowed\nOmitting model name allowed\nVariables headers values ignoredVariables headers values allowedOmitting model name allowedInformation model belongs second row denoted columns.\ninformation available one metadata (e.g., submodel),leave cells header empty.\ninformation available one metadata (e.g., submodel),leave cells header empty.Insert variable names SIV headers end names “.SIV”.model suitability curves example, add headers “SIV4”, “SIV5”,etc.Upload metadata .csv file first upload box.Download example file guidance model formatting.\nHeaders model variables belong first row, starting first column.\nfirst column header SIV1 variable name metadata except “.SIV” ending.\nField data first model variable (SIV) goes rows header/column.\nvalues may numeric (e.g., 1, 2, 3) categorical (e.g., ,b,c).\n\nsecond column header name SIV1 including “.SIV” ending.\nSuitable index values (SIVs) first model variable (SIV1) go cells header/column.\nvalues must numeric value 0-1.\n\nRepeat SIVs (e.g., .variable.name goes first, .variable.name.SIV goes second)\nmodel variable, number populated rows within field data pair must match number populated rows suitable index column. words, every field data point must suitable index value associated .\nRows columns values must continuous throughout file.\nHeaders model variables belong first row, starting first column.first column header SIV1 variable name metadata except “.SIV” ending.\nField data first model variable (SIV) goes rows header/column.\nvalues may numeric (e.g., 1, 2, 3) categorical (e.g., ,b,c).\nField data first model variable (SIV) goes rows header/column.values may numeric (e.g., 1, 2, 3) categorical (e.g., ,b,c).second column header name SIV1 including “.SIV” ending.\nSuitable index values (SIVs) first model variable (SIV1) go cells header/column.\nvalues must numeric value 0-1.\nSuitable index values (SIVs) first model variable (SIV1) go cells header/column.values must numeric value 0-1.Repeat SIVs (e.g., .variable.name goes first, .variable.name.SIV goes second)model variable, number populated rows within field data pair must match number populated rows suitable index column. words, every field data point must suitable index value associated .Rows columns values must continuous throughout file.Upload user-specified model .csv file second upload box.","code":""},{"path":"ecorest-web-app.html","id":"visualize-model","chapter":"8 Ecorest Web App","heading":"8.2.2 Visualize Model","text":"section allows user verify metadata suitability plots chosen model. model selected available USFWS bluebook models uploaded (user-specified), metadata suitability plots appear shown .upper box prints metadata chosen model lower box visualizes suitability indices model variable.metadata can downloaded Download Outputs section discussed towards end. produced suitability plots can downloaded right-clicking plots selecting “copy” “save” option.issues arise tab, verify uploaded metadata model data configured according input file formatting. See “Choose Model”section reference guide formatting information.","code":""},{"path":"ecorest-web-app.html","id":"habitat-suitability-calculator","chapter":"8 Ecorest Web App","heading":"8.3 Habitat Suitability Calculator","text":"choosing model, second step conducting HSI analysis selecting equation entering data compute patch quantity (Habitat Unit; HU) quality.\n### Manual input\nsubtab calculates HSI using inputs provided user can either single scenario (left panel) multiple scenarios (right panel). multiple scenarios, scenario entered separate rows. Examples multiple scenarios multiple project sites habitat patches.","code":""},{"path":"ecorest-web-app.html","id":"single-scenario-left-panel","chapter":"8 Ecorest Web App","heading":"8.3.0.1 Single scenario (left panel):","text":"Enter habitat patch size (numeric) associated patch quality project area.Select appropriate HSI function. bluebook models, select ‘HSIeqtn’.Enter numeric categorical values (e.g., , b, c) variable\nadd units inputs\nsuitability index variable appear input box.\nInsert numeric categorical values box accordingly.\nHSI outputs visible bottom left.\nadd units inputsEach suitability index variable appear input box.Insert numeric categorical values box accordingly.HSI outputs visible bottom left.","code":""},{"path":"ecorest-web-app.html","id":"multiple-scenario-right-panel","chapter":"8 Ecorest Web App","heading":"8.3.0.2 Multiple scenario (right panel):","text":"Select HSI function.‘Compiler’, select ‘Edit’ enter data table.Double click cells table enter data.Enter different scenario per row\nLeave last row(s) empty less 10 scenarios\n10 scenarios, use CSV Input subtab\nLeave last row(s) empty less 10 scenariosIf 10 scenarios, use CSV Input subtabClick ‘Calculate’ view outputs.","code":""},{"path":"ecorest-web-app.html","id":"csv-input","chapter":"8 Ecorest Web App","heading":"8.3.1 CSV input","text":"tab allows user insert data via .csv file process multiple scenarios one without need use ‘Manual Input’.","code":""},{"path":"ecorest-web-app.html","id":"hsi-calculator-left-panel","chapter":"8 Ecorest Web App","heading":"8.3.1.1 HSI calculator (left panel):","text":"panel allows user computes HSI quality multiple scenarios uploading .csv file field data.\n1. Download example format .csv file.\n2. Upload .csv file containing field data file upload box\n* File formatting conventions file include:\n- Headers must start 1st row column\n- header names must match model variable name\n- IMPORTANT bluebook models, names must match variable names listed ‘Manual Input tab left panel.\n- Bluebook models ’NA’ headers…\n- Field data starts second row populated column\n- Categorical bluebook models must appropriate letter match suitability lowercase (e.g., ,b,c)\n- Rows columns field data must contiguous throughout. empty rows columnes . Except ones NAs?\n- Must comma delimited file extension (.csv)\n* HSI outputs visible bottom right","code":""},{"path":"ecorest-web-app.html","id":"hu-calculator-right-panel","chapter":"8 Ecorest Web App","heading":"8.3.1.2 HU Calculator (right panel):","text":"panel computes habitat units given information habitat quantity (area) multiple scenarios. (Requires HSU multiple scenario completed!)Enter numeric value area\nadd units numeric inputs\nadd units numeric inputsSelect function\nShow – Requires vector weights\nHSIarimean – input required\nHSIgeomean – input required\nHSImin – input required\nHSIwarimean – Requires vector weights\nShow – Requires vector weightsHSIarimean – input requiredHSIgeomean – input requiredHSImin – input requiredHSIwarimean – Requires vector weightsUpload .csv file containing vector weights file upload box required\nfile containing example formatting field data can downloaded tab\nFile formatting conventions vector weights file include:\nLabels belong first row, starting first column\nWeighted values belong denoted columns 0 1\nnumber populated rows every column must equivalent\nRows weighted values must contiguous throughout column\nMust comma delimited file extension (.csv)\nHU outputs visible bottom\n\nfile containing example formatting field data can downloaded tabFile formatting conventions vector weights file include:\nLabels belong first row, starting first column\nWeighted values belong denoted columns 0 1\nnumber populated rows every column must equivalent\nRows weighted values must contiguous throughout column\nMust comma delimited file extension (.csv)\nHU outputs visible bottom\nLabels belong first row, starting first columnWeighted values belong denoted columns 0 1The number populated rows every column must equivalentRows weighted values must contiguous throughout columnMust comma delimited file extension (.csv)HU outputs visible bottom","code":""},{"path":"ecorest-web-app.html","id":"cost-effective-incremental-cost-analysis","chapter":"8 Ecorest Web App","heading":"8.4 Cost-effective Incremental Cost Analysis","text":"","code":""},{"path":"ecorest-web-app.html","id":"annualizer","chapter":"8 Ecorest Web App","heading":"8.4.1 Annualizer","text":"section computes time-averaged quantities based linear interpolation.Upload .csv file containing numeric vector time intervals first file upload box (timevec) .csv file containing numeric vector values interpolated second upload box (benefits)\nfile containing example formatting input files can found upload box\nFile formatting conventions vector values file vector time intervals file include:\nContains single column\nLabel belongs first cell\nValues must numeric\nColumn values must contiguous throughout\nfiles must contain equivalent populated row counts\n-Must comma delimited file extension (.csv)\n\nfile containing example formatting input files can found upload boxFile formatting conventions vector values file vector time intervals file include:\nContains single column\nLabel belongs first cell\nValues must numeric\nColumn values must contiguous throughout\nfiles must contain equivalent populated row counts\n-Must comma delimited file extension (.csv)\nContains single columnLabel belongs first cellValues must numericColumn values must contiguous throughoutBoth files must contain equivalent populated row counts\n-Must comma delimited file extension (.csv)single time averaged quantity appear \nvalue may extracted highlighting value cursor, right clicking, selecting copy\nvalue may extracted highlighting value cursor, right clicking, selecting copy","code":""},{"path":"ecorest-web-app.html","id":"annualizer-1","chapter":"8 Ecorest Web App","heading":"8.4.2 Annualizer","text":"section computes plots Cost-effective Incremental Cost Analysis.Upload .csv file containing Cost-effective incremental Cost Analysis (CEICA) data file drop location .\nfile containing example formatting input files can found upload box\nFile formatting conventions vector values file vector time intervals file include:\nContains three columns, starting first cell\nLabel belongs first row order: altnames, benefit, cost\nAltnames values string\nBenefit cost values must numeric\nColumn row values must contiguous\nrow must contain equivalent populated row counts\nMust comma delimited file extension (.csv)\nfile containing example formatting input files can found upload boxFile formatting conventions vector values file vector time intervals file include:Contains three columns, starting first cellLabel belongs first row order: altnames, benefit, costAltnames values stringBenefit cost values must numericColumn row values must contiguousEach row must contain equivalent populated row countsMust comma delimited file extension (.csv)Two plots appear, cost-effective analysis bottom left, incremental cost analysis bottom right\nplots can downloaded right-clicking plots selecting “copy” “save” option.\nplots can downloaded right-clicking plots selecting “copy” “save” option.","code":""},{"path":"ecorest-web-app.html","id":"download-outputs","chapter":"8 Ecorest Web App","heading":"8.5 Download Outputs","text":"","code":""},{"path":"ecorest-web-app.html","id":"export-files","chapter":"8 Ecorest Web App","heading":"8.5.1 Export Files","text":"section allows user download metadata, SI, HU results .csv file. files can compiled single file downloadable file “Generate Report” button.","code":""},{"path":"ecorest-web-app.html","id":"references","chapter":"8 Ecorest Web App","heading":"8.6 References","text":"Robinson R. Hansen W., Orth K. 1995. Evaluation environmental investments procedures manual interim: Cost effectiveness incremental cost analyses. IWR Report 95-R-1. Institute Water Resources, U.S. Army Corps Engineers, Alexandria, Virginia.USFW 1991","code":""},{"path":"sensitivity-and-uncertainty-analysis-for-habitat-suitability-index-hsi-models.html","id":"sensitivity-and-uncertainty-analysis-for-habitat-suitability-index-hsi-models","chapter":"9 Sensitivity and uncertainty analysis for habitat suitability index (HSI) models","heading":"9 Sensitivity and uncertainty analysis for habitat suitability index (HSI) models","text":"module teach assess sensitivity uncertainty HSI models using sensobol package R .Authors: Kiara Cushway, Ed Stowe, Todd Swannack, Kyle McKayLast update: 2025-08-20Acknowledgements: Much background information oyster reefs habitat suitability index model used module come Swannack et al. (2014). Information conduct sensitivity uncertainty analyses using sensobol comes Puy et al. (2022).","code":""},{"path":"sensitivity-and-uncertainty-analysis-for-habitat-suitability-index-hsi-models.html","id":"learning-objectives-6","chapter":"9 Sensitivity and uncertainty analysis for habitat suitability index (HSI) models","heading":"9.1 Learning objectives","text":"Understand benefits sensitivity uncertainty analysis HSI models.Learn conduct variance-based sensitivity analysis using sensobol package R.::::::::::::::::::::::::::::::::::::::::::::::::","code":""},{"path":"sensitivity-and-uncertainty-analysis-for-habitat-suitability-index-hsi-models.html","id":"benefits-of-uncertainty-and-sensitivity-analysis-for-hsi-modeling","chapter":"9 Sensitivity and uncertainty analysis for habitat suitability index (HSI) models","heading":"9.2 Benefits of uncertainty and sensitivity analysis for HSI modeling","text":"Habitat suitability index (HSI) modeling widely applied method habitat evaluation, success relies good data model frameworks. Various sources uncertainty can limit utility HSI models, usually represent habitat quality single index value indication confidence value (Bender et al. 1996; Burgman et al. 2001). Uncertainty analysis (UA) sensitivity analysis (SA) can help modelers practitioners explore quantify sources uncertainty HSI models (Razavi et al. 2021), providing confidence results ensuring models successfully capture ecological relationships observed field. UA assesses confidence model’s results, SA identifies model parameters contribute uncertainty (Pianosi et al. 2016).module, conduct variance-based global sensitivity uncertainty analysis HSI models using oyster habitat suitability index model (OHSIM) developed Swannack et al. (2014).","code":""},{"path":"sensitivity-and-uncertainty-analysis-for-habitat-suitability-index-hsi-models.html","id":"variance-based-global-sensitivity-and-uncertainty-analysis-with-sensobol","chapter":"9 Sensitivity and uncertainty analysis for habitat suitability index (HSI) models","heading":"9.3 Variance-based global sensitivity and uncertainty analysis with sensobol","text":"Variance measure variable data . Variance-based UA/SA assess deviations values model input parameters contribute variation/uncertainty HSI scores. test much given parameter influences variance output, can estimate average variance model’s output one input parameter model allowed vary, specific parameter fixed every possible value across entire range uncertainty (Puy et al. 2022).variety approaches conducting sensitivity analyses (see Pianosi et al. 2016 options). use global, ---time (AAT) methods module using sensobol package R. Global methods test parameter inputs across entire range possible values rather subset values. ---time methods vary input parameters simultaneously, enables measurement interactive effects parameters output uncertainty (Pianosi et al. 2016).sensobol package allows users calculate Sobol’ indices, focus two main ones:Si: first-order index describes much variance output decrease parameter question fixed.Ti: total order index describes first-order effect parameter, plus effect parameter interacting parameters model (Puy et al. 2022).remainder module, conduct UA/SA OHSIM model using sensobol.need sensobol, ecorest, tidyverse packages analysis.","code":"\nlibrary(sensobol)\nlibrary(ecorest)\nlibrary(tidyverse)"},{"path":"sensitivity-and-uncertainty-analysis-for-habitat-suitability-index-hsi-models.html","id":"habitat-suitability-modeling-for-oyster-reef-restoration","chapter":"9 Sensitivity and uncertainty analysis for habitat suitability index (HSI) models","heading":"9.4 Habitat suitability modeling for oyster reef restoration","text":"Eastern Oyster (Crassostrea virginica) benthic mollusk found estuarine ecosystems, ’s important ecosystem engineer provider ecosystem services (Grabowski Peterson 2007; Swannack et al. 2014). Oyster populations decline, several efforts implemented restore oyster reefs U.S.HSI models used quantify available oyster habitat identify areas high potential successful restoration (Swannack et al. 2014). facilitate reef restoration activities, Swannack et al. (2014) developed spatially explicit oyster habitat suitability index model (OHSIM) incorporates salinity substrate variables assess oyster habitat. variables include:Percent cultch: percentage waterbody’s bottom composed hard substrates, essential larval development.Percent cultch: percentage waterbody’s bottom composed hard substrates, essential larval development.Mean salinity spawning season (MSSS): average daily salinity May 1 Sept. 30, reflecting need higher salinity spawning larval development.Mean salinity spawning season (MSSS): average daily salinity May 1 Sept. 30, reflecting need higher salinity spawning larval development.Minimum annual salinity (MAS): minimum annual salinity, quantifies effects freshwater pulses oyster habitat.Minimum annual salinity (MAS): minimum annual salinity, quantifies effects freshwater pulses oyster habitat.Annual mean salinity (): average annual salinity, captures suitable conditions adult oysters.Annual mean salinity (): average annual salinity, captures suitable conditions adult oysters.relationship variables habitat suitability described using oyster suitability indices (OSIs). import oyster suitability curves, little data wrangling, plot ggplot.Oyster habitat suitability indices (OSIs) based important habitat parameters Eastern Oyster (Crassostrea virginica).Uncertainty OSIs contributes uncertainty habitat suitability. much OSI contributes can investigated sensitivity analysis.","code":"\n# Bring in data\n#load(\"ohsim.RData\")\nohsim <- read.csv(\"data/ohsim_orig.csv\")\n\n# Create data frame of variable names for plotting\nohsim_names <- data.frame(variable = c(\"AS\", \"MAS\", \"MSSS\", \"PercentCultch\"),\n                          name = c(\"Annual mean salinity (AS)\",\n                          \"Mean salinity during spawning season (MSSS)\",\n                          \"Minimum annual salinity (MAS)\",\n                          \"Percent cultch\"))\n\n# Pivot oyster dataframe to long format for plotting\nohsim_long <- ohsim %>%\n  pivot_longer(cols = everything(),\n                 names_to = c(\"variable\", \".value\"),\n                 names_sep = \"_\") %>%\n  left_join(ohsim_names) %>%      # Join name data frame\n  arrange(variable) %>%           # Sort by variable\n  drop_na()                       # Remove NA values\n\nggplot(ohsim_long,\n       aes(val, OSI))+\n  geom_point()+\n  geom_line()+\n  facet_wrap(~name, scales = \"free\")+\n  labs(x = \"Parameter value\", y = \"Habitat suitability\") +\ntheme_classic()"},{"path":"sensitivity-and-uncertainty-analysis-for-habitat-suitability-index-hsi-models.html","id":"restoration-suitability-index","chapter":"9 Sensitivity and uncertainty analysis for habitat suitability index (HSI) models","heading":"9.4.1 Restoration suitability index","text":"Swannack et al. (2014) combined OSIs restoration suitability index (RSI) using geometric mean equation, calculated multiplying n numbers together taking nth root (e.g., three numbers, multiply three together take cube root). typical reason use geometric mean modeler believes input parameters value zero, total suitability also zero.can use HSIgeomean function ecorest code geometric mean. input HSIgeomean vector OSIs.","code":""},{"path":"sensitivity-and-uncertainty-analysis-for-habitat-suitability-index-hsi-models.html","id":"creating-a-sobol-matrix-to-generate-sample-data-for-analysis","chapter":"9 Sensitivity and uncertainty analysis for habitat suitability index (HSI) models","heading":"9.4.2 Creating a Sobol’ matrix to generate sample data for analysis","text":"first step analysis create sample matrix using sobol_matrices function sensobol package. matrix serves basis sampling scheme, provide sufficient coverage entire sampling space, .e., possible combinations parameters included model (Puy et al. 2022). larger number parameters, number dimensions making sampling space also grows, samples needed explore possible combinations. rule thumb determining many samples required use equation:\\(N = n * (k + 2)\\), N total number model evaluations, n base sample size greater 500 (Saltelli et al. 2010), k number input parameters model (Pianosi Wagener 2015). sobol_matrices function, N automatically calculated user enters base sample size (referred N function) vector containing names parameters (params argument). use base sample size 10,000 evaluation today, adjust results sensitivity analysis indicate sample size sufficient (.e., results contain lot noise). input settings result total 60,000 model evaluations analysis.creating sample matrix, follow recommendations generate , B, AB matrices (Puy et al. 2022). default matrices generated sobol_matrices (Puy et al. 2022). Setting matrices = c(\"\", \"B\", \"AB\") tells sobol_matrices function want generate matrix N rows (case, N = 60,000), row represents set OSI values can input RSI. total n (case, n = 10,000) rows matrix make matrix, another n rows make B matrix. remaining n x k rows (case, 40,000) make AB matrix, individual row OSI samples match row matrix, expect one OSI value (-th column) drawn B matrix. calculate RSI value every row sample matrix.Next, select order effects interested investigating; evaluate first order total order effects parameters, although sensobol capable evaluating fourth order effects (Puy et al. 2022).Finally, construct sample matrix, need select sampling scheme. determines total input space sampled (.e., sample values selected possible combinations input parameters). use Quasi-random sampling (“QRN”), default sobol_matrices (see Puy et al. 2022 options).sample matrix generate used assess model uncertainty sensitivity, obtain Si Ti. calculate Si, compare RSI values calculated sets sampling points vary respect parameters except one (parameter xi) (Puy et al. 2022). calculate Ti, compare RSI values sets sampling points parameters value except parameter xi, vary (Puy et al. 2022).set preview sample matrix :sample_matrix now contains series model inputs OSI need converted specific probability distributions. probability distribution describes likelihood given outcome changes across distribution possible outcomes. example, uniform probability distribution, parameter values equal chance occurring, whereas normal distribution, parameter values likely occur closer mean. probability distribution depend knowledge parameter. general, sufficient information input parameter’s uncertainty distributed, best practice use uniform distribution (Puy et al. 2022). However, use particular probability distribution requires careful consideration, choice can affect outcome sensitivity analysis (Puy et al. 2022).analysis, don’t much information percent cultch, use uniform distribution. Estuarine salinity, however, influenced semi-predictable seasonal factors like evaporation river discharge, certain values tend common others. check assumption, ’ll look sample data estuary systems Florida located near Choctawhatchee Bay, using data National Water Quality Monitoring Council’s Water Quality Portal (https://www.waterqualitydata.us/) .Based distributions, minimum annual salinity annual mean salinity follow somewhat normal distribution, averages around 13 20 ppt, respectively. Mean salinity spawning season flatter distribution, ’ll use uniform distribution MSSS, normal distribution MAS . implement normal probability distributions, use sample mean standard deviation qnorm function inform sampling scheme.Although want sensitivity analysis explore entire input variability space, may less interested portion input space OSI equal zero. Furthermore, including zero ranges analysis result many RSI scores zero, presents challenges. Since sensitivity indices calculated based output variance, outputs zero (resulting low variance), hard detect OSI parameters inducing variance model. Given , parameters uniform distribution, define entire input variability range input space OSI greater zero. words, MSSS, sample space salinity falls 5 40 ppt, rather sampling across range presented OSI (0-40 ppt).","code":"\n# Set seed for repeatability\nset.seed(73)\n\n# Define parameter names\nparams = c(\"PercentCultch_val\", \"MSSS_val\", \"MAS_val\", \"AS_val\")\n\n# Create a sample matrix of probabilities to compute Sobol' indices\nsample_matrix = sobol_matrices(matrices = c(\"A\", \"B\", \"AB\"), N = 10000, params = params, order = \"first\", type = \"QRN\")\n\n# Preview sample matrix\nhead(sample_matrix)##      PercentCultch_val MSSS_val MAS_val AS_val\n## [1,]             0.500    0.500   0.500  0.500\n## [2,]             0.750    0.250   0.750  0.250\n## [3,]             0.250    0.750   0.250  0.750\n## [4,]             0.375    0.375   0.625  0.125\n## [5,]             0.875    0.875   0.125  0.625\n## [6,]             0.625    0.125   0.375  0.375\n# For each parameter in the model, use the probability in sample_matrix to obtain a sample value from a probability distribution function\nsample_matrix[, \"PercentCultch_val\"] = qunif(sample_matrix[, \"PercentCultch_val\"], min = 0, max = 100)\nsample_matrix[, \"MSSS_val\"] = qunif(sample_matrix[, \"MSSS_val\"], min = 5, max = 40)\nsample_matrix[, \"MAS_val\"] = qnorm(sample_matrix[, \"MAS_val\"], mean = 13.2, sd = 4.85)\nsample_matrix[, \"AS_val\"] = qnorm(sample_matrix[, \"AS_val\"], mean = 19.85, sd = 2.88)"},{"path":"sensitivity-and-uncertainty-analysis-for-habitat-suitability-index-hsi-models.html","id":"calculating-osi-and-rsi-scores-with-sample-data","chapter":"9 Sensitivity and uncertainty analysis for habitat suitability index (HSI) models","heading":"9.4.3 Calculating OSI and RSI scores with sample data","text":"Now sample data prepared, can calculate OSI score parameter every row sample_matrix. using SIcalc function ecorest. SIcalc function takes matrix suitability curves (case, OSI_curves) uses calculate OSI scores series inputs. applying SIcalc function row-wise sample_matrix using “-loop,” can obtain OSI scores 60,000 sets sample data.Finally, using OSI scores calculated sample_matrix last step, can apply HSIgeomean function set sampling points order generate empirical distribution RSI scores. use variance present empirical distribution understand uncertainty RSI values interpret sensitive RSI model changes input parameters.","code":"\n# Create empty data frame with the same dimensions as sample_matrix\nOSI_scores = data.frame(matrix(NA, nrow = nrow(sample_matrix), ncol = ncol(sample_matrix)))\n\n# Convert sample data to OSI scores for each row of the sample_matrix\nfor (row in 1:nrow(sample_matrix)) {\n  OSI_scores[row,] = SIcalc(SI = ohsim, input.proj = as.numeric(sample_matrix[row,]))\n}\n# Create empty vector to store RSI scores\nRSI = vector()\n\n# Calculate geometric mean based on OSI_scores\nfor (row in 1:nrow(sample_matrix)) {\n  RSI[row] = HSIgeomean(OSI_scores[row,])\n}"},{"path":"sensitivity-and-uncertainty-analysis-for-habitat-suitability-index-hsi-models.html","id":"examining-uncertainty-in-restoration-suitability","chapter":"9 Sensitivity and uncertainty analysis for habitat suitability index (HSI) models","heading":"9.4.4 Examining uncertainty in restoration suitability","text":"vector RSI now contains results model evaluations generated sample_matrix. can examine distribution RSI values across parameter uncertainty using plot_uncertainty function sensobol, generates empirical distribution RSI scores.Based distribution, RSI scores spread across entire possible output range (0-1).can quantify variability RSI values ways. example, can use quantile function assess 95% confidence intervals.suggests RSI score falls 0.14 0.87 ninety-five percent time.can calculate typical summary statistics standard deviation empirical distribution RSI scores well:Based results, across probability distributions OSI values, RSI scores commonly fall around 0.5, 50% OSI conditions resulting scores 0.37 0.66.mean USACE modeling planning context? Well, want make sure HSI model functions expected, whether ’re designing new HSI testing model new location. , allowed OSI values vary widely, included values expected poor oysters. Consequently, expect overall RSI also exist across wide range suitability values. hadn’t done , might indicated potential problems model structure, individual OSIs, input parameter distributions. unexpected outcome overall HSI occurs stage, either new model model new context, suggests model may need tweaked.","code":"\n# Plot uncertainty\nplot_uncertainty(Y = RSI, N = 10000) + labs(x = \"RSI score\")\n# Generate quantiles from RSI values\nquantile(RSI, probs = c(0.025, 0.975))##      2.5%     97.5% \n## 0.1401421 0.8690388\n# Calculate summary statistics for RSI\nsummary(RSI)##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##  0.0000  0.3736  0.5163  0.5140  0.6589  0.9984\nsd(RSI)## [1] 0.1948817"},{"path":"sensitivity-and-uncertainty-analysis-for-habitat-suitability-index-hsi-models.html","id":"using-sensitivity-analysis-to-assess-the-relative-importance-of-input-parameters","chapter":"9 Sensitivity and uncertainty analysis for habitat suitability index (HSI) models","heading":"9.4.5 Using sensitivity analysis to assess the relative importance of input parameters","text":"now use sensitivity analysis understand model parameters contribute uncertainty RSI values. calculate sensitivity indices sensobol, use sobol_indices function:’ll use default settings arguments sobol_indices (see Puy et al. 2022 detailed explanations settings). code Y vector RSI scores, N base sample size specified sample matrix, params list parameter names. use boot = \"TRUE\" type = \"percent\" calculate confidence intervals using percents.Walking results, sum first-order point estimates 0.959, meaning ~96% uncertainty RSI variables can attributed influence individual parameters. Hence, OSIs model mostly act independently explaining variance RSI scores, interactive effects small.table summarizes Sobol’ Si Ti indices, standard errors, confidence intervals parameter model. results suggest RSI model sensitive mean salinity spawning season, accounts somewhere 41 52% uncertainty RSI values (Si) 48 51% uncertainty RSI combination parameters (Ti). Percent cultch contributes second model uncertainty, followed minimum annual salinity (MAS) annual mean salinity ().Knowing different parameters contribute uncertainty RSI scores beneficial several reasons. can help :Prioritize data collection parameters influential habitat suitability.Remove unnecessary parameters model.Ensure structure function model reflects habitat requirements species.example, parameter known important habitat suitability HSI model insensitive parameter, suggests model structure sufficiently reflect overall suitability.","code":"\n# Compute Sobol' sensitivity indices\nsensitivity_indices = sobol_indices(matrices = c(\"A\", \"B\", \"AB\"), Y = RSI, N = 10000, params = params, first = \"saltelli\", total = \"jansen\", order = \"first\", boot = TRUE, R = 1000, parallel = \"no\", conf = 0.95, type = \"percent\")\n\n# View results\nsensitivity_indices$si.sum## [1] 0.9587992\nsensitivity_indices$results %>%\n  as.data.frame() %>%\n  mutate(across(where(is.numeric), ~round(.x,3)))##   original   bias std.error low.ci high.ci sensitivity        parameters\n## 1    0.292 -0.001     0.020  0.252   0.329          Si PercentCultch_val\n## 2    0.464  0.000     0.026  0.411   0.516          Si          MSSS_val\n## 3    0.148  0.000     0.012  0.124   0.174          Si           MAS_val\n## 4    0.055  0.001     0.010  0.037   0.076          Si            AS_val\n## 5    0.318  0.000     0.006  0.306   0.329          Ti PercentCultch_val\n## 6    0.496  0.000     0.008  0.480   0.510          Ti          MSSS_val\n## 7    0.165  0.000     0.007  0.151   0.180          Ti           MAS_val\n## 8    0.063  0.000     0.001  0.060   0.066          Ti            AS_val"},{"path":"sensitivity-and-uncertainty-analysis-for-habitat-suitability-index-hsi-models.html","id":"calculating-dummy-parameters-for-comparison","chapter":"9 Sensitivity and uncertainty analysis for habitat suitability index (HSI) models","heading":"9.4.6 Calculating dummy parameters for comparison","text":"know given Si Ti value actually signifies parameter influencing model?determine need threshold comparison. can consider Sobol’ indices threshold influential variance RSI. , ’ll calculate dummy parameter (Korashadi Zadeh et al. 2017; Puy et al. 2022). dummy parameter calculated along model inputs actually included model (Korashadi Zadeh et al. 2017). OHSIM sensitivity analysis, generate 60,000 RSI values using various combinations OSI variables. combination, also randomly pull store value 0 1 uniform distribution. value completely unrelated RSI score, meaning relationship RSI pure coincidence (.e., spurious). represents numerical approximation error model (Puy et al. 2022). parameter model dummy comparable influence RSI values, model parameter essentially unimportant within model ’s formulated.code generates values dummy parameters (one Si one Ti):Interpretation dummies similar Sobol’ indices, values represent noise occurring model rather contributions actual parameters. , 0.067 0.071% uncertainty RSI scores Si 0 6.958% Ti can attributed noise model. Sobol’ sensitivity indices fall within ranges, can say RSI model relatively insensitive parameters. Dummy parameters Sobol’ indices Si Ti calculated separately, parameter fail contribute RSI scores (Si) still important influence interactions variables (Ti), vice versa.intuitive summary results sensitivity analysis, can use plot function create bar chart results:plot, bars represent two Sobol’ indices. Horizontal dashed lines represent upper confidence interval dummy parameters Si Ti indices. RSI model insensitive OSI variables, error bars, represent 95% confidence intervals sensitivity index, overlap fall dashed lines Si Ti.Based dummy parameter confidence intervals plot, RSI scores sensitive four parameters model, annual mean salinity () important total order influence (.e., interact variables model)., type analysis can beneficial USACE context. Planners biologists creating new HSI model can use sensitivity analysis gauge importance functioning different potential parameters. model applied across spatially variable region, sensitivity model can examined different locations see parameter contributions uncertainty vary among different project locations.","code":"\n# Calculate dummy parameter\ndummy_indices = sobol_dummy(Y = RSI, N = 10000, params = params, boot = TRUE, R = 1000)\n\n# View dummy parameters\ndummy_indices %>%\n  mutate(across(where(is.numeric), ~round(.x,3)))##   original   bias std.error low.ci high.ci sensitivity parameters\n## 1    0.001  0.000     0.000  0.001   0.001          Si      dummy\n## 2    0.000 -0.003     0.037  0.000   0.070          Ti      dummy\n# Plot results of sensitivity index\nplot(sensitivity_indices, dummy = dummy_indices)"},{"path":"sensitivity-and-uncertainty-analysis-for-habitat-suitability-index-hsi-models.html","id":"summary-3","chapter":"9 Sensitivity and uncertainty analysis for habitat suitability index (HSI) models","heading":"9.5 Summary","text":"Uncertainty analysis sensitivity analysis (UA/SA) can greatly improve utility HSI modelsUA/SA enable users explore magnitude uncertainty models, assess role parameter playing model outcome uncertainty, determine models functioning expectedThe sensobol package R offers straightforward UA/SA method using global, ---time, variance-based analyses","code":""},{"path":"sensitivity-and-uncertainty-analysis-for-habitat-suitability-index-hsi-models.html","id":"references-1","chapter":"9 Sensitivity and uncertainty analysis for habitat suitability index (HSI) models","heading":"9.6 References","text":"Bender, L. C., Roloff, G. J., & Haufler, J. B. (1996). Evaluating confidence intervals habitat suitability models. Wildlife Society Bulletin (1973-2006), 24(2), 347–352.Brooks, R. P. (1997). Improving habitat suitability index models. Wildlife Society Bulletin (1973-2006), 25(1), 163–167.Burgman, M. ., Breininger, D. R., Duncan, B. W., & Ferson, S. (2001). Setting reliability bounds habitat suitability indices. Ecological Applications, 11(1), 70–78. https://doi.org/10.1890/1051-0761(2001)011[0070:SRBOHS]2.0.CO;2Grabowski, J. H., & Peterson, C. H. (2007). Restoring oyster reefs recover ecosystem services. Theoretical Ecology Series (Vol. 4, pp. 281–298). Elsevier. https://doi.org/10.1016/S1875-306X(07)80017-7Jansen, M. J. W. (1999). Analysis variance designs model output. Computer Physics Communications, 117(1–2), 35–43. https://doi.org/10.1016/S0010-4655(98)00154-4\nPianosi, F., Beven, K., Freer, J., Hall, J. W., Rougier, J., Stephenson, D. B., & Wagener, T. (2016). Sensitivity analysis environmental models: systematic review practical workflow. Environmental Modelling & Software, 79, 214–232. https://doi.org/10.1016/j.envsoft.2016.02.008Pianosi, F., & Wagener, T. (2015). simple efficient method global sensitivity analysis based cumulative distribution functions. Environmental Modelling & Software, 67, 1–11. https://doi.org/10.1016/j.envsoft.2015.01.004Puy, ., Piano, S. L., Saltelli, ., & Levin, S. . (2022). sensobol : R Package Compute Variance-Based Sensitivity Indices. Journal Statistical Software, 102(5). https://doi.org/10.18637/jss.v102.i05Razavi, S., Jakeman, ., Saltelli, ., Prieur, C., Iooss, B., Borgonovo, E., Plischke, E., Lo Piano, S., Iwanaga, T., Becker, W., Tarantola, S., Guillaume, J. H. ., Jakeman, J., Gupta, H., Melillo, N., Rabitti, G., Chabridon, V., Duan, Q., Sun, X., … Maier, H. R. (2021). Future Sensitivity Analysis: essential discipline systems modeling policy support. Environmental Modelling & Software, 137, 104954. https://doi.org/10.1016/j.envsoft.2020.104954Saltelli, ., Aleksankina, K., Becker, W., Fennell, P., Ferretti, F., Holst, N., Li, S., & Wu, Q. (2019). many published sensitivity analyses false: systematic review sensitivity analysis practices. Environmental Modelling & Software, 114, 29–39. https://doi.org/10.1016/j.envsoft.2019.01.012Saltelli, ., Annoni, P., Azzini, ., Campolongo, F., Ratto, M., & Tarantola, S. (2010). Variance based sensitivity analysis model output. Design estimator total sensitivity index. Computer Physics Communications, 181(2), 259–270. https://doi.org/10.1016/j.cpc.2009.09.018Sobol’, . M. (1967). distribution points cube approximate evaluation integrals. USSR Computational Mathematics Mathematical Physics, 7(4), 86–112. https://doi.org/10.1016/0041-5553(67)90144-9Sobol’, . M. (1976). Uniformly distributed sequences additional uniform property. USSR Computational Mathematics Mathematical Physics, 16(5), 236–242. 10. 1016/0041-5553(76)90154-3.Swannack, T. M., Reif, M., & Soniat, T. M. (2014). Robust, Spatially Explicit Model Identifying Oyster Restoration Sites: Case Studies Atlantic Gulf Coasts. Journal Shellfish Research, 33(2), 395–408. https://doi.org/10.2983/035.033.0208Zajac, Z., Stith, B., Bowling, . C., Langtimm, C. ., & Swain, E. D. (2015). Evaluation habitat suitability index models global sensitivity uncertainty analyses: case study submerged aquatic vegetation. Ecology Evolution, 5(13), 2503–2517. https://doi.org/10.1002/ece3.1520","code":""},{"path":"spatial-habitat-models.html","id":"spatial-habitat-models","chapter":"10 Spatial Habitat Models","heading":"10 Spatial Habitat Models","text":"Consider exploring “Geospatial suitability indices (GSI) toolbox : user’s guide”\nhttps://erdc-library.erdc.dren.mil/items/7e880360-d004-4601-9a42-eddd64233e34","code":""},{"path":"linear-models.html","id":"linear-models","chapter":"11 Linear Models","heading":"11 Linear Models","text":"module teach use linear models R assess relationships among environmental factors USACE-managed Upper Mississippi River.Authors: Ed StoweLast update: 2025-02-03Acknowledgements: inspiration/language/code used adapted permission : Ben Staton’s Intro R Natural Resources course Quebec Center biodiversity Science’s Linear model workshop","code":""},{"path":"linear-models.html","id":"relevance-of-linear-models-to-usace","chapter":"11 Linear Models","heading":"11.1 Relevance of linear models to USACE","text":"Linear models offshoots estimate relationship response variable one predictor variables. models used widely ecologists understand, example, environmental management factors important determining biological outcomes phenomena. Linear models several potential applications USACE projects ecological modeling practices:Validating updating relationships within existing HSI modelsCreating new habitat models based empirical data instead conjectureEstimating relationship focal taxa habitat characteristics order predict response species USACE habitat-altering projects (thus potentially acting alternative habitat model)","code":""},{"path":"linear-models.html","id":"learning-objectives-7","chapter":"11 Linear Models","heading":"11.2 Learning objectives","text":"Understand purpose structure linear modelRun linear model RInterpret output linear modelAssess linear model validity","code":""},{"path":"linear-models.html","id":"background-on-linear-models","chapter":"11 Linear Models","heading":"11.3 Background on linear models","text":"Linear models used estimate relationship () response variable one predictor variable(s). typically constructed analyze priori hypothesis variables correlated. results analysis indicate direction strength relationship variables, level confidence relationship.","code":""},{"path":"linear-models.html","id":"linear-model-formulation","chapter":"11 Linear Models","heading":"11.3.1 Linear model formulation","text":"linear model, response variable variable wish explain, also known dependent variable, value may depend values predictor variables. one response variable. Predictor variables (also known independent explanatory variables), hand, variables potential explain predict response variable. Linear models can multiple predictor variables.linear model, single observation response variable \\(y\\) defined \\(y_i\\), corresponding observation predictor variable \\(x\\) defined \\(x_i\\). values \\(y\\) \\(x\\) related using formula, describes relationship two variables straight line:\\[ y_i = \\beta_0 + \\beta_1 \\times x_i + \\epsilon_i\\]\\(y_i\\) response variable\\(x_i\\) predictorThe parameter \\(\\beta_0\\) interceptThe parameter \\(\\beta_1\\) quantifies effect** \\(x\\) \\(y\\)residual \\(\\epsilon_i\\) represents unexplained variationBasically, just reformulation classic middle school equation used graph straight line–\\(y = mx + b\\)–case \\(\\beta_1\\) stands \\(m\\) \\(\\beta_0\\) \\(b\\). , also inherently idea explanatory variables never explain variation predictor: ’s left model error \\(\\epsilon_i\\), also knows residuals.Remember, main purpose fitting linear model estimate coefficient values (\\(\\beta\\)); provides insight magnitude direction relationship predictors response variables. also interested much variation response variable explained predictor variables","code":""},{"path":"linear-models.html","id":"linear-model-assumptions","chapter":"11 Linear Models","heading":"11.3.2 Linear model assumptions","text":"Linear models important assumptions. statistician Andrew Gelman lists important linear model assumptions, order importance, follows:Validity: response predictors pertain directly research question.Additivity linearity: response variable model linear function separate predictors.Independence errors: value one residual completely independent value adjacent residuals.Equal variance errors: greater variance values predictor others.Normality errors: residuals taken together distributed normally.last three assumptions listed model residuals. Remember, model error represent difference data point \\(y_i\\) model prediction \\(\\hat{y}_i\\). figure indicates model residuals meet assumptions., one think assumptions violated? Well, academic Jan Vanhove helpful way thinking : assumptions violated, model inferences shouldn’t necessarily thought incorrect, rather less relevant.","code":""},{"path":"linear-models.html","id":"application-of-a-linear-model","chapter":"11 Linear Models","heading":"11.4 Application of a linear model","text":"demonstrate linear modeling practice, ’ll use data USACE’s Upper Mississippi River Restoration Program’s Long Term Resource Monitoring Program. data ’ve collated source contains fisheries catch data corresponding environmental conditions. may interested broadly understanding environmental conditions–depth, current, etc.–determine aspects fish communities. However, throwing several variables model predict drivers , say, fish abundance, assess whether correlations dependencies among environmental variables. need avoid mistaking effect one predictor another.example, aquatic systems, often relationship water temperature dissolved oxygen: solubility oxygen water decreases temperature increases, meaning colder water frequently oxygenated. However, dissolved oxygen levels may also affected aquatic plants, water turbulence can introduce oxygen water surface. Understanding dependencies may important ultimate goal understanding factors affect fish communities. , use linear models R.","code":""},{"path":"linear-models.html","id":"import-and-explore-the-data","chapter":"11 Linear Models","heading":"11.4.1 Import and explore the data","text":"First, read subset fish environmental data fall sampling navigation pools, 4, 8, 13 Upper Mississippi River. filter data remove dissolved oxygen values > 20 mg/L, since values levels may erroneous.can use ggplot plot relationship dissolved oxygen temperature. can also add line predictors; specify want line linear adding call method = \"lm\".can see ’s pretty clear relationship two variables. ’d like go beyond quantify relationship statistically.","code":"\nlibrary(tidyverse)\n\numr_wide <- read_csv(\"data/umr_counts_wide.csv\")\n\numr_sub <- umr_wide %>%\n  filter(do < 20, \n         !is.na(vegd))\nggplot(umr_sub, \n       aes(temp, do))+\n  geom_point(alpha = .5, pch = 21, size = 1)+\n  geom_smooth(method = \"lm\") +\n  labs(x = \"Temperature (C)\", y = \"Dissolved Oxygen\")+\n  theme_classic()"},{"path":"linear-models.html","id":"fit-a-model-and-assess-output","chapter":"11 Linear Models","heading":"11.4.2 Fit a model and assess output","text":"can now use lm() function r create linear model assessing relationship variables. use lm() function, provide R formula model. formula include (~ temp) indicates interested modeling function temp. words, telling R dissolved oxygen response variable, temperature predictor variable. wanted add another predictor change formula (e.g., ~ temp + depth ~ temp*detph wanted interactive effective). also indicate dataset use.’s fit first model, now stored lm.mod object.use summary function assess model results.output summary function contains important information :Coefficients: Estimates parameters standard deviationPr(>|t|): Results t-test determine parameters different 0Adjusted R squared: well model explain data?F-statistic(ANOVA): model significantly different model predictor (null model)?Taken together, values can used assess extent temperature predicts dissolved oxygen study location. can first look estimate \\(\\beta_1\\) coefficient temp, indicates effect size temperature dissolved oxygen, approximate value -0.26. means average, every 1 degree C increase temperature, dissolved oxygen decreases -0.26 mg/L. represents moderately substantial relationship. also get estimate intercept, \\(\\beta_0\\) coefficient. suggests predict dissolved oxygen level 12.6 mg/L temperature 0 degrees.can also see Adjusted R-squared estimate 0.16 model explains 16% variance dissolved oxygen. ecological research decent amount variance. may still wish refine model better job explaining variance.","code":"\nlm.mod <- lm(do ~ temp,\n           data = umr_sub)\nmod_summ <- summary(lm.mod)\nmod_summ## \n## Call:\n## lm(formula = do ~ temp, data = umr_sub)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -9.1881 -1.1783 -0.0571  1.0723 10.5807 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) 12.64919    0.19403   65.19   <2e-16 ***\n## temp        -0.25855    0.01161  -22.27   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 2.295 on 2570 degrees of freedom\n## Multiple R-squared:  0.1617, Adjusted R-squared:  0.1614 \n## F-statistic: 495.8 on 1 and 2570 DF,  p-value: < 2.2e-16"},{"path":"linear-models.html","id":"assess-model-validity","chapter":"11 Linear Models","heading":"11.4.3 Assess model validity","text":"can trust results? first need verify model assumptions met, examining residuals turns key assessing linear model appropriate choice modeling data. can making diagnostic plots residuals using plot(lm.mod) function. Running function create four plots sequentially R, examine individually following code. See detailed explanation plots .first plot shows Residuals vs. Fitted values. Remember, residuals difference predicted values response variable (dissolved oxygen ) observed values response variable. model assumptions met, plot horizontal line residuals spread around fairly evenly. residuals show non-linear patterns (e.g., parabola) may indicate non-linear relationship predictors response variable. appears fairly random scatter residuals, suggesting major problems non-linearity.Q-Q residual plot used assess whether residuals follow normal distribution; points plot closely follow diagonal line. example, residuals deviate bit tails, suggesting residuals may perfectly distributed, normality residuals critical large datasets like considered one less important linear model assumptions.Scale-Location plot checks constant variance residuals. Similar first plot, red line roughly flat, shouldn’t systematic spread data. model fairly well , although slight increase variance higher fitted values.Finally Residuals vs. Leverage plot indicate whether influential outliers can bias results. plot, specifically look large points upper lower right: points large residuals high leverage. Depending spread residuals, may dashed lines points called “Cook’s distance” scores. Points outside dashed lines, may strong influence regression results, suggesting parameter estimates might biased.Taken together, plots suggest linear model reasonable choice assessing relationship temperature dissolved oxygen. model violated linear model assumptions egregiously, need use different model type, often generalized linear model, covered subsequent module.","code":"\nplot(lm.mod, which = 1)\nplot(lm.mod, which = 2)\nplot(lm.mod, which = 3)\nplot(lm.mod, which = 5)"},{"path":"linear-models.html","id":"ancova","chapter":"11 Linear Models","heading":"11.4.4 ANCOVA","text":"Even linear model reasonable case, may hope explain variance. alluded possibility variables may important. submersed aquatic vegetation pumping oxygen water column? dataset contains categorical variable density vegetation vegd 1 vegetation sparse 2 vegetation dense. Categorical variables can added linear models R, although need make sure coded factors, numeric variables.model categorical variables, ANOVA. categorical variable used concert continuous variable, known ANCOVA, equivalent fitting two regression lines within model; lines parallel different intercepts. difference intercepts represents average effect categorical variable response.can start look possibility influence vegetation first plotting data , time coloring points ggplot differentially (.e., adding color = vegd earlier code):assess magnitude effect, run new linear model includes continuous temperature predictor categorical vegetation predictor.summary shows coefficient vegd2 -0.59. means average, locations temperature 0.6 mg/L less oxygen dense vegetation present (.e., vegd equals 2) compared sparse vegetation. Note, coefficient one level vegetation variable (.e., vegd equals 2, vegd equals 1). one level categorical variable (, vegd equals 1) always used reference level, coefficients reported represent difference response variable level baseline level. Beyond coefficients, can see R-squared increased marginally: adding vegd model increased explanation variance response data 1.5%.real analysis, might try data transformations improve model fit, /explore variables potential model inclusion. Finally, might assess model best using model selection approach like AIC. -depth coverage topics, point user following sources:Quebec Center biodiversity Science’s Linear model workshopBen Staton’s Intro R Natural Resources course","code":"\numr_sub <- mutate(umr_sub, \n                  vegd = as.factor(vegd))\nggplot(umr_sub, \n       aes(temp, do, color = vegd))+\n  geom_point(alpha = .5, pch = 21, size = 1.5)+\n  geom_smooth(method = \"lm\")+\n  labs(x = \"Temperature (C)\", y = \"Dissolved Oxygen (mg/L)\", color = \"Veg.\\ndensity\")+\n  theme_classic()\nancov.mod <- lm(do ~ temp + vegd, \n                data = umr_sub)\n\nsummary(ancov.mod)## \n## Call:\n## lm(formula = do ~ temp + vegd, data = umr_sub)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -8.8610 -1.1965 -0.1168  1.0716 10.8873 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) 12.78663    0.19367  66.021  < 2e-16 ***\n## temp        -0.24924    0.01161 -21.469  < 2e-16 ***\n## vegd2       -0.58551    0.09052  -6.468 1.18e-10 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 2.277 on 2569 degrees of freedom\n## Multiple R-squared:  0.1752, Adjusted R-squared:  0.1745 \n## F-statistic: 272.8 on 2 and 2569 DF,  p-value: < 2.2e-16"},{"path":"linear-models.html","id":"summary-4","chapter":"11 Linear Models","heading":"11.5 Summary","text":"saw linear models can used describe linear relationship response variable number predictor variables. best linear models thought means assess hypotheses developed based subject matter expertise. results linear model indicate whether data support hypothesis, giving us estimates magnitude direction effect sizes, much variability model explains much uncertainy .Linear models best fairly constrained scenarios .e., response data continuous variables, linear relationship variables, etc. However, ecological research, many data types–especially binary count data–rarely ever meet criteria, model types required. example next module presents extension linear models can used ubiquitous data types like counts binary data conform assumptions linear models.","code":""},{"path":"generalized-linear-models.html","id":"generalized-linear-models","chapter":"12 Generalized Linear Models","heading":"12 Generalized Linear Models","text":"module teach use Generalized Linear Models assess importance management-relevant environmental factors fish speciesAuthors: Ed StoweLast update: 2025-02-03Acknowledgements: inspiration/language/code used adapted permission : Ben Staton’s Intro R Natural Resources course; Quebec Center biodiversity Science’s Generalized linear model workshop","code":""},{"path":"generalized-linear-models.html","id":"generalized-linear-model-background","chapter":"12 Generalized Linear Models","heading":"12.1 Generalized Linear Model Background","text":"models examined previous section linear models. assume residuals normally-distributed response variable predictor variables linearly-related. However several ubiquitous types data used ecological modeling–presence/absence data counts organisms–follow assumptions. types data, Generalized linear models (GLMs) can overcome problems.go statistical details GLMs, -depth description statistical underpinnings, see [] (https://r.qcbs.ca/workshop06/book-en/) elsewhere. important , however, understand three key elements GLMs:Response distribution: probability distribution response variable Y, binomial distribution binary data Poisson distribution count data.Linear predictor: linear combination explanatory variables (η=Xβ), analogous standard linear model.Link function: function relates mean response variable’s distribution (μ) linear predictor (η). choice link function typically guided response distribution; example, count data modeled Poisson distribution often use natural logarithm link function.common examples GLMs ecology logistic regression binary data poisson negative binomial regression count data.","code":""},{"path":"generalized-linear-models.html","id":"logistic-regression","chapter":"12 Generalized Linear Models","heading":"12.2 Logistic regression","text":"Binary data, two opposite outcomes, common ecology, example, present/absent, success/failure, lived/died, male/female, etc. want predict probability one outcome opposite changes depending variable(s), need use logistic regression model, written :\\[\\begin{equation}\n  logit(p_i)=\\beta_0 + \\beta_1 x_{i1} + ... + \\beta_j x_{ij}+ ... + \\beta_n x_{}, y_i \\sim Bernoulli(p_i)\n\\tag{12.1}\n\\end{equation}\\], \\(p_i\\) probability success trial \\(\\) values predictor variables \\(x_{ij}\\). \\(logit(p_i)\\) link function links linear parameter scale data scale. logit link function constrains value \\(p_i\\) 0 1 regardless values \\(\\beta\\) coefficients.logit link equivalent log odds ratio, words, natural log likely event compared happening.Therefore, logistic regression, model calculate log odds outcome, want know odds ratio (.e., ), just take inverse log. example analysis conduct make obvious.","code":""},{"path":"generalized-linear-models.html","id":"example-logistic-regression-anaysis","chapter":"12 Generalized Linear Models","heading":"12.2.1 Example logistic regression anaysis","text":"example logistic regression analysis, ’ll use fisheries dataset three navigation pools Upper Mississippi River used chapter linear models. fish data collected understand long-term trends river, ’ll use data see relationships may exist habitat predictors fish responses.","code":""},{"path":"generalized-linear-models.html","id":"import-and-explore-data","chapter":"12 Generalized Linear Models","heading":"12.2.1.1 Import and explore data","text":"First read data filter includes data daylight electrofishing (gear == \"D\"), backwater habitat, depths < 5 m, gear accurate.Let’s look column names dataframe. see last 10 columns, starting BLGL codes species. columns counts different species sampling event.Counts can “noisy” data often difficult predict, ’s common instead model presence/absence. order , can convert counts 1’s 0’s using following code. using mutate change values species columns. case_when dplyr function operates similarly -else statement. , saying columns species names, value greater 0, convert 1, value (.e., ’s 0), remain 0.’re going examine whether predictor variables dataset predictive occurrence one species, common carp. able predict habitat variables influence presence species might basis predicting USACE management activities impact species.First, let’s plot common carp occurrence function several predictor variables: temperature, water transparency (.e., secchi disk reading), depth, conductivity. easily ggplot, need variables one column can use facet_wrap function make singe panel variable. create dataframe called carp_long using pivot_longer function go wide format long format, makes plotting seemless.pivot_longer tell columns want combine, name column now inclues variable names name column includes variable values.Now can plot variables.plots, variables pop potentially important. example locations carp occurrences appear lower secchi disk readings (.e., less clear water) higher temperatures, strength relationships unclear, modeling comes play.","code":"\nlibrary(tidyverse)\n\numr_wide <- read_csv(\"data/umr_counts_wide.csv\")\n\n#Period already filtered to 3\numr_sub <-  umr_wide %>%\n  filter(gear == \"D\",\n         stratum_full == \"Backwater\",\n         depth < 5)\nnames(umr_sub)##  [1] \"date\"         \"utm_e\"        \"utm_n\"        \"stratum_full\" \"year\"         \"period\"      \n##  [7] \"pool\"         \"gear\"         \"secchi\"       \"temp\"         \"depth\"        \"cond\"        \n## [13] \"current\"      \"do\"           \"substrt\"      \"vegd\"         \"BLGL\"         \"FWDM\"        \n## [19] \"CARP\"         \"LMBS\"         \"ERSN\"         \"BKCP\"         \"SHRH\"         \"GZSD\"        \n## [25] \"WTBS\"         \"SFSN\"\nspecies <- names(umr_sub)[17:26]\n\nfish_binary <- umr_sub %>% \n mutate(across(any_of(species), ~ case_when(. > 0 ~ 1, TRUE ~ 0)))\ncarp_long <- fish_binary %>%\n  select(CARP, secchi:do) %>%\n  pivot_longer(cols = secchi:cond, names_to = \"variable\", values_to = \"value\")\nggplot(carp_long, \n         aes(value, CARP))+\n  geom_jitter(height = .1, width = .02, alpha =0.5, color = \"steelblue\")+\n  geom_boxplot(aes(group = CARP), \n                   width = 0.5, alpha = 0.5, color = \"black\", fill = \"gray\")+\n  facet_wrap(~variable, nrow = 2, scales = \"free_x\")+\n  scale_y_continuous(breaks = c(0,1))+\n  labs(x = \"Value of predictor\", y = \"Carp occurrence\")+\n  theme_classic()## Warning: Removed 170 rows containing non-finite outside the scale range (`stat_boxplot()`).## Warning: Removed 170 rows containing missing values or values outside the scale range\n## (`geom_point()`)."},{"path":"generalized-linear-models.html","id":"fit-model-with-glm-function","chapter":"12 Generalized Linear Models","heading":"12.2.1.2 Fit model with glm function","text":"’ll use glm function base R , kind formula notation linear models, indicating want model carp occurrences function four predictor variables. ’ll use fish_binary dataset . Finally need select ‘family’ error distribution response variable. presence/absence data, binomial distribution typically chosen ’s distribution two states, typically 1 “success” 0 “failure.”can look results model summary function.summary function generates output similar ’ve observed linear modeling. Estimate column shows us parameter estimate different model parameters, can also get indication whether parameters considered significant, e.g., p-values less 0.05. , see parameters considered significant, except conductivity.interpret model coefficient values? Let’s look values :logistic regression involves logit link function, ’s easy linear modeling. , coefficient values can interpreted additive effect , example, temperature log odds success.odds ratio interpretable log odds. get taking inverse natural log coefficicent values, words raising e power coefficients. R, use exp() function .values indicate odds occurrence 2.71 times (171% higher) depth increases meter.can now use ggplot generate graph model results looking effect depth Carp occurrence probability. Note, however, approximate, model also included parameters, plot merely considers depth.","code":"\nglm.bin<- glm(CARP ~ secchi + temp + depth + cond,\n            data = fish_binary,\n            family = binomial)\nsummary(glm.bin)## \n## Call:\n## glm(formula = CARP ~ secchi + temp + depth + cond, family = binomial, \n##     data = fish_binary)\n## \n## Coefficients:\n##               Estimate Std. Error z value Pr(>|z|)    \n## (Intercept) -1.2036029  0.5342537  -2.253  0.02427 *  \n## secchi      -0.0322025  0.0035546  -9.059  < 2e-16 ***\n## temp         0.0957375  0.0191719   4.994 5.93e-07 ***\n## depth        0.9963667  0.2239745   4.449 8.64e-06 ***\n## cond         0.0027675  0.0009828   2.816  0.00486 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 1074.79  on 777  degrees of freedom\n## Residual deviance:  936.89  on 773  degrees of freedom\n##   (68 observations deleted due to missingness)\n## AIC: 946.89\n## \n## Number of Fisher Scoring iterations: 3\n# Extracting model coefficients\nglm.bin$coefficients##  (Intercept)       secchi         temp        depth         cond \n## -1.203602861 -0.032202531  0.095737510  0.996366684  0.002767454\nexp(glm.bin$coefficient)## (Intercept)      secchi        temp       depth        cond \n##   0.3001110   0.9683104   1.1004702   2.7084234   1.0027713\nggplot(fish_binary, aes(x = depth, y = CARP)) +\n  geom_jitter(height = 0.05, width = 0.05) +\n  stat_smooth(\n    method = \"glm\",\n    method.args = list(family = binomial),\n    se = TRUE\n  ) +\n  xlab(\"Depth (m)\") +\n  ylab(\"Probability of presence\") +\n  ggtitle(\"Probability of presence of  Carp as a function of depth\") +\n  theme_classic()## `geom_smooth()` using formula = 'y ~ x'"},{"path":"generalized-linear-models.html","id":"glm-with-the-tidymodels-package","chapter":"12 Generalized Linear Models","heading":"12.2.1.3 GLM with the tidymodels package","text":"R, certain kinds analysis, may different packages can perform statistical procedures, different strengths. simple glm CARP, glm function base R works well, model complexity increases may instances additional capacities make sense. One package enables tidymodels package, ’ll first introduce , can come back later. following code implements type analysis, now using tidymodels code.want model counts?interpret coefficients, can say one-unit increase current, log count \nY increases significantly 5.246739516, expected count increases 189.85\nexp(5.246739516)≈189.85. one-unit increase current lot scale, though may better consider 0.1 m/s increase current increases counts 19 fishTidymodels version","code":"\n# Load required libraries\nlibrary(tidymodels)\n\nresults <- data.frame()\n\nfor (i in 1:length(species)){\n\ndata <- fish_binary %>%\n  rename(species = species[i])\n\n# Split the data into training and testing sets\nset.seed(123)\ndata_split <- initial_split(data, prop = 0.8)\ntrain_data <- training(data_split)\ntest_data <- testing(data_split)\n\n# Specify the model: GLM for regression\nglm_spec <- linear_reg() %>%\n  set_engine(\"glm\") %>%\n  set_mode(\"regression\")\n\n# Define a recipe: Preprocessing steps\nglm_recipe <- recipe(species ~ secchi + temp + depth + current + do, data = train_data) %>%\n  step_normalize(all_numeric_predictors())\n\n# Create a workflow\nglm_workflow <- workflow() %>%\n  add_recipe(glm_recipe) %>%\n  add_model(glm_spec)\n\n# Perform 10-fold cross-validation\nset.seed(123)\ncv_splits <- vfold_cv(train_data, v = 10)\n\n# Fit the model with cross-validation\ncv_results <- fit_resamples(\n  glm_workflow,\n  resamples = cv_splits,\n  metrics = metric_set(rmse, rsq),\n  control = control_resamples(save_pred = TRUE)\n)\n\n# View cross-validation metrics\ncollect_metrics(cv_results)\n\n# Finalize the model on the entire training set\nfinal_glm <- fit(glm_workflow, data = train_data)\n\n# Test the finalized model on the testing set\ntest_results <- predict(final_glm, test_data) %>%\n  bind_cols(test_data) %>%\n  metrics(truth = species, estimate = .pred) %>%\n  mutate(sp = species[i])\n\n#print(test_results)\n\nresults <- bind_rows(results, test_results)\n\n}\n# Load required libraries\numr_sub %>%\n  select(LMBS, secchi:do) %>%\n  pivot_longer(cols = secchi:do, names_to = \"variable\", values_to = \"value\") %>%\n  ggplot(aes(value, LMBS))+\n  geom_jitter(height = .05, pch = 21, alpha =0.5)+\n  geom_smooth(method = \"glm\")+\n  facet_wrap(~variable, nrow = 2, scales = \"free_x\")\nglm.poisson = glm(LMBS ~ secchi + temp + depth + current + do,\n  data = umr_sub,\n  family = poisson) # this is what makes it a Poisson GLM! Note the default link is log.\n\nsummary(glm.poisson)## \n## Call:\n## glm(formula = LMBS ~ secchi + temp + depth + current + do, family = poisson, \n##     data = umr_sub)\n## \n## Coefficients:\n##               Estimate Std. Error z value Pr(>|z|)    \n## (Intercept)  2.1876972  0.0730252  29.958  < 2e-16 ***\n## secchi       0.0060138  0.0003939  15.268  < 2e-16 ***\n## temp         0.0305910  0.0024742  12.364  < 2e-16 ***\n## depth       -0.1558713  0.0317927  -4.903 9.45e-07 ***\n## current     -1.7909347  0.1238641 -14.459  < 2e-16 ***\n## do           0.0262556  0.0042626   6.160 7.30e-10 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for poisson family taken to be 1)\n## \n##     Null deviance: 10137.7  on 496  degrees of freedom\n## Residual deviance:  9474.3  on 491  degrees of freedom\n##   (349 observations deleted due to missingness)\n## AIC: 11573\n## \n## Number of Fisher Scoring iterations: 5\nglm.poisson$coefficients\nlibrary(poissonreg)\n\nresults_counts <- data.frame()\n\nfor (i in 1:length(species)){\n\ndata <- umr_sub %>%\n  rename(count = species[i])\n\n# Split the data into training and testing sets\nset.seed(123)\ndata_split <- initial_split(data, prop = 0.8)\ntrain_data <- training(data_split)\ntest_data <- testing(data_split)\n\n# Specify the model: Poisson Regression\npoisson_spec <- poisson_reg() %>%\n  set_engine(\"glm\") %>%\n  set_mode(\"regression\")\n\n# Specify the model: Negative Binomial Regression\n#nb_spec <- gen_additive_mod() %>%\n # set_engine(\"glm\", family = MASS::negative.binomial(theta = 1)) %>%\n  #set_mode(\"regression\")\n\n# Define a recipe: Preprocessing steps\nreg_recipe <- recipe(count ~ secchi + temp + depth + current + do, data = train_data) %>%\n  step_normalize(all_numeric_predictors())\n\n# Create workflows\npoisson_workflow <- workflow() %>%\n  add_recipe(reg_recipe) %>%\n  add_model(poisson_spec)\n\n#nb_workflow <- workflow() %>%\n # add_recipe(reg_recipe) %>%\n  #add_model(nb_spec)\n\n# Perform 10-fold cross-validation\nset.seed(123)\ncv_splits <- vfold_cv(train_data, v = 10)\n\n# Fit the Poisson model with cross-validation\npoisson_cv_results <- fit_resamples(\n  poisson_workflow,\n  resamples = cv_splits,\n  metrics = metric_set(rmse, mae, rsq),\n  control = control_resamples(save_pred = TRUE)\n)\n\n# Fit the Negative Binomial model with cross-validation\n# nb_cv_results <- fit_resamples(\n#   nb_workflow,\n#   resamples = cv_splits,\n#   metrics = metric_set(rmse, mae, rsq),\n#   control = control_resamples(save_pred = TRUE)\n# )\n\n# View cross-validation metrics\nprint(species[i])\ncat(\"Poisson Regression Metrics:\\n\")\ncollect_metrics(poisson_cv_results)\n\n#cat(\"\\nNegative Binomial Regression Metrics:\\n\")\n#collect_metrics(nb_cv_results)\n\n# Finalize and test the Poisson model\nfinal_poisson <- fit(poisson_workflow, data = train_data)\npoisson_test_results <- predict(final_poisson, test_data) %>%\n  bind_cols(test_data) %>%\n  metrics(truth = count, estimate = .pred) %>%\n  mutate(sp = species[i],\n         model = \"pois\")\n\n# Finalize and test the Negative Binomial model\n#final_nb <- fit(nb_workflow, data = train_data)\n#nb_test_results <- predict(final_nb, test_data) %>%\n # bind_cols(test_data) %>%\n  #metrics(truth = count, estimate = .pred) %>%\n  #mutate(sp = comm_sp[i],\n   #      model = \"nb\")\n\nresults_counts <- bind_rows(results_counts, poisson_test_results)#, nb_test_results)\n\n}"},{"path":"random-effects.html","id":"random-effects","chapter":"13 Random Effects","heading":"13 Random Effects","text":"","code":""},{"path":"generalized-additive-models.html","id":"generalized-additive-models","chapter":"14 Generalized Additive Models","heading":"14 Generalized Additive Models","text":"","code":""},{"path":"random-forest.html","id":"random-forest","chapter":"15 Random Forest","heading":"15 Random Forest","text":"Adapted Carpentries workshop\nhttps://carpentries-incubator.github.io/r-ml-tabular-data/Random forest portion :\nhttps://carpentries-incubator.github.io/r-ml-tabular-data/04-Decision-Forests/index.htmlUse tidymodels version","code":""},{"path":"random-forest.html","id":"learning-objectives-8","chapter":"15 Random Forest","heading":"15.1 Learning objectives","text":"Understand value MLs like random forestUnderstand overview random forestUnderstand basic workflow MLLearn implement simple random forest model","code":""},{"path":"random-forest.html","id":"background-1","chapter":"15 Random Forest","heading":"15.2 Background","text":"","code":""},{"path":"random-forest.html","id":"can-i-do-machine-learning","chapter":"15 Random Forest","heading":"15.2.1 Can I do machine learning?","text":"Yes. ’s getting easier.","code":""},{"path":"random-forest.html","id":"what-is-it-actually","chapter":"15 Random Forest","heading":"15.2.2 What is it actually?","text":"","code":""},{"path":"random-forest.html","id":"when-would-it-be-good","chapter":"15 Random Forest","heading":"15.2.3 When would it be good?","text":"Prediction","code":""},{"path":"random-forest.html","id":"what-are-the-basic-steps","chapter":"15 Random Forest","heading":"15.2.4 What are the basic steps","text":"SplitTune trainAssessPredict","code":""},{"path":"random-forest.html","id":"what-can-i-use-for-ml","chapter":"15 Random Forest","heading":"15.2.5 What can I use for ML?","text":"Tons packages R Python.Tidymodels R offers integrated framework using bunch different MLs.","code":""},{"path":"random-forest.html","id":"how-does-random-forest-work","chapter":"15 Random Forest","heading":"15.2.6 How does random forest work","text":"Decision trees start basic question, , “surf?”\n, can ask series questions determine answer, , “long period swell?” “wind blowing offshore?”.questions make decision nodes tree, acting means split data.question helps individual arrive final decision, denoted leaf node.Observations fit criteria follow “Yes” branch don’t follow alternate path.Decision trees seek find best split subset data, tend problems: often biased poor making predictions. Random forest algorithms solve problems.Instead single tree, random forest creates many unique decision trees. :Create Many Decision Trees: algorithm makes many decision trees using random part data. every tree bit different.Pick Random predictors: building tree doesn’t look features (columns) . picks random decide split data, tree unique.Tree Makes Prediction: Every tree gives answer prediction based learned part data.Combine Predictions:classification choose category final answer one trees agree .e majority voting.\nregression predict number final answer average trees predictions.Works Well: Using random data features tree helps avoid overfitting makes overall prediction accurate trustworthy.","code":""},{"path":"random-forest.html","id":"case-study","chapter":"15 Random Forest","heading":"15.3 Case study","text":"","code":""},{"path":"random-forest.html","id":"quick-data-exploration","chapter":"15 Random Forest","heading":"15.3.1 Quick data exploration","text":"","code":""},{"path":"random-forest.html","id":"data-partitioning","chapter":"15 Random Forest","heading":"15.3.2 Data partitioning","text":"","code":""},{"path":"random-forest.html","id":"set-up-traintest-split","chapter":"15 Random Forest","heading":"15.3.3 Set up train/test split","text":"","code":"\nlibrary(tidymodels)\n\nset.seed(123)\ntrees_split <- initial_split(trees_df, strata = legal_status)\ntrees_train <- training(trees_split)\ntrees_test <- testing(trees_split)"},{"path":"random-forest.html","id":"pre-processing-with-recipes","chapter":"15 Random Forest","heading":"15.3.4 Pre-processing with recipes","text":"Build recipe data preprocessing.First, must tell recipe() model going (using formula ) training data .\nNext, update role tree_id, since variable might like keep around convenience identifier rows predictor outcome.\nNext, use step_other() collapse categorical levels species, caretaker, site info. step, 300+ species!\ndate column tree planted may useful fitting model, probably exact date, given slowly trees grow. Let’s create year feature date, remove original date variable.\nmany DPW maintained trees , let’s downsample data training.\nobject tree_rec recipe trained data yet (example, categorical levels collapsed calculated) tree_prep object trained data.","code":"\ntree_rec <- recipe(legal_status ~ ., data = trees_train) %>%\n  update_role(tree_id, new_role = \"ID\") %>%\n  step_other(species, caretaker, threshold = 0.01) %>%\n  step_other(site_info, threshold = 0.005) %>%\n  step_dummy(all_nominal(), -all_outcomes()) %>%\n  step_date(date, features = c(\"year\")) %>%\n  step_rm(date) %>%\n  step_downsample(legal_status)\n\ntree_prep <- prep(tree_rec)\njuiced <- juice(tree_prep)  "},{"path":"random-forest.html","id":"model-specifications-with-parsnip","chapter":"15 Random Forest","heading":"15.3.5 Model specifications with parsnip","text":"Tune parameters.","code":"\nrf_spec <- \n    rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %>% \n    set_engine(\"ranger\") %>% \n    set_mode(\"classification\")"},{"path":"random-forest.html","id":"workflow","chapter":"15 Random Forest","heading":"15.3.6 Workflow","text":"","code":"\ntune_wf <- workflow() %>%\n  add_recipe(tree_rec) %>%\n  add_model(tune_spec)"},{"path":"random-forest.html","id":"tune-hyperparameters---notes-from-julia-slige-httpsjuliasilge.comblogsf-trees-random-tuning","chapter":"15 Random Forest","heading":"15.3.7 Tune hyperparameters - Notes from Julia Slige https://juliasilge.com/blog/sf-trees-random-tuning/","text":"Now ’s time tune hyperparameters random forest model. First, let’s create set cross-validation resamples use tuning.can’t learn right values training single model, can train whole bunch models see ones turn best. can use parallel processing make go faster, since different parts grid independent. Let’s use grid = 20 choose 20 grid points automatically.turn ? Let’s look AUC.grid involve every combination min_n mtry can get idea going . looks like higher values mtry good (10) lower values min_n good (10). can get better handle hyperparameters tuning one time, time using regular_grid(). Let’s set ranges hyperparameters want try, based results initial tune.","code":"\nset.seed(234)\ntrees_folds <- vfold_cv(trees_train)\ndoParallel::registerDoParallel()\n\nset.seed(345)\ntune_res <- tune_grid(\n  tune_wf,\n  resamples = trees_folds,\n  grid = 20\n)\n\ntune_res\ntune_res %>%\n  collect_metrics() %>%\n  filter(.metric == \"roc_auc\") %>%\n  select(mean, min_n, mtry) %>%\n  pivot_longer(min_n:mtry,\n    values_to = \"value\",\n    names_to = \"parameter\"\n  ) %>%\n  ggplot(aes(value, mean, color = parameter)) +\n  geom_point(show.legend = FALSE) +\n  facet_wrap(~parameter, scales = \"free_x\") +\n  labs(x = NULL, y = \"AUC\")\nrf_grid <- grid_regular(\n  mtry(range = c(10, 30)),\n  min_n(range = c(2, 8)),\n  levels = 5\n)\n\nrf_grid  "},{"path":"random-forest.html","id":"choosing-the-best-model","chapter":"15 Random Forest","heading":"15.3.8 Choosing the best model","text":"’s much clear best model now. can identify using function select_best(), update original model specification tune_spec create final model specification.can tune one time, time targeted way rf_grid.results look like now?Let’s explore final model bit. can learn variable importance, using vip package?private caretaker characteristic important categorization, latitude longitude. Interesting year (.e. age tree) important!Let’s make final workflow, fit one last time, using convenience function last_fit(). function fits final model entire training set evaluates testing set. just need give funtion original train/test split.metrics test set look good indicate overfit tuning.Let’s bind testing results back original test set, make one map. San Francisco less incorrectly predicted trees?","code":"\nset.seed(456)\nregular_res <- tune_grid(\n  tune_wf,\n  resamples = trees_folds,\n  grid = rf_grid\n)\n\nregular_res\nregular_res %>%\n  collect_metrics() %>%\n  filter(.metric == \"roc_auc\") %>%\n  mutate(min_n = factor(min_n)) %>%\n  ggplot(aes(mtry, mean, color = min_n)) +\n  geom_line(alpha = 0.5, size = 1.5) +\n  geom_point() +\n  labs(y = \"AUC\")\n\nbest_auc <- select_best(regular_res, \"roc_auc\")\n\nfinal_rf <- finalize_model(\n  tune_spec,\n  best_auc\n)\n\nfinal_rf\nlibrary(vip)\n\nfinal_rf %>%\n  set_engine(\"ranger\", importance = \"permutation\") %>%\n  fit(legal_status ~ .,\n    data = juice(tree_prep) %>% select(-tree_id)\n  ) %>%\n  vip(geom = \"point\")\nfinal_wf <- workflow() %>%\n  add_recipe(tree_rec) %>%\n  add_model(final_rf)\n\nfinal_res <- final_wf %>%\n  last_fit(trees_split)\n\nfinal_res %>%\n  collect_metrics()\nfinal_res %>%\n  collect_predictions() %>%\n  mutate(correct = case_when(\n    legal_status == .pred_class ~ \"Correct\",\n    TRUE ~ \"Incorrect\"\n  )) %>%\n  bind_cols(trees_test) %>%\n  ggplot(aes(longitude, latitude, color = correct)) +\n  geom_point(size = 0.5, alpha = 0.5) +\n  labs(color = NULL) +\n  scale_color_manual(values = c(\"gray80\", \"darkred\"))"},{"path":"boosted-regression-trees.html","id":"boosted-regression-trees","chapter":"16 Boosted Regression Trees","heading":"16 Boosted Regression Trees","text":"Adapted Carpentries workshop\nhttps://carpentries-incubator.github.io/r-ml-tabular-data/Boosted trees portion \nhttps://carpentries-incubator.github.io/r-ml-tabular-data/05-Gradient-Boosting/index.html","code":""},{"path":"population-modeling.html","id":"population-modeling","chapter":"17 Population Modeling","heading":"17 Population Modeling","text":"","code":""},{"path":"population-modeling.html","id":"occupancy-models-in-r-with-unmarked-james-paterson","chapter":"17 Population Modeling","heading":"17.0.1 Occupancy models in R with unmarked: James Paterson","text":"Intro occupancy models\nhttps://jamesepaterson.github.io/jamespatersonblog/2020-09-01_occupancyintroduction.htmlPart 2: Comparing Occ. Models\nhttps://jamesepaterson.weebly.com/blog/occupancy-models--r-part-2-model-comparisonsDynamic occupancy modeling:\nhttps://jamesepaterson.github.io/jamespatersonblog/2021-01-01_dynamicoccupancy.htmlSee mark recapture models (e.g., JS, CJS):\nhttps://jamesepaterson.weebly.com/blog","code":""},{"path":"population-modeling.html","id":"course-outline","chapter":"17 Population Modeling","heading":"17.1 Course outline","text":"Population modeling\nBackground population modeling, drawing :\nPrimer Ecology using R (https://hankstevens.github.io/Primer--Ecology/expo.html)\nModeling Population Growth module UNL (https://dshizuka.github.io/RCourse/09.2.PopGrowth.html)\n\nDensity independent growth\nDensity-independent demography\nDensity dependent growth\nPopulations space\nUse simple analyses Wenger et al. (https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/ece3.9339) demonstrate relating poulations environmental covariates\nBackground population modeling, drawing :\nPrimer Ecology using R (https://hankstevens.github.io/Primer--Ecology/expo.html)\nModeling Population Growth module UNL (https://dshizuka.github.io/RCourse/09.2.PopGrowth.html)\nPrimer Ecology using R (https://hankstevens.github.io/Primer--Ecology/expo.html)Modeling Population Growth module UNL (https://dshizuka.github.io/RCourse/09.2.PopGrowth.html)Density independent growthDensity-independent demographyDensity dependent growthPopulations spaceUse simple analyses Wenger et al. (https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/ece3.9339) demonstrate relating poulations environmental covariates","code":""},{"path":"network-models-and-connectivity.html","id":"network-models-and-connectivity","chapter":"18 Network Models and Connectivity","heading":"18 Network Models and Connectivity","text":"Placeholder edited exploration WUCT Tool, combined materials WUCT Github repoAuthors: Kyle McKay (writing, code), Ed Stowe (writing, editing); Background WUCT materials (Readme & RMD) GitHubLast update: 2025-08-22Acknowledgements: ","code":""},{"path":"network-models-and-connectivity.html","id":"background-on-network-analysis","chapter":"18 Network Models and Connectivity","heading":"18.1 Background on network analysis","text":"","code":""},{"path":"network-models-and-connectivity.html","id":"overview-on-wuct-tool","chapter":"18 Network Models and Connectivity","heading":"18.2 Overview on WUCT tool","text":"model document describes procedure quantifying benefits associated removal organism movement barriers within watershed (e.g., dam removal, culvert repair, fish ladder installation) impacts barrier addition (e.g., dam construction, weir installation). model focuses upstream movement migratory organisms fish intended application watershed-scale.algorithm based four primary components: habitat quantity upstream dam, habitat quality upstream dam, passability structure given organism, shape/topology watershed. algorithm combines data estimate quality-weighted, accessible habitat watershed scale.","code":""},{"path":"network-models-and-connectivity.html","id":"background-2","chapter":"18 Network Models and Connectivity","heading":"18.3 Background","text":"Water resources transportation infrastructure dams culverts provide countless socio-economic benefits; however, infrastructure can also disconnect movement organisms, sediment, water river ecosystems (Pringle 2001). Trade-offs associated competing costs benefits occur globally, applications barrier addition (e.g., dam road construction), reengineering (e.g., culvert repair, fish ladder installation), removal (e.g., dam removal aging infrastructure). Nationwide, millions barriers inhibit movement aquatic organisms, barrier removal repair emerged key techniques river restoration. 2016, 1,200 dams removed nationwide (Bellmore et al. 2016) well countless road-stream crossings repaired facilitate movement (Januchowski-Hartley et al. 2013).Restoration practitioners require tools informing barrier removal, repair, retrofit decisions. important application rapid selection efficient sites removal, given large number watershed barriers (.e., barrier-barrier-B priority restoration?). Another application computation restoration benefits particular site compare relative effects alternative management actions (e.g., fish ladder dam removal preferrable?). Dozens studies globally examined questions different methods techniques (O’Hanley Tomberlin 2005, Neeson et al. 2015, McKay et al. 2016).module showcases framework prioritizing barriers quantifying benefits restoration actions watershed-scale. purpose tool assess watershed scale habitat connectivity quality migratory organisms inform restoration planning. framework adaptable varying project timelines, scopes, applications. following issues crucial scoping application framework:focus migratory organisms moving upstream watershed (e.g., salmon seeking access breeding habitat). Downstream movement considered limiting model framework neglected, although can important limiting factor species.framework focuses quantifying accessibility habitat focal taxa, habitat quantity quality can incorporated multiple methods.Overall, modeling approach may used rapidly screen barriers site selection quantify benefits alternative restoration actions site scale, often referred “barrier prioritization techniques”.framework can used context benefits barrier removal desired taxa impacts barrier removal relative invasive species. either case, watershed connectivity may assessed following procedure, interpretation results may different. instance, user may chose maximize connectivity desirable species minimizing connectivity invasive species.Many barrier prioritization studies address multiple taxa development watershed priorities. framework applied multiple taxa separately (.e., developing connectivity estimates taxa), overall assessment obtained using combined metric (e.g., sum average connected habitat species).","code":""},{"path":"network-models-and-connectivity.html","id":"case-study-bronx-river","chapter":"18 Network Models and Connectivity","heading":"18.4 Case Study: Bronx River","text":"case study examines fish passage prioritization Hudson-Raritan Estuary (HRE) project led USACE New York District (NAN). Specifically, application quantifies fish passage benefits associated two sites Bronx River watershed, proposed 2017 Draft Feasibility Report (USACE 2017) supported Comprehensive Restoration Plan (USACE 2016). Fish passage outputs quantified terms “accessible habitat” using Watershed-Scale Upstream Connectivity Toolkit (WUCT) river herring focal taxa.Three dams interest Bronx River system moving downstream upstream. First, East 182nd Street Dam first barrier encountered, fish ladder (Alaskan Steep pass) constructed partners including NYC Parks, Bronx Borough, Wildlife Conservation Society, National Oceanic Atmospheric Administration, US Fish Wildlife Service, New York State, National Fish Wildlife Foundation (Lumbian Larson 2015). Second, Bronx Zoo Dam next structure, USACE proposed three restoration alternatives part feasibility study (including fish ladder). Third, Stone Mill Dam (aka. Snuff Mill Dam) next structure, USACE proposed three restoration alternatives part feasibility study (one fish ladder + attractors, one fish ladder, one without fish ladder). significant amount habitat accessible Stone Mill Dam, considering main steam tributary habitats. Bronx River shown support river herring populations, accessibility limiting (Larson Sugar 2004), 182nd St Dam fish ladder subsequently demonstrated river herring utilize technical fishways region.WUCT requires three general types inputs, HRE parameterization follows (also shown Table 3):Habitat Quantity - barrier, area upstream habitat opened used primary basis habitat quantity. First, length upstream habitat computed (.e., distance dam next upstream barrier) included tributary habitats newly accessible. River width estimated aerial photos Google Earth smallest observable width upstream structure (.e., extremely conservative estimate). create area-based metric , length river multiplied width.Habitat Quality - Habitat quality predicted based watershed-scale, geospatial analysis upstream habitat presented McKay et al. (2017). model included three metrics accounting land use development pressure, water quality, proportion basin conservation status.Passability - Local studies fish passage rates (.e., passability) unavailable Bronx River. , passability estimated based studies elsewhere efficacy technical fishways general river herring specifically. Prior restoration actions, structures assumed zero passage river herring. Alewife studies New England (Franklin et al. 2012) report high overall passage rates 64-99% passage East River, Massachusetts particularly high rates technical steep passes 94-97%. values line meta-analysis 65 published fish passage studies (Noonan et al. 2011), indicated fish passage efforts typically result 42% fish passage average across variety taxa. Based Massachusetts data, used 80% passability fish ladders conservative estimate passage efficiency. 182nd Street Dam Ladder included analyses part future without prokect condition. Stone Mill Dam, Alternative-includes fish attractors increase utilization ladder, assumed increase passability 10% (.e., 88% total).following script imports data, isolates necessary input data, computes permutations restoration actions, computes connectivity watershed-scale restoration plan. Table 4 presents output WUCT application. connectivity values represent connectivity- quality-weighted assessment total habitat watershed scale.Table 18.1: Table 3. Input data WUCT Bronx River described text. node specifies existing condition (shown Alt=0), multiple restoration alternatives also specified node (shown Alt>0).Next create connectivity function.Table 18.2: Table 4. Alternatives analysis using WUCT Bronx River Watershed. Competing restoration alternatives shown rows, alternative used plan denoted number shown.","code":"\n#Dummy example - Import data\nbarrieralts <- read.csv(\"data/WUCT_HREData_2018-07-18_BarrierAlts.csv\", header=TRUE, dec=\".\")\nA <- data.matrix(read.csv(\"data/WUCT_HREData_2018-07-18_Adjacency.csv\", header=FALSE, dec=\".\"))\n\n##########\n#Send input data from the example problem as a table\nknitr::kable(barrieralts, caption=\"Table 3. Input data for the WUCT to the Bronx River as described in the text. Each node specifies the existing condition (shown as Alt=0), and multiple restoration alternatives are also be specified for each node (shown as Alt>0).\")\n#Isolate properties of the alternatives at each barrier\nbnames <- paste(unique(barrieralts$BarrierID))\nnbar <- length(bnames)\n\n##########\n#Compute the number of alternatives at each barrier\nnalts <- c()\nfor(i in 1:nbar){nalts[i] <- length(which(barrieralts$BarrierID == bnames[i]))}\nnalts.total <- prod(nalts) #Compute the total number of combinations of actions\n\n##########\n#Create a list which stores all alternatives at each site\n  #This list stores the row number of each alternative from \"barrieralts\".\nsite.alts <- list() #Create an empty list to store the combinations of sites/alts\nfor(i in 1:nbar){site.alts[i] <- list(which(barrieralts$BarrierID == bnames[i]))}\n\n#Compute all combinations of alternatives and sites\n  #This returns a data frame where each columns is a site or barrier and each row is a unique plan with a combination of sites and alternatives.\nsitealts.combos <- expand.grid(site.alts)\n\n#Convert this data frame to a matrix \nsitealts.combos <- data.matrix(sitealts.combos)\n\n##########\n#Compute connectivity for each plan\nWC.out <- c() #Empty vector to store watershed connectivity for each plan\nfor(i in 1:nalts.total){\n  #Isolate the passability and habitat values to be used\n  pass.temp <- barrieralts$Passability[sitealts.combos[i,]]\n  hab.temp <- barrieralts$QualityUpstream[sitealts.combos[i,]] * barrieralts$QuantityUpstream[sitealts.combos[i,]]\n  \n  #Compute connectivity and store result\n  WC.out[i] <- connectivity(nbar,A,pass.temp,hab.temp)\n}\n\n##########\n#Send output from the example problem as a table\nWC.HRE <- matrix(NA, nrow=nalts.total,ncol=nbar+2)\nfor(i in 1:nalts.total){WC.HRE[i,] <- c(i,barrieralts$Alt[sitealts.combos[i,]],WC.out[i])}\ncolnames(WC.HRE) <- c(\"Plan\",bnames,\"Connectivity\")\nknitr::kable(WC.HRE, caption=\"Table 4. Alternatives analysis using the WUCT in the Bronx River Watershed. Competing restoration alternatives are shown as rows, and the alternative used in the plan is denoted by the number shown.\")"},{"path":"network-models-and-connectivity.html","id":"summary-5","chapter":"18 Network Models and Connectivity","heading":"18.5 Summary","text":"model document describes procedure quantifying benefits associated removal organism movement barriers within watershed (e.g., dam removal, culvert repair, fish ladder installation) impacts barrier addition (e.g., dam construction, weir installation). model focuses upstream movement migratory organisms fish operates watershed-scale. algorithm based four primary components: habitat quantity upstream dam, habitat quality upstream dam, passability structure given organism, shape/topology watershed. algorithm combines data estimate quality-weighted, accessible habitat watershed scale.modeling framework applied multiple USACE projects varying input parameters ecological contexts. notable limitations assumptions include following issues. Future improvements WUCT consider addressing limitations.Upstream movement. current model incorporates movement upstream, implicitly assumes downstream movement limiting factor organism’s life cycle. However, downstream movement can important driver ecological function (e.g., fish trapping agricultural diversion ditches fish mortality due hydropower turbines). connectivity algorithms can incorporate bidirectional movement, currently incorporated WUCT.WUCT primarily applicable context migratory fishes focus connectivity outlet system. Alternative connectivity algorithms exist resident fishes (e.g., Cote et al. 2009).Numerical limits. basic WUCT algorithm example application extremely flexible large-scale examination watershed priorities benefits. However, numerical problem size increases rapidly number barriers alternatives. instance, watershed 30 barriers considering presence absence barriers result billion combinations actions. , care taken limit problem size extent practicable.User interface. lack user interface limits applicability toolkit familiar R programming language.","code":""},{"path":"network-models-and-connectivity.html","id":"summary-6","chapter":"18 Network Models and Connectivity","heading":"18.6 Summary","text":"WUCT simple, flexible framework quantifying connectivity watersheds potential improvements restoration actionsThe adaptability framework primary strength tool, flexibility also leads significant variation application.","code":""},{"path":"agent-based-models.html","id":"agent-based-models","chapter":"19 Agent-based Models","heading":"19 Agent-based Models","text":"Consider exploring abmR package conduct agent-based models Rhttps://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.14014","code":""},{"path":"decision-support.html","id":"decision-support","chapter":"20 Decision Support","heading":"20 Decision Support","text":"Using ecological model output restoration decisions.","code":""},{"path":"annualizing-benefits-and-costs-with-ecorest-and-engrecon.html","id":"annualizing-benefits-and-costs-with-ecorest-and-engrecon","chapter":"21 Annualizing benefits and costs with ecorest and EngrEcon","heading":"21 Annualizing benefits and costs with ecorest and EngrEcon","text":"module teach use ecorest EngrEcon packages R efficiently reproducibly calculate annualized benefits costs restoration project planning:Authors: Ed Stowe (Writing, code), Darixa Hernandez-Abrams (benefits code), Kyle McKay (benefits code), Liya Abera (costs code) Cost annualization write-Utoy Creek: Kyle McKay, Stephen Phillips, Liya Abera, Garrett MenichinoLast update: 2025-02-06Acknowledgements: ","code":""},{"path":"annualizing-benefits-and-costs-with-ecorest-and-engrecon.html","id":"learning-objectives-9","chapter":"21 Annualizing benefits and costs with ecorest and EngrEcon","heading":"21.1 Learning objectives","text":"Understand annualization important making comparisons costs benefits among projectsLearn calculate annualized ecological benefits ecorest packageLearn calculate annualized costs using EngrEcon package","code":""},{"path":"annualizing-benefits-and-costs-with-ecorest-and-engrecon.html","id":"background-on-annualization","chapter":"21 Annualizing benefits and costs with ecorest and EngrEcon","heading":"21.2 Background on annualization","text":"Annualization technique determining costs benefits projects annual time scale, regardless overall time horizon project. Calculating annualized costs benefits important component restoration planning enables fair comparison alternative projects whose costs /benefits occur different periods time following implementation.","code":""},{"path":"annualizing-benefits-and-costs-with-ecorest-and-engrecon.html","id":"annualizing-benefits","chapter":"21 Annualizing benefits and costs with ecorest and EngrEcon","heading":"21.3 Annualizing benefits","text":"Annualization benefits can easily carried ecorest package. Along ecorest also load tidyverse package plotting/wrangling EngrEcon package cost annualization.Let’s imagine simple scenario ecosystem restoration benefits calculated four time periods year 0 year 50. can store benefits dataframe plot ggplot follows:\nessence, total benefits project can thought area curve, can add plot using geom_area() function.annualization take total shaded area benefits curve, divide number years. essentially looks like carving area triangles rectangles, calculating areas shapes adding together, finally dividing get annual benefit. df_rect code simply creates dataframe coordinates two rectangles add plot visualization purposes.ecorest package provides easy way achieve benefit annualization annualizer function, computes “time-averaged quantities based linear interpolation.” Linear interpolation just means connect different points time straight lines order calculate overall area.Inputs annualizer function 1) vector time intervals \n2) vector values interpolate.can see, therefore, annualized ecological benefits toy project 87.2 units (e.g., acres, habitat units, etc.).","code":"\nlibrary(tidyverse)\nlibrary(ecorest)\nlibrary(EngrEcon)\nlibrary(scales) # Needed to convert costs to dollars for table outputs\n#User-specified time intervals\nben_df <- data.frame(year = c(0,2,20,50),\n                     ben = c(0,40,35,30))\n\np <- ggplot(ben_df)+\n  geom_point(aes(year, ben), size = 2)+\n  geom_line(aes(year, ben))+\n  labs(x = \"Year\", y = \"Benefits\")+\n  theme_classic()\n  \np\np <- p + \n  geom_area(aes(x = year, y = ben), \n            fill = \"skyblue\")\n\np\ndf_rect <- tibble(xmin = ben_df$year[2:3],\n                  xmax = ben_df$year[3:4],\n                  ymin = c(0,0),\n                  ymax = ben_df$ben[3:4])\n\np + geom_rect(data = df_rect,\n            aes(xmin = xmin, xmax = xmax, ymin = ymin, ymax = ymax),\n            fill = NA, lty = \"dashed\", color = \"black\")\nannualizer(ben_df$year, ben_df$ben)## [1] 33.8"},{"path":"annualizing-benefits-and-costs-with-ecorest-and-engrecon.html","id":"annualizing-costs","chapter":"21 Annualizing benefits and costs with ecorest and EngrEcon","heading":"21.4 Annualizing costs","text":"Cost annualization carried fair cost comparisons can made among alternative project plans costs incurred different time points.Cost annualization procedures vary among projects, typically involves first calculating present value project costs, including operation maintenance costs incurred future time periods, interest construction costs. Present value means current value future sum money, calculated discounting future value much money earned invested. present values costs calculated, annualized costs can calculated present values. Annualized costs serve basis comparing cost differences among project alternatives.provide real-world example carry cost annualization, use data feasibility study potential restoration actions Utoy Creek, degraded watershed Atlanta, GA. USACE’s Mobile District partnering city Atlanta carry work. Cost effectiveness Incremental Cost analysis (CEICA) used guide development recommended analysis CEICA requires use annualized cost (benefit) data. See 22 full implementation CEICA.annualize cost data using EngrEcon R package. Note: web application also exists conducting calculations leverages R package.","code":""},{"path":"annualizing-benefits-and-costs-with-ecorest-and-engrecon.html","id":"cost-data","chapter":"21 Annualizing benefits and costs with ecorest and EngrEcon","heading":"21.4.1 Cost data","text":"order carry cost annualization, start storing federal discount rate 3.0% object called discount. , load cost data inspect glimpse function fromdplyr.dataset several columns pertaining site alternative plan characteristics (e.g., ConstrDuration duration construction), well three cost columns: ConstrCost, MonitoringCost, AdManCost, refer costs construction, monitoring adaptive management, respectively.","code":"\n# Set federal discont rate for water resources projects in FY24\ndiscount <- 0.03\n\n# Read in initial costs\ncosts <- read_csv(\"data/utoy_cost.csv\")\n\nglimpse(costs)## Rows: 31\n## Columns: 9\n## $ SiteID         <chr> \"17F2M\", \"17F2M\", \"17F2M\", \"17F2M\", \"17D2E\", \"17D2E\", \"17D2E\", \"17D2E\", \"17…\n## $ SiteAction     <chr> \"FWOP\", \"Alternative1\", \"Alternative2\", \"Alternative3\", \"FWOP\", \"Alternativ…\n## $ Description    <chr> \"No action\", \"Concrete channel removal with extensive riparian restoration\"…\n## $ ConstrDuration <dbl> 0, 10, 9, 11, 0, 14, 14, 2, 0, 12, 12, 2, 0, 6, 5, 2, 0, 4, 4, 2, 0, 8, 14,…\n## $ RipAreaFt2     <dbl> 0.0, 94150.0, 33894.0, 232812.0, 0.0, 916856.4, 804130.4, 664130.4, 0.0, 43…\n## $ ReachLengthFt  <dbl> 0.000, 2639.110, 2378.154, 2639.110, 0.000, 3610.575, 3610.575, 3610.575, 0…\n## $ ConstrCost     <dbl> 0, 776047, 691092, 798779, 0, 1070854, 1065670, 276233, 0, 1002697, 880983,…\n## $ MonitoringCost <dbl> 0, 60000, 56000, 48000, 0, 48000, 48000, 48000, 0, 48000, 48000, 48000, 0, …\n## $ AdManCost      <dbl> 0, 77605, 69109, 79878, 0, 160628, 106567, 27623, 0, 150405, 132147, 38642,…"},{"path":"annualizing-benefits-and-costs-with-ecorest-and-engrecon.html","id":"operations-and-maintenance-costs","chapter":"21 Annualizing benefits and costs with ecorest and EngrEcon","heading":"21.4.2 Operations and maintenance costs","text":"Operations, maintenance, repair, replacement, rehabilitation (OMRRR) costs occur periodically project’s lifespan, first need calculate present value costs can calculate annualized costs. Utoy project, present value OMRRR costs estimated assuming four common management practices:Quarterly site visits trash removal minor maintenance small restoration educational features. Cost: $10,000 per year.Quarterly site visits trash removal minor maintenance small restoration educational features. Cost: $10,000 per year.Bi-annual site visits include simple inspection restoration work. Cost: $4.00 per linear foot stream every two years.Bi-annual site visits include simple inspection restoration work. Cost: $4.00 per linear foot stream every two years.Invasive species removal. Cost: $15,000 every 10 years.Invasive species removal. Cost: $15,000 every 10 years.Repair structural restoration features (e.g., rock, wood, earth work). Cost: 10% construction cost every 25 years.Repair structural restoration features (e.g., rock, wood, earth work). Cost: 10% construction cost every 25 years.EngrEcon package simple function called om_distribute calculate present values OMRRR costs. ran code ?om_distribute, see documentation function takes four inputs:Discount rate percent per yearDiscount rate percent per yearProject life span (years)Project life span (years)Frequency costs incurred (years, e.g., value 1 indicates every year)Frequency costs incurred (years, e.g., value 1 indicates every year)Cost incurred interval (present value)Cost incurred interval (present value)Therefore, starting dataframe costs project (costs) using om_distribute() function, can use following code create new dataframe OMRRR costs, including one column OMRRR categories listed , along column sums OMRRR costs together (TotalOMRR). cost category, interval costs (e.g., $10,000 general maintenance) multiplied project specific value, ensures OMRRR costs FWOP scenarios 0 cases, also adjusts interval costs reach length case biannual monitoring, original construction cost case repair structural restoration features.new dataframe costs_omrr now contains present value costs one dataframe.","code":"\ncosts_omrrr <- costs %>%\n  rowwise() %>%\n  mutate(General = om_distribute(discount, 50, 1, 10000 * if_else(RipAreaFt2>0, 1, 0)),\n         InspectionTrash = om_distribute(discount, 50, 2, 4 * ReachLengthFt),\n         Invasives = om_distribute(discount, 50, 10, 15000 * if_else(RipAreaFt2>0, 1, 0)),\n         WoodRockEarth = om_distribute(discount, 50, 25, 0.1 * ConstrCost),\n         TotalOMRR = General + InspectionTrash + Invasives + WoodRockEarth)"},{"path":"annualizing-benefits-and-costs-with-ecorest-and-engrecon.html","id":"interest-during-construction","chapter":"21 Annualizing benefits and costs with ecorest and EngrEcon","heading":"21.4.3 Interest during construction","text":"also need calculate **interest construction* (IDC). context USACE, IDC thought “opportunity cost capital incurred druing construction period.” words, money spent project, represents opportunities carried , additionally, construction, asset built yet functional doesn’t yet represent benefit. Thus, IDC create appearance added cost projects longer construction durations higher construction costs.Utoy Creek, interest construction computed based construction costs specific construction durations alternative plan. EngrEcon provides function calculating IDC interest_during_construction, takes inputs discount rate, duration construction months, construction costs. following code calculates new column IDC. Note: OMRR costs, need account FWOP, construction durations 0 months, function interest_during_construction like. use if_else statement indicate IDC calculated using interest_during_construction function construction duration greater 0 months, otherwise 0 (FWOP projects).","code":"\ncosts_idc <- costs_omrrr %>%\n  rowwise() %>%\n  mutate(IDC = if_else(ConstrDuration > 0, \n                       interest_during_construction(discount, ConstrDuration, ConstrCost),\n                       0))"},{"path":"annualizing-benefits-and-costs-with-ecorest-and-engrecon.html","id":"annualize-all-costs","chapter":"21 Annualizing benefits and costs with ecorest and EngrEcon","heading":"21.4.4 Annualize all costs","text":"Finally, need convert present value costs expenditures annualized costs, can make fair comparisons among projects. , use EngrEcon function present_to_annual. function similar inputs previous functions: discount rate, life span project (50 years) finally present value cost item.following code uses mutate finction calculate new annualized costs cost category, followed summation annualized costs, TotalAnn, ultimately used compare among projects!Let’s create table display results. use dollar() function scales packages format cost columns dollars.Table 21.1: Table 1. Monetary cost data Utoy sites.","code":"\ncosts_ann <- costs_idc %>% \n  rowwise() %>%\n  mutate(ConstrCostAnn = present_to_annual(discount, 50, ConstrCost),\n         IDCAnn = present_to_annual(discount, 50, IDC),\n         MonitoringCostAnn = present_to_annual(discount, 50, MonitoringCost),\n         AdManCostAnn = present_to_annual(discount, 50, AdManCost),\n         OMAnn = present_to_annual(discount, 50, TotalOMRR),\n         TotalAnn = ConstrCostAnn + MonitoringCostAnn + AdManCostAnn + IDCAnn + OMAnn)\ncost_table <- costs_ann %>%\n  select(SiteID, SiteAction, ConstrDuration, ConstrCost, MonitoringCost, AdManCost, OMAnn, TotalAnn) %>%\n  mutate(ConstrCost = dollar(ConstrCost), # Format as dollars with scales package\n         MonitoringCost = dollar(MonitoringCost),\n         AdManCost = dollar(AdManCost),\n         OMAnn = dollar(round(OMAnn), 1),\n         TotalAnn = dollar(round(TotalAnn), 1))\n\ncolnames(cost_table) <- c(\"Site\", \"Alternative\", \"Construction Duration (mo)\", \n                       \"Construction Cost\", \"Monitoring Cost\", \"Adapative Mgmt. Cost\",\n                       \"OMRRR Annual Cost\", \"Total Annualized Cost\")\n\nknitr::kable(cost_table, caption=\"Table 1. Monetary cost data for Utoy sites.\", align='c') "},{"path":"annualizing-benefits-and-costs-with-ecorest-and-engrecon.html","id":"summary-7","chapter":"21 Annualizing benefits and costs with ecorest and EngrEcon","heading":"21.5 Summary","text":"Annualizing costs benefits important step apples--apples comparisons can made among alternative projects may oeprate different time scalesAnnualizing benefits simple calculate using annualizer function ecorest packageAnnualizing costs involved, also made considerably manageable using EngrEcon package","code":""},{"path":"ceica-chapter.html","id":"ceica-chapter","chapter":"22 Cost-Effectiveness and Incremental Cost Analysis (CEICA) with ecorest","heading":"22 Cost-Effectiveness and Incremental Cost Analysis (CEICA) with ecorest","text":"module teach use ecorest package R efficiently reproducibly carry Cost Effectivness Incremental Cost Analysis (CEICA) generate useful output like just lines code:Authors: Ed Stowe (Writing, code), Darixa Hernandez-Abrams (writing, code), Kyle McKay (writing, code). CEICA background information adapted draft tech report ecorest R package, authored Kyle McKay, Darixa Hernández-Abrams, Rachel Nifong, Todd SwannackLast update: 2025-02-06Acknowledgements: ","code":""},{"path":"ceica-chapter.html","id":"learning-objectives-10","chapter":"22 Cost-Effectiveness and Incremental Cost Analysis (CEICA) with ecorest","heading":"22.1 Learning objectives","text":"Understand purpose Cost Effectiveness Incremental Cost AnalysisUnderstand cost effective best buy alternatives representLearn use ecorest package determine cost effective best buy options","code":""},{"path":"ceica-chapter.html","id":"background-on-cost-effectivness-and-incremental-cost-analysis","chapter":"22 Cost-Effectiveness and Incremental Cost Analysis (CEICA) with ecorest","heading":"22.2 Background on Cost Effectivness and Incremental Cost Analysis","text":"Cost-effectiveness incremental cost analyses (CEICA) analytical tools assessing relative benefits costs ecosystem restoration actions informing decisions. Benefits costs assessed prior analyses using ecological models cost engineering methods, respectively. CEICA may conducted site scale compare alternatives single location (e.g., action vs. dam removal vs. fish ladder) system scale compare relative merits multiple sites (e.g., sites vs. Site-vs. Site-B vs. Site-Site-B).Cost-effectiveness analysis provides mechanism examining efficiency alternative actions. given level investment (.e., cost), agency wants identify plan return--investment (.e., environmental benefits), given level environmental benefits, agency wants plan least cost. “efficiency frontier” identifies plans efficiently provide benefits per cost basis (.e., cost-effective plans).Incremental cost analysis conducted set cost-effective plans. technique sequentially compares plan higher cost plans reveal changes unit cost output levels increase eliminates plans efficiently provide benefits per unit cost basis. Specifically, analysis examines slope cost-effectiveness frontier isolate incremental unit cost ($/unit) increases magnitude environmental benefit increases. Incremental cost analysis ultimately intended inform decision-makers consequences increasing unit cost increasing benefits (.e., unit becomes expensive). Plans emerging incremental cost analysis efficiently accomplish objective relative unit costs typically referred “best buys”. Importantly, “best buys” cost-effective, cost-effective plans best buys.","code":""},{"path":"ceica-chapter.html","id":"ceica-with-ecorest-in-4-easy-steps","chapter":"22 Cost-Effectiveness and Incremental Cost Analysis (CEICA) with ecorest","heading":"22.3 CEICA with ecorest in 4 easy steps!","text":"Within USACE, Institute Water Resources provided toolkit conducting CEICA, IWR Planning Suite (http://www.iwr.usace.army.mil/Missions/Economics/IWR-Planning-Suite/). However, CEICA can also accomplished swiftly easily using ecorest package R, associated benefits occur conducting analyses programmatically R (e.g., reproduciblilty, re-usable scripts, etc.).Section 22.4 walks CEICA works step--step users can develop deeper understanding CEICA works. first section, demonstrate easy process using functions ecorest package. following code everything one needs determine alternatives cost effective ‘best buys’, plot results. demonstration, use data Beaver Island Habitat Rehabilitation Enhancement Project, part Upper Mississippi River Restoration program.First, load required packages Beaver Island cost-benefit data.Let’s check cost-benefit data. , ’ll first create new dataframe ’s suitable output table: can classify cost figures dollars (using scales package), rename columns ’re easier interpret. ’ll use kable function knitr package view table.determine project alternatives cost effective, ecorest package function called CEfinder. can use args() function determine inputs needed CEfinder function.inputs consist 2 vectors: 1) Project benefits; 2) Project annualized costs. ’ll make vectors use function.see restCE object vector 1s 0s, 1s cost effective options. indicates three options cost-effective. still need figure options ‘best buys.’, can use BBfinder function. inputs vectors benefits costs, well vector cost effectiveness scores ’ve just created.output BB_finder() function list comprised two dataframes. first data frame columns restoration benefits, costs, whether alternatives cost effective best buys.second dataframe features cost, benefit, incremental costs best buy compared previous best buy (.e., slope cost effectivness frontier). example, third best buy costs $318,026 second best buy, yields additional benefits 45 habitat units. dividing incremental costs benefits, can see per-unit incremental cost best buy three $7,067.24. cost additional unit benefit beyond second best buy.Now ’ve determined cost effective best buy options can create table output. Note: BB_list object list two dataframes, need extract just column 1s 0s indicating ‘best buy’ status. , use code BB_list[[1]][,4], pulls first dataframe list (.e., [[1]]), fourth column dataframe (.e., [,4]). See information subsetting brackets.create new dataframe based earlier table created: remove ‘Description’ column save space add two new columns cost effectiveness best buy information.Finally, can use CEICAplotter function create save plots depicting results cost effectiveness analysis, incremental cost analysis. five required inputs function project alternative names (can pull original dataframe using bi_cost_bens$AltID), benefits, annualized costs, cost effectiveness scores, best buy scores, file name plot. plot created user’s working directory filename choice.’s ! user cost benefit data, three functions takes find cost effective (CEfinder) best buy options (BBfinder), create plot overall CEICA results (CEICAplotter). underlying cost benefit data change, R script can instantly update output new data.","code":"\nlibrary(tidyverse)\nlibrary(ecorest)\nlibrary(knitr) # Needed to created quality tables\nlibrary(scales) # Needed to convert costs to dollars for table generation\n\n\n# Most basic with Beaver Island\nbi_cost_bens <- read_csv(\"data/beaver_island_cost_ben.csv\") \nfirst_table <- bi_cost_bens %>%\n  mutate(AnnCost = dollar(AnnCost), # Format as dollars with scales package\n         AvgCost = dollar(AvgCost)) %>%  # Format as dollars\n  rename(\"Alt. ID\" = AltID,\n         \"Description\" = AltName,\n         \"Rest. Benefit\" = RestBen,\n         \"Ann. Cost\" = AnnCost,\n         \"Avg. Cost\" = AvgCost)\n\nkable(first_table, padding = 2, align = \"clccc\")\nargs(CEfinder)## function (benefit, cost) \n## NULL\nbenefits <- bi_cost_bens$RestBen\ncosts <- bi_cost_bens$AnnCost\n\nrestCE <- CEfinder(benefits, costs)\nrestCE##  [1] 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 1 1 1 1\nBB_list <- BBfinder(benefits, costs, restCE)\nBB_list## [[1]]\n##       benefit    cost CE BB\n##  [1,]     0.0       0  1  1\n##  [2,]   162.5  496526  1  0\n##  [3,]   163.4  499260  1  0\n##  [4,]   172.1  513615  1  0\n##  [5,]   179.2  518462  1  1\n##  [6,]   196.1  721698  0  0\n##  [7,]   203.1  804896  0  0\n##  [8,]   203.2  719258  1  0\n##  [9,]   210.2  809741  1  0\n## [10,]   217.1  831390  1  0\n## [11,]   224.2  836488  1  1\n## [12,]   224.5  927732  1  0\n## [13,]   207.5  933020  0  0\n## [14,]   233.1  944914  1  0\n## [15,]   240.2  950447  1  1\n## [16,]   252.1 1117284  1  0\n## [17,]   259.2 1120429  1  0\n## [18,]   262.1 1202194  1  0\n## [19,]   269.2 1204094  1  1\n## \n## [[2]]\n##      benefit    cost  inccost\n## [1,]     0.0       0    0.000\n## [2,]   179.2  518462 2893.203\n## [3,]   224.2  836488 7067.244\n## [4,]   240.2  950447 7122.438\n## [5,]   269.2 1204094 8746.448\nbb_vector <- BB_list[[1]][,4]\n\n#Create a table of the results\nceica_table <- first_table %>% \n  select(-Description) %>%\n  mutate(\n    \"Cost Effectiveness\" = restCE,\n    \"Best Buy\" = bb_vector) \n\n  kable(ceica_table, padding = 2, align = \"lccccc\")\n# Create a plot of CEICA results\nCEICAplotter(bi_cost_bens$AltID, benefits, costs, restCE, bb_vector, \"images/CEICAexample1.jpeg\")"},{"path":"ceica-chapter.html","id":"ceica-full","chapter":"22 Cost-Effectiveness and Incremental Cost Analysis (CEICA) with ecorest","heading":"22.4 CEICA in depth","text":"’ve seen CEICA can accomplished quickly code. However, user wishes understanding projects determined cost effective best buys, go greater depth .","code":""},{"path":"ceica-chapter.html","id":"identifying-cost-effective-alternatives","chapter":"22 Cost-Effectiveness and Incremental Cost Analysis (CEICA) with ecorest","heading":"22.4.1 Identifying cost effective alternatives","text":"first component CEICA cost effectiveness analysis., alternative, ask: alternatives produce higher ecological outcomes equal lower costs?\nanswer yes, alternative considered non-cost effective.Non-cost effectiveness can though two ways:\n* Inefficient Production: alternative output level can generated lesser cost another alternative.\n* Ineffective Production: alternative greater output level can generated lesser equal cost another alternative.Let’s look closely cost/benefit data :first alternative, cost $0, considered cost effective, projects lesser equal cost. let’s look second option see ’s cost effective. , first see alternatives greater benefits, alternatives greater benefits lower cost, option 2 cost effective.function used return positions (e.g., first number, second, ) within benefit vector benefits greater equal benefits second option vector (.e., benefit[2]).Looking output, see projects greater equal benefits alternative #2.Now, ’re looking see projects equal greater benefits lower costs.value 1 indicates cost first position (.e., alternative #2’s cost) less equal. words, alternatives greater equal benefits cost less , meaning cost effective option. stoll don’t know though best buy.Let’s look different alternative using code: Alternative E2L1, sixth alternative:suggests another alternative greater equal level benefits costs less E2L1. index value 3 (compared index value 1 E2L1), suggests ’s two projects E2L1 project list: E2L2, lower cost higher benefit. Therefore, E2L1 considered cost effective.’ve just done project, take time tedious. Instead, can iterate projects using ‘-loop’, following code indicates:iteration -loop (.e., values 1 19), run essentially code . -else statement indicates length second vector produced 1 (first alternative ), project cost effective assign value 1 CE vector. However, length second vector 1 (alternative E2L1), project cost-effective assign 0.Using -loop thus lets us quickly determine alternatives cost-effective .","code":"\nbi_cost_bens %>% \n  select(AltID, AnnCost, RestBen) %>% \n  knitr::kable()\n# Vectors defined to make calculations easier\nbenefits <- bi_cost_bens$RestBen\ncosts <- bi_cost_bens$AnnCost\n\n# Which projects have greater than or equal benefits than the second project's benefits\ngreater_bens <- which(benefits >= benefits[2])\ngreater_bens##  [1]  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19\nwhich(costs[greater_bens] <= costs[2])## [1] 1\ngreater_bens <- which(benefits >= benefits[6])\n\nwhich(costs[greater_bens] <= costs[6])## [1] 1 3\n# Set up empty vector to store whether each alternative is cost effective\nCE <- c()\n\n# Iterate through each project to see if another project renders that one cost ineffective\n# An ifelse statement assigns a value of 1 to projects that are cost effective, and 0 to ones that are not\nfor (i in 1:length(benefits)) {\n        bigben <- which(benefits >= benefits[i])\n        CE[i] <- ifelse(length(which(costs[bigben] <= costs[i])) == \n            1, 1, 0)\n    }\n\ncbind(bi_cost_bens$AltID, CE)##              CE \n##  [1,] \"0\"    \"1\"\n##  [2,] \"D2L3\" \"1\"\n##  [3,] \"D2L4\" \"1\"\n##  [4,] \"D2L1\" \"1\"\n##  [5,] \"D2L2\" \"1\"\n##  [6,] \"E2L1\" \"0\"\n##  [7,] \"F2L1\" \"0\"\n##  [8,] \"E2L2\" \"1\"\n##  [9,] \"F2L2\" \"1\"\n## [10,] \"H2L1\" \"1\"\n## [11,] \"H2L2\" \"1\"\n## [12,] \"I2L3\" \"1\"\n## [13,] \"H2L3\" \"0\"\n## [14,] \"G2L1\" \"1\"\n## [15,] \"G2L2\" \"1\"\n## [16,] \"J2L1\" \"1\"\n## [17,] \"J2L2\" \"1\"\n## [18,] \"K2L1\" \"1\"\n## [19,] \"K2L2\" \"1\""},{"path":"ceica-chapter.html","id":"identifying-best-buys","chapter":"22 Cost-Effectiveness and Incremental Cost Analysis (CEICA) with ecorest","heading":"22.4.2 Identifying ‘best buys’","text":"Identifying best buys matter finding cost effective alternatives can increase environmental benefits lowest incremental cost (.e., lowest cost per additional habitat unit).first best buy always one lowest average cost 0.find best buys, can start vectors benefits costs, now subset include just cost effective options, reorder benefits costs lowest greatest cost.costs benefits greater first cost effective option (.e., $0 cost-0 benefit future without project FWOP), now calculate incremental costs: difference costs among subsequent alternatives, divided difference benefits among subsequent alternatives. cost.CE2[-1] ben.CE2[-1] code costs benefits except first item list, (.e., costCE2[1]).want find lowest incremental cost FWOP. Eyeballing list, can see ’s fourth cost, can also using code. find project corresponds , need add 1 value, incremental costs include first project, .e., FWOP.indicates fifth alternative second best-buy FWOP.look next best buy, want just look projects fifth project. first generate cost benefit vectors projects alternative five. calculate incremental costs projects fifth alternative. values bracket [-1:-5] indicate remove first five values vectors.new vector incremental costs projects subsequent fifth alternative., wish find lowest incremental cost alternative, use code previously, except , need add 5 (considering projects fifth alternative)indicates 9th option (among cost effective alternatives) next one best buy.user iterate way options. previously noted, iterations required, often great time use -loops. following code accomplishes . code nearly identical ’ve already run. main difference use BB vector store alternatives best buys sure iteration, new set alternatives consider exclusively subsequent iterations. Note, end loop code, statement ends (.e., breaks) -loop BB vector length equal greater total number cost effective options (nCE) (.e., get end vector projects).indicates best buys positions 1, 5, 9, 12, 16 cost effective projects, (total list projects).see projects overall list can run following code. start original cost-benefit dataframe bi_cost_bens, add cost effectiveness vector CE filter just cost effective options finally arrange cost (since previously sorted way finding best buys). best buys now just project alternatives rows identified BB vector (.e., rows: 1, 5, 9, 12, 16), can use dplyr::slice() function filter just rows. Finally use pull() create vector just alternative IDs best buys.want view results tabular form previously, can make new dataframe alternative IDs, costs benefits, options cost-effective, best buys; print using kable() function. create best_buy column, use -else function scores row 1 row’s Alt ID vector BB_IDs otherwise codes row 0.","code":"\n# Subset to cost effective options\nben.CE <- benefits[which(CE == 1)]\ncost.CE <- costs[which(CE == 1)]\n\n# Reorder benefits and costs from lowest to greatest cost\nben.CE2 <- ben.CE[order(cost.CE)] \ncost.CE2 <- cost.CE[order(cost.CE)]\ninccost <- (cost.CE2[-1] - cost.CE2[1])/(ben.CE2[-1] - ben.CE2[1])\n            \ninccost %>% dollar()##  [1] \"$3,055.54\" \"$3,055.45\" \"$2,984.40\" \"$2,893.20\" \"$3,539.66\" \"$3,852.24\" \"$3,829.53\" \"$3,730.99\"\n##  [9] \"$4,132.44\" \"$4,053.69\" \"$3,956.90\" \"$4,431.91\" \"$4,322.64\" \"$4,586.78\" \"$4,472.86\"\nwhich(inccost == min(inccost)) + 1## [1] 5\nce.ben.temp <- ben.CE2[-1:-5] # Selects all the project benefits with greater benefits than the current best buy\nce.cost.temp <- cost.CE2[-1:-5] # All the project costs with greater costs than current best buy\ninccost <- (ce.cost.temp - cost.CE2[5])/(ce.ben.temp - ben.CE2[5]) # calculate avg. costs from each costlier option compared to the current best buy\n\ninccost %>% dollar()##  [1] \"$8,366.50\" \"$9,396.10\" \"$8,256.68\" \"$7,067.24\" \"$9,034.66\" \"$7,911.91\" \"$7,081.72\" \"$8,214.29\"\n##  [9] \"$7,524.59\" \"$8,247.67\" \"$7,618.13\"\nwhich(inccost == min(inccost)) + 5## [1] 9\nBB <- c(1) # Create vector to store BB index positions; the lowest cost option (i.e., position 1 is always a best-buy)\nnCE <- length(ben.CE)\n\nfor (i in 1:nCE) {\n        ce.bentemp <- ben.CE2[-1:-BB[i]] # Selects all the project benefits with greater benefits than the current best buy\n        ce.costtemp <- cost.CE2[-1:-BB[i]] # All the project costs with greater costs than current best buy\n        inccost <- (ce.costtemp - cost.CE2[BB[i]])/(ce.bentemp - ben.CE2[BB[i]]) # calculate incr. costs from each subsequent option compared to the current best buy\n        BB[i + 1] <- which(inccost == min(inccost)) + BB[i] # Of all the incremental costs above this best_buy, which has the lowest?\n        if (BB[i + 1] >= nCE) {\n            break\n        }\n    }\n\nBB## [1]  1  5  9 12 16\nBB_IDs <- bi_cost_bens %>%    # Start with the original data frame\n  mutate(cost_eff = CE) %>%   # Add a column of the cost effectiveness scores\n  filter(cost_eff == 1) %>%   # Filter to just the cost effective options\n  arrange(AnnCost) %>%        # Arrange by cost\n  slice(BB) %>%               # Keep rows in the positions identified by BB\n  pull(AltID)                 # Pull the alternative names out\n  \nBB_IDs## [1] \"0\"    \"D2L2\" \"H2L2\" \"G2L2\" \"K2L2\"\nfirst_table %>%\n  select(-Description) %>%\n  mutate(\"Cost Effective\" = CE,\n         \"Best Buy\" = ifelse(`Alt. ID` %in% BB_IDs, 1, 0)) %>%\n  knitr::kable(padding = 2, align = \"c\")"},{"path":"ceica-chapter.html","id":"summary-8","chapter":"22 Cost-Effectiveness and Incremental Cost Analysis (CEICA) with ecorest","heading":"22.5 Summary","text":"Cost-effectiveness analysis provides mechanism examining efficiency alternatives.Alternatives cost effective aren’t alternatives produce higher ecological outcomes ≤ costsNot cost effective options best buys; options lowest incremental costs compared previous best buyThe ecorest package provides simple easy set functions determining project alternatives cost effective best buys plotting results","code":""},{"path":"model-integration.html","id":"model-integration","chapter":"23 Model Integration","heading":"23 Model Integration","text":"Integrating physical biological modelsThis section designed synthesize many components trainingEflows case studies\nNY Bight Ecological model exampleFloodplain inundation -> Flow metrics -> biological model\n- Take eco-value curve\n- Generate flow metrics duration inundation\n- Plug ecological model","code":""},{"path":"r-for-gis.html","id":"r-for-gis","chapter":"24 R for GIS","heading":"24 R for GIS","text":"Use info :\nhttps://rspatial.org/index.htmlObjectives","code":""},{"path":"intersect.html","id":"intersect","chapter":"25 Intersect","heading":"25 Intersect","text":"","code":""},{"path":"using-r-for-data-access.html","id":"using-r-for-data-access","chapter":"26 Using R for data access","heading":"26 Using R for data access","text":"","code":""},{"path":"using-r-for-data-access.html","id":"other-datasources-to-consider","chapter":"26 Using R for data access","heading":"26.1 Other datasources to consider","text":"","code":""},{"path":"using-r-for-data-access.html","id":"nlcd","chapter":"26 Using R for data access","heading":"26.2 NLCD","text":"","code":""},{"path":"using-r-for-data-access.html","id":"precipitation-data-prism-noaa-data","chapter":"26 Using R for data access","heading":"26.3 Precipitation data (PRISM? NOAA data?)","text":"","code":""},{"path":"using-r-for-data-access.html","id":"temperature-data-noaa","chapter":"26 Using R for data access","heading":"26.4 Temperature data (NOAA)","text":"","code":""},{"path":"using-r-for-data-access.html","id":"other-data-sources-commonly-used-in-usace","chapter":"26 Using R for data access","heading":"26.5 Other data sources commonly used in USACE?","text":"","code":""},{"path":"using-r-for-data-access.html","id":"code-for-ras-output-files-dss-netcdf","chapter":"26 Using R for data access","heading":"26.6 Code for RAS output files (DSS, netCDF)","text":"RAS post-processing code Brian Breaker, etc., dssrip?","code":""},{"path":"usgs-gage-data-using-the-dataretrieval-package.html","id":"usgs-gage-data-using-the-dataretrieval-package","chapter":"27 USGS gage data using the dataRetrieval package","heading":"27 USGS gage data using the dataRetrieval package","text":"Authors: Brian Breaker, USACE (dataRetrieval section); Ed Stowe (adaptation, edits)Last update: 2025-08-20Acknowledgements: exercise uses USGS dataRetrieval package USGS explore\ndata availability retrieve data. USGS dataRetrieval package accesses NWIS web portal.\nDocumentation packages can found :\nhttps://github.com/USGS-R/dataRetrievalLet’s start seeing streamflow data available Arkansas (AR)? can use whatNWISsites function.\n, Parameter code 00060 refers daily flow cubic feet per second (cfs).can subset just active sites, just ones unit-value flow data (e.g., 15 min flow).Now sites look , can reduce . example, just want sites Buffalo River? require filtering using character strings. Hint: look ?str_detectNow can get flow data 5 sites Buffalo River.“uv” going retrieve 15min data Sys.Date set equal date script run.can also get data associated gages using siteInfo function, including data drainage area.Let’s look data, mean flow data last hour site create simple time-series plotNow wanted convert data hourly data saved end hour.Create new data frame can open look back forth see changing stepWe can also download daily data starting many years ago, including data starting water year 2020 5 sites Buffalo River.can plot data using ggplot.wanted write gages instead filtering data,\ncan reference site number directly function.","code":"\nlibrary(tidyverse)\nlibrary(dataRetrieval)\nwhatSites <- whatNWISsites(stateCd = \"AR\", parameterCd = \"00060\")## Warning in whatNWISsites(stateCd = \"AR\", parameterCd = \"00060\"): NWIS servers are slated for\n## decommission. Please begin to migrate to read_waterdata_monitoring_location## GET: https://api.waterdata.usgs.gov/samples-data/codeservice/states?mimeType=application%2Fjson## GET: https://waterservices.usgs.gov/nwis/site/?stateCd=AR&parameterCd=00060&format=mapper\nwhatSites <- read_waterdata_monitoring_location(state_name = \"Arkansas\")## Requesting:\n## https://api.waterdata.usgs.gov/ogcapi/v0/collections/monitoring-locations/items?f=json&lang=en-US&limit=10000&state_name=Arkansas## ⠙ iterating 3 done (1.4/s) | 2.2s⠹ iterating 4 done (1.3/s) | 3s ⠸ iterating 5 done (1.3/s) | 3.9s⠼\n## iterating 6 done (1.3/s) | 4.7s⠴ iterating 7 done (1.2/s) | 5.6s⠦ iterating 8 done (1.3/s) | 6.3s\nwhatSites <- whatNWISsites(stateCd = \"AR\", parameterCd = \"00060\", \n                           siteStatus = \"active\")## Warning in whatNWISsites(stateCd = \"AR\", parameterCd = \"00060\", siteStatus = \"active\"): NWIS\n## servers are slated for decommission. Please begin to migrate to read_waterdata_monitoring_location## GET: https://api.waterdata.usgs.gov/samples-data/codeservice/states?mimeType=application%2Fjson## GET: https://waterservices.usgs.gov/nwis/site/?stateCd=AR&parameterCd=00060&siteStatus=active&format=mapper\nwhatSites <- whatNWISsites(stateCd = \"AR\", parameterCd = \"00060\", \n                           siteStatus = \"active\", hasDataTypeCd = \"dv\")## Warning in whatNWISsites(stateCd = \"AR\", parameterCd = \"00060\", siteStatus = \"active\", : NWIS\n## servers are slated for decommission. Please begin to migrate to read_waterdata_monitoring_location## GET: https://api.waterdata.usgs.gov/samples-data/codeservice/states?mimeType=application%2Fjson## GET: https://waterservices.usgs.gov/nwis/site/?stateCd=AR&parameterCd=00060&siteStatus=active&hasDataTypeCd=dv&format=mapper\n# filter all of our sites by the \"Buffalo\" in station name\nwhatSites <- whatSites %>% \n  dplyr::filter(str_detect(station_nm, \"Buffalo\"))\n\n# what information is available for those sites? includes start and end dates\nwhatInfo <- whatNWISdata(siteNumber = whatSites$site_no)## GET: https://waterservices.usgs.gov/nwis/site/?seriesCatalogOutput=true&sites=07055646,07055660,07055680,07056000,07056700## WARNING: whatNWISdata does not include\n## discrete water quality data newer than March 11, 2024.\n## For additional details, see:\n## https://doi-usgs.github.io/dataRetrieval/articles/Status.html\n# we don't want to look at water quality data... so let's filter it out\nwhatInfo <-  dplyr::filter(whatInfo, !data_type_cd == \"qw\")\nflowDat <- readNWISuv(whatSites$site_no, startDate = Sys.Date(), parameterCd = \"00060\")## GET: https://waterservices.usgs.gov/nwis/iv/?site=07055646%2C07055660%2C07055680%2C07056000%2C07056700&format=waterml%2C1.1&ParameterCd=00060&startDT=2025-08-27\n# Rename unwieldy names; dataRetrieval has a function to do that.\nflowDat <- renameNWISColumns(flowDat)\nsiteInfo <- readNWISsite(whatSites$site_no)## Warning in readNWISsite(whatSites$site_no): NWIS servers are slated for decommission. Please begin\n## to migrate to read_waterdata_monitoring_location## GET: https://waterservices.usgs.gov/nwis/site/?siteOutput=Expanded&format=rdb&site=07055646,07055660,07055680,07056000,07056700\n# What if we just want site number and drainage area for the sites\nsiteInfo <-  dplyr::select(siteInfo, site_no, drain_area_va)\np <- ggplot(flowDat, aes(x = dateTime, y = Flow_Inst, color = site_no)) + \n  geom_line()\n\np\nflowDat2 <- flowDat %>% \n  mutate(dateTime = as.POSIXct(dateTime, format = \"%Y-%m-%d %H:%M:%S\", tz = \"UTC\")) %>%\n  mutate(date_hour = floor_date(dateTime, unit = \"hour\")) %>%\n  group_by(site_no, date_hour) %>%  \n  summarize(flow = mean(Flow_Inst)) %>% \n  ungroup()\n\n# is there a relationship between flow and drainage area\nggplot(data = flowDat, aes(x = dateTime, y = Flow_Inst, color = site_no)) + \n     geom_line()+\n     geom_line(data = flowDat2, aes(x = date_hour, y = flow , color = site_no),\n               linetype=\"dashed\")\nflowDatdaily <- readNWISdv(whatSites$site_no, startDate = \"2019-10-01\", parameterCd = \"00060\", )\nflowDatdaily <- renameNWISColumns(flowDatdaily)\nlibrary(scales)\n\nggplot(data = flowDatdaily, aes(x = Date, y = Flow, color = site_no)) + \n     geom_line()+\n     scale_y_continuous(breaks = seq(0, 500000, 5000) )+ \n     scale_x_date(breaks=\"6 month\", labels=date_format(\"%Y-%m\"),expand = c(0,0) )\nflowDatdailyone <- dataRetrieval::readNWISdv(\"07055646\", startDate = \"2019-10-01\", parameterCd = \"00060\", statCd = \"00003\")## Warning in dataRetrieval::readNWISdv(\"07055646\", startDate = \"2019-10-01\", : NWIS servers are\n## slated for decommission. Please begin to migrate to read_waterdata_daily.## GET: https://waterservices.usgs.gov/nwis/dv/?site=07055646&format=waterml%2C1.1&ParameterCd=00060&StatCd=00003&startDT=2019-10-01\nflowDatdailytwo <- readNWISdv(c(\"07055646\", \"07055660\"), startDate = \"2019-10-01\", parameterCd = \"00060\")## Warning in readNWISdv(c(\"07055646\", \"07055660\"), startDate = \"2019-10-01\", : NWIS servers are\n## slated for decommission. Please begin to migrate to read_waterdata_daily.## GET: https://waterservices.usgs.gov/nwis/dv/?site=07055646%2C07055660&format=waterml%2C1.1&ParameterCd=00060&StatCd=00003&startDT=2019-10-01"},{"path":"national-hydrography-dataset.html","id":"national-hydrography-dataset","chapter":"28 National Hydrography Dataset","heading":"28 National Hydrography Dataset","text":"module teach interact National Hydrography Dataset river network data using nhdplusTools package R.Author: Ed StoweLast update: 2025-08-20Acknowledgements: Material inspired nhdplusTools vignette developed David Blodgett Mike Johnson, creators nhdplusTools package ","code":""},{"path":"national-hydrography-dataset.html","id":"learning-objectives-11","chapter":"28 National Hydrography Dataset","heading":"28.1 Learning objectives","text":"Learn National Hydrography Dataset contains.Learn build interact watershed networks using nhdplusTools package R.Learn R can facilitate analyses bring together NHD data along diverse datasets related dams, stream gages, land cover.","code":""},{"path":"national-hydrography-dataset.html","id":"background-3","chapter":"28 National Hydrography Dataset","heading":"28.2 Background","text":"National Hydrography Dataset (NHD) database represents water drainage network U.S. ’s features include rivers, streams, lakes, canals waterbodies, well features relevant water management, stream gages dams.NHD database packed information. stream segment, example, associated hundred variables, including length Strahler stream order. Several attributes also relate given segment neighboring segment, information entire watershed can characterized.NHD data can downloaded web accessed GIS software can unwieldy. However, useful package R, nhdplusTools package, makes easy perform lot common watershed related tasks, harnessing advantages performing tasks R, automating repetitive tasks.module, investigate watershed upstream Juliette Dam, former hydropower dam Ocmulgee River Georgia. Discussions removing dam ongoing, occurs, can helpful consider habitat upstream might re-connected migratory fishes.information nhdplusTools package, visit extensive package website.","code":""},{"path":"national-hydrography-dataset.html","id":"juliette-dam-case-study","chapter":"28 National Hydrography Dataset","heading":"28.3 Juliette Dam case study","text":"perform main case study need load nhdplusTools, well sf package spatial operations tidyverse package plotting data wrangling. don’t packages installed, first uncomment lines run code install , loading .Later module, also look two applications assess additional dams land use upstream Juliette Dam, applications require two additional R packages well.","code":""},{"path":"national-hydrography-dataset.html","id":"downloading-nhd-network-data","chapter":"28 National Hydrography Dataset","heading":"28.3.1 Downloading NHD network data","text":"build stream network upstream Juliette Dam, first need coordinates find nearest stream segment. coordinates, code creates spatial point using st_sfc function sf package. function, need tell coordinates (always listing longitude–x coordinate–first). also need set coordinate system (crs). give 4269, code (called EPSG) unique coordinate reference system. one coordinate system using latitude-longitude coordinates North America.Now want find associated stream segment catchment. stream segment identified ID called COMID. ID also identifies local catchment segment, area contributes surface flow given NHD segment. COMID also associated numerous attributes describe stream segment catchment.use discover_nhdplus_id function find COMID stream segment closest point.Adding raindrop = true creates downslope trace point location nearest flowline, although ’s important case point dam, middle river.returns spatial dataframe two rows, one stream segment interest containing important COMID, one path raindrop.Now visualize starting point stream segment ggplot. geom_sf function ggplot conveniently plots spatial objects created sf package. adjust x-axis labels make legible.now build river network upstream point using Network Linked Data Index spatially links several different hydrological data sources, including USGS stream gages. , first create list object, indicate using COMID data (featureSource = \"comid\"), value COMID stored earlier start_comid object. , use navigate_nldi function, set look upstream tributaries setting mode = \"UT. options include upstream mainstem (UM) downstream mainstem diversions (DM DD, respectively). indicate stop navigating 1,000 km.data returned navigate_nldi list object two spatial dataframes, one origin segment, one upstream stream network.can plot network ggplot. plot original segment new steam network referring different list elements using $ symbol.’s great get upstream river network easily, downloading flowline NLDI function provides geometry COMID flowlines. want data extensive NHD dataset?can download subset NHD layer subset_nhdplus function, get us attributes associated stream segments, well NHD layers.subset_nhdplus function several options. See comments code brief explanation, function description info.data returned subset_nhdplus list object spatial dataframes, case five different ones listed , including flowlines, also useful data catchment layer.create standalone layers nhd_sub object.Now info segment, can lot . example, can plot just bigger streams within network (.e., streams Strahler stream order 3 ).can also calculate length flows upstream dam kilometers:full flowline data, segment ton attributes useful. print attributes . information attributes can found online, including .wanted understand upstream affects removing dam increasing connectivity fish, several things might want figure , including whether upstream land use likely support fish, much riverine habitat actually connected removing dam. following code use data packages along river network provide window questions.","code":"\n# install.packages(\"nhdplusTools\")\n# install.packages(\"sf\")\n# install.packages(\"tidyverse\")\n\nlibrary(tidyverse)\nlibrary(nhdplusTools)\nlibrary(sf)\nlat <- 33.106056\nlon <- -83.794806\n\nstart_point <- st_sfc(st_point(c(lon, lat)), crs = 4269)\nstart_segment <- discover_nhdplus_id(start_point, raindrop = TRUE)\n\nstart_segment## Simple feature collection with 2 features and 7 fields\n## Geometry type: LINESTRING\n## Dimension:     XY\n## Bounding box:  xmin: -83.79481 ymin: 33.09087 xmax: -83.78007 ymax: 33.10659\n## Geodetic CRS:  WGS 84\n## # A tibble: 2 × 8\n##   id           gnis_name        comid reachcode      raindrop_pathDist measure intersection_point\n##   <chr>        <chr>            <int> <chr>                      <dbl>   <dbl> <list>            \n## 1 nhdFlowline  Ocmulgee River 6336752 03070103000084              26.1    97.7 <dbl [2]>         \n## 2 raindropPath <NA>                NA <NA>                        NA      NA   <dbl [0]>         \n## # ℹ 1 more variable: geometry <LINESTRING [°]>\nstart_comid <- as.integer(start_segment$comid)[1]\n\nstart_comid## [1] 6336752\nggplot()+\n  geom_sf(data = start_segment)+\n  geom_sf(data = start_point, color = \"red\", size = 4)+\n  scale_x_continuous(breaks = seq(-83.794, -83.782, .004))\nnldi_feature <- list(featureSource = \"comid\", \n                     featureID = start_comid)\n\nflowline_nldi <- navigate_nldi(nldi_feature, mode = \"UT\", \n                               distance_km = 1000)\nggplot()+\n  geom_sf(data = flowline_nldi$UT, linewidth = 0.5, color = \"lightblue\")+\n  geom_sf(data = flowline_nldi$origin, linewidth = 1, color = \"black\")+\n  geom_sf(data = start_point, size = 1, linewidth = 2, color = \"red\") +\n  scale_x_continuous(breaks = seq(-84.4, -83.8, .2))\nnhd_sub <-subset_nhdplus(comids = as.integer(flowline_nldi$UT$nhdplus_comid),\n                         nhdplus_data = \"download\", # Or use nhdplus_path()\n                         flowline_only = FALSE,     # FALSE downloads flowlines, catchments, waterbodies, etc.\n                         return_data = TRUE)        # FALSE downloads to file; TRUE downloads into list in R session\n                         \n\nnames(nhd_sub)## [1] \"NHDFlowline_Network\"    \"CatchmentSP\"            \"NHDArea\"               \n## [4] \"NHDWaterbody\"           \"NHDFlowline_NonNetwork\"\nflowline <- nhd_sub$NHDFlowline_Network\ncatchment <- nhd_sub$CatchmentSP\nwaterbody <- nhd_sub$NHDWaterbody\nbigger_streams <- filter(flowline, streamorde > 2)\n\nggplot()+\n  geom_sf(data = flowline, linewidth = 1, color = \"blue\")+\n  geom_sf(data = bigger_streams, linewidth = 1.5, color = \"grey30\")  + \n  scale_x_continuous(breaks = seq(-84.4, -83.8, .2))\ntotal_upstream_km <- sum(flowline$lengthkm)\nnames(flowline)##   [1] \"comid\"        \"fdate\"        \"resolution\"   \"gnis_id\"      \"gnis_name\"    \"lengthkm\"    \n##   [7] \"reachcode\"    \"flowdir\"      \"wbareacomi\"   \"ftype\"        \"fcode\"        \"shape_length\"\n##  [13] \"streamleve\"   \"streamorde\"   \"streamcalc\"   \"fromnode\"     \"tonode\"       \"hydroseq\"    \n##  [19] \"levelpathi\"   \"pathlength\"   \"terminalpa\"   \"arbolatesu\"   \"divergence\"   \"startflag\"   \n##  [25] \"terminalfl\"   \"dnlevel\"      \"uplevelpat\"   \"uphydroseq\"   \"dnlevelpat\"   \"dnminorhyd\"  \n##  [31] \"dndraincou\"   \"dnhydroseq\"   \"frommeas\"     \"tomeas\"       \"rtndiv\"       \"vpuin\"       \n##  [37] \"vpuout\"       \"areasqkm\"     \"totdasqkm\"    \"divdasqkm\"    \"tidal\"        \"totma\"       \n##  [43] \"wbareatype\"   \"pathtimema\"   \"hwnodesqkm\"   \"maxelevraw\"   \"minelevraw\"   \"maxelevsmo\"  \n##  [49] \"minelevsmo\"   \"slope\"        \"elevfixed\"    \"hwtype\"       \"slopelenkm\"   \"qa_ma\"       \n##  [55] \"va_ma\"        \"qc_ma\"        \"vc_ma\"        \"qe_ma\"        \"ve_ma\"        \"qa_01\"       \n##  [61] \"va_01\"        \"qc_01\"        \"vc_01\"        \"qe_01\"        \"ve_01\"        \"qa_02\"       \n##  [67] \"va_02\"        \"qc_02\"        \"vc_02\"        \"qe_02\"        \"ve_02\"        \"qa_03\"       \n##  [73] \"va_03\"        \"qc_03\"        \"vc_03\"        \"qe_03\"        \"ve_03\"        \"qa_04\"       \n##  [79] \"va_04\"        \"qc_04\"        \"vc_04\"        \"qe_04\"        \"ve_04\"        \"qa_05\"       \n##  [85] \"va_05\"        \"qc_05\"        \"vc_05\"        \"qe_05\"        \"ve_05\"        \"qa_06\"       \n##  [91] \"va_06\"        \"qc_06\"        \"vc_06\"        \"qe_06\"        \"ve_06\"        \"qa_07\"       \n##  [97] \"va_07\"        \"qc_07\"        \"vc_07\"        \"qe_07\"       \n##  [ reached getOption(\"max.print\") -- omitted 38 entries ]"},{"path":"national-hydrography-dataset.html","id":"use-epa-streamcat-data-for-land-use","chapter":"28 National Hydrography Dataset","heading":"28.3.2 Use EPA StreamCat data for land use","text":"EPA created StreamCat dataset contains substantial information catchment found NHD layer. ’re going use dataset look little bit land use upstream watershed.dataset can downloaded online, ’ll use StreamCatTools package pull landuse data directly R. package hosted CRAN ’s still developed. , users need “uncomment” following code run download StreamCatTools first time analysis.’s downloaded, can load StreamCatTools package. several ways download NLCD data package (see ), easy way use sc_nlcd function. , indicate interested catchment data (cat), opposed watershed data (ws) bigger scale, interested data Georgia, latest year NLCD data available, 2019.can see addition column catchment COMID, percentage various land cover types, percentage mixed forest (pctmxfst2019cat) columns urban land cover (high, medium, low, open).’re interested land cover data watershed, GA. , use left_join merge land cover data catchment layer. join done matching COMIDs, although column COMIDs called featureid catchment dataset.interested , say, urban land cover, can sum across different categories urban land cover data, accomplished summing across columns names match string “urb”.Let’s plot catchments now see urban different catchments dam.Based , headwaters basin lot urban land cover, approaching 100% cases, clear ramifications kinds organisms might persist .","code":"\n#install.packages(\"remotes\")\n#library(remotes)\n#install_github(\"USEPA/StreamCatTools\", build_vignettes=FALSE)\nlibrary(StreamCatTools)\n\nga_nlcd <- sc_nlcd(aoi = 'cat', state='GA', year = '2019')\n\nnames(ga_nlcd)##  [1] \"comid\"           \"pctbl2019cat\"    \"pctconif2019cat\" \"pctcrop2019cat\"  \"pctdecid2019cat\"\n##  [6] \"pctgrs2019cat\"   \"pcthay2019cat\"   \"pcthbwet2019cat\" \"pctice2019cat\"   \"pctmxfst2019cat\"\n## [11] \"pctow2019cat\"    \"pctshrb2019cat\"  \"pcturbhi2019cat\" \"pcturblo2019cat\" \"pcturbmd2019cat\"\n## [16] \"pcturbop2019cat\" \"pctwdwet2019cat\"\ncatchment_lc <- left_join(catchment, ga_nlcd,\n                    by = c(\"featureid\" = \"comid\")) %>%\n  mutate(pct_urb = rowSums(across(matches(\"urb\"))))\nggplot()+\n  geom_sf(data = catchment_lc, aes(fill = pct_urb))+\n  geom_sf(data = filter(flowline, streamorde > 2), \n     linewidth = 1.5, color = \"blue\")+\n  scale_fill_continuous(type = \"viridis\")+\n  scale_x_continuous(breaks = seq(-84.4, -83.8, .2))"},{"path":"national-hydrography-dataset.html","id":"use-nid-for-dams","chapter":"28 National Hydrography Dataset","heading":"28.3.3 Use NID for dams","text":"Another consideration thinking connectivity watershed upstream Juliette concerns stream network impacted dams.take look , ’ll use data National Inventory Dams (NID), database managed USACE documents known dams U.S.NID can queried downloaded online, can also use dams package R download subset dams.load dams package. (Install package first time use .) can quickly filter NID just Georgia ’s wieldy.Next, can make NID spatial object st_as_sf function sf package. , need tell function columns contain x- y-coordinates dam, well coordinate system.need find stream segments associated dam can see river network obstructions. , flowline object line feature, can’t intersected/overlaid directly point object (dam object), even point line extremely close space, overlap. Therefore, see flowlines dam points meet, ’ll turn line object polygon can intersected dam points. ’ll using st_buffer function.First, ’ll imagine ’re interested habitat fish likes larger streams, ’ll filter theflowline dataframe just stream orders greater 2. may want buffer line 100 meters . However, , first need re-project coordinate system one uses metric system instead latitude longitude (, NAD83, Zone 17 N, EPSG 26917). end line, also want buffer completely flat segment flush next one, opposed rounded ends overlap (hence code endCapStyle = \"Flat\").Now can use st_intersection function see dams intersect larger streams network. Note: , also need convert coordinate system dams buffered stream segments.second code simply shows dam now associated COMID, thus specific stream segment.Now can see bifurcated larger stream network . , simply filter original flowline remove small streams streams COMIDs found associated dams (.e., COMID values now found nid100 object thus segments dam).Based plot, can now see upstream network definitely bisected dams.want find much mainstem river opened removing dam?can use network attributes streamlayer answer question, particular fromnode tonode attributes. fromnode unique code representing upstream end stream segment, identical value tonode downstream end segment immediately upstream . , can use values two nodes map upstream watershed disjointed network, navigation stop get dams small streams.First, can use COMID downstream-segment find value fromnode want start .COMID? ’s stored object created outset:can use fromnode segment find adjacent stream segmentsTo show works, show filter flowline just flowline tonode equals initial fromnode, get new steam segment:Given proof concept, just need package -loop, seen , can iterate pairs nodes. example, simply create empty vector store COMIDs encountered. iteration loop, use fromnode values new segments look tonode values next set segments.COMIDs newly navigated area, can filter original flowline just newly-opened mainstem sterams.Finally, let’s plot new network larger streams connected dam question removed, along dams found river network general, total upstream watershed.’s lot new network, let’s just compare length length overall network.suggests removing Juliette Dam open approximately 159 kms upstream mainstem habitat. course, ’s likely bit , conisder partial portion stream segments contain dams, amounts fairly trivial, stream segments network average 1.37.","code":"\n#install.packages(\"dams\")\nlibrary(dams)\n\n# Download subset of dam data\nnid <- dams::nid_subset %>%\n  filter(state == \"GA\")\n# Make NID data a spatial object; set coordinate reference system (crs) to Lat Long EPSG\nnid_sf <- st_as_sf(nid, coords = c(\"longitude\", \"latitude\"), crs = 4269) \n\nnames(nid_sf)##  [1] \"recordid\"              \"dam_name\"              \"nidid\"                 \"county\"               \n##  [5] \"owner_type\"            \"dam_type\"              \"purposes\"              \"year_completed\"       \n##  [9] \"nid_height\"            \"max_storage\"           \"normal_storage\"        \"nid_storage\"          \n## [13] \"hazard\"                \"eap\"                   \"inspection_frequency\"  \"state_reg_dam\"        \n## [17] \"spillway_width\"        \"volume\"                \"number_of_locks\"       \"length_of_locks\"      \n## [21] \"width_of_locks\"        \"source_agency\"         \"state\"                 \"submit_date\"          \n## [25] \"party\"                 \"numseparatestructures\" \"permittingauthority\"   \"inspectionauthority\"  \n## [29] \"enforcementauthority\"  \"jurisdictionaldam\"     \"geometry\"\n# Create buffered streams of third order and bigger streams\n# Transform to NAD83 zone 17n so can use meters for buffer\nstream100 <- filter(flowline, streamorde > 2) %>%\n  st_transform(crs = 26917) %>%\n  st_buffer(dist = 100, endCapStyle=\"FLAT\")\n#Dams that intersect with mainstem buffer\nnid100 <- nid_sf %>%\n  st_transform(crs = 26917) %>%\n  st_intersection(stream100) ## Warning: attribute variables are assumed to be spatially constant throughout all geometries\nnid100 %>% select(dam_name, comid) %>%\n  head()## Simple feature collection with 6 features and 2 fields\n## Geometry type: POINT\n## Dimension:     XY\n## Bounding box:  xmin: 222446 ymin: 3715728 xmax: 240149.9 ymax: 3764001\n## Projected CRS: NAD83 / UTM zone 17N\n## # A tibble: 6 × 3\n##   dam_name                                               comid           geometry\n##   <chr>                                                  <int>        <POINT [m]>\n## 1 ATLANTA AUTOMOTIVE DISTRIBUTION CENTER DETENTION DAM 6330502 (227982.4 3764001)\n## 2 JACK TURNER RESERVOIR DAM                            6330976 (227895.6 3736098)\n## 3 MILSTEAD                                             6331062   (222446 3731921)\n## 4 CORNISH CREEK RESERVOIR DAM                          6331166 (240149.9 3726872)\n## 5 TWIN HILLS LAKE                                      6331166 (239756.8 3727592)\n## 6 CAPES SAUSAGE COMPANY LAKE DAM                       6331416 (222935.6 3715728)\n# Filter flowline to just \nflow_no_dam <- filter(flowline, \n                      streamorde > 2,\n                      !comid %in% nid100$comid)\n\nplot(flow_no_dam$geometry)\nstart_comid## [1] 6336752\nfrom_node <- flowline$fromnode[flowline$comid == start_comid]\n\nfrom_node## [1] 270015756\nfilter(flow_no_dam, tonode %in% from_node)## Simple feature collection with 1 feature and 137 fields\n## Geometry type: LINESTRING\n## Dimension:     XY\n## Bounding box:  xmin: -83.79718 ymin: 33.10659 xmax: -83.79452 ymax: 33.11509\n## Geodetic CRS:  NAD83\n## # A tibble: 1 × 138\n##     comid fdate               resolution gnis_id gnis_name     lengthkm reachcode flowdir wbareacomi\n## *   <int> <dttm>              <chr>      <chr>   <chr>            <dbl> <chr>     <chr>        <int>\n## 1 6336742 2008-12-10 00:00:00 Medium     356443  Ocmulgee Riv…    0.995 03070103… With D…  120049944\n## # ℹ 129 more variables: ftype <chr>, fcode <int>, shape_length <dbl>, streamleve <int>,\n## #   streamorde <int>, streamcalc <int>, fromnode <dbl>, tonode <dbl>, hydroseq <dbl>,\n## #   levelpathi <dbl>, pathlength <dbl>, terminalpa <dbl>, arbolatesu <dbl>, divergence <int>,\n## #   startflag <int>, terminalfl <int>, dnlevel <int>, uplevelpat <dbl>, uphydroseq <dbl>,\n## #   dnlevelpat <dbl>, dnminorhyd <dbl>, dndraincou <int>, dnhydroseq <dbl>, frommeas <dbl>,\n## #   tomeas <dbl>, rtndiv <int>, vpuin <int>, vpuout <int>, areasqkm <dbl>, totdasqkm <dbl>,\n## #   divdasqkm <dbl>, tidal <int>, totma <dbl>, wbareatype <chr>, pathtimema <dbl>, …\n# Empty vector to store new comids\nsegs_all <- vector()\n\nfor (i in 1:length(flow_no_dam)){\n\n  to_segs <- filter(flow_no_dam, tonode %in% from_node)\n  from_node <- unique(to_segs$fromnode)\n\n  segs_all <- c(segs_all, to_segs$comid)\n  \nif(nrow(to_segs) == 0) {break}\n  }\nnew_flow <- filter(flowline, comid %in% segs_all)\nggplot()+\n  geom_sf(data = flowline, color = \"lightblue\", linewidth = .5)+\n  geom_sf(data = new_flow, color = \"blue\", linewidth = 1)+\n  geom_sf(data = nid100, color = \"red\", size = 2)+\n  scale_x_continuous(breaks = seq(-84.5, -83.5, .2 ))\ntotal_upstream_km## [1] 3926.602\nsum(new_flow$lengthkm)## [1] 159.549\nsum(new_flow$lengthkm)/total_upstream_km## [1] 0.04063284"},{"path":"national-hydrography-dataset.html","id":"finding-streamgages","chapter":"28 National Hydrography Dataset","heading":"28.3.4 Finding streamgages","text":"applications nhdplusTools package extensive. One final application highlight find gages upstream point might used hydrologic analysis. code , highly analogous code already used, finds gages flow networks upstream gage interest, case gage Madison, WI.National Hydrography Data huge amount information fundamental water resource management, can unwieldy useThe nhdplusTools package makes using NHD data simple powerfulPerforming analysis R makes simple access data packages well use powerful R programming tool interacting NHD data","code":"\n# Create starting nldi feature\nnldi_feature <- list(featureSource = \"nwissite\", \n                     featureID = \"USGS-05428500\")\n\n# Create a flowline from this feature to extract COMIDs\nflowline_nldi <- navigate_nldi(nldi_feature, \n                               mode = \"upstreamTributaries\", \n                               distance_km = 1000)\n\n# Download full NHD flow data from these COMIDs\ngage_download <-subset_nhdplus(comids = as.integer(flowline_nldi$UT$nhdplus_comid),\n                                  nhdplus_data = \"download\",\n                                  return_data = TRUE)## All intersections performed in latitude/longitude.## Reading NHDFlowline_Network## Writing NHDFlowline_Network\n# Extract flow network from the download\nflowline_nwis <- gage_download$NHDFlowline_Network\n\n# Find all of the upstream gages\nupstream_nwis <- navigate_nldi(nldi_feature,\n                               mode = \"upstreamTributaries\",\n                               data_source = \"nwissite\", \n                               distance_km = 1000)\n\n# Plot gages and stream network\nggplot()+\n  geom_sf(data = flowline_nwis, linewidth = 1, color = \"blue\")+\n  geom_sf(data = upstream_nwis$UT_nwissite, \n     size = 1.5, linewidth = 2, color = \"red\")"}]
